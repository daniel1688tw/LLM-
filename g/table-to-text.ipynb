{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387fc1a5",
   "metadata": {},
   "source": [
    "æ­¤ç¯‡ç ”ç©¶åªéœ€æä¾›\n",
    "\"main.txt\"ç‚ºä½¿ç”¨è€…çš„å¤§ç¶±èˆ‡ç°¡çŸ­æƒ³æ³•\n",
    "\"data_description.txt\"ç‚ºè¦åˆ†æçš„table columnsæ‰€ä»£è¡¨çš„æ„ç¾©\n",
    "å°±å¯ç”¢ç”Ÿå®Œæ•´å ±å°\n",
    "(å¯ä½¿ç”¨åœ¨ç”¢ç”Ÿä»»ä½•å ±å°ä¸Šä¸é™æ–¼ç¾½çƒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c8faf",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "\n",
    "åˆªæ¸›ä¸å¿…è¦çš„columns\n",
    "\n",
    "çµæœä¿ç•™['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9666a701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åŸå§‹å›æ‡‰: ```python\n",
      "['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n",
      "```\n",
      "âœ… ç¯©é¸å‡ºçš„æ¬„ä½: ['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n",
      "\n",
      "æœ€çµ‚æ¬„ä½æ¸…å–®:\n",
      "['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n"
     ]
    }
   ],
   "source": [
    "#æ­£å¼\n",
    "import dspy\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "     \n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "         \n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "         \n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{\n",
    "                'text': response.text,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "     \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def parse_list_from_response(response_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a Python list from various response formats including markdown code blocks\n",
    "    \"\"\"\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```(?:python)?\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    \n",
    "    # Remove any additional backticks\n",
    "    text = text.strip('`').strip()\n",
    "    \n",
    "    # Try to find a list pattern in the text\n",
    "    list_patterns = [\n",
    "        r'\\[([^\\]]+)\\]',  # Match content within square brackets\n",
    "        r'(\\[.*?\\])',     # Match the entire list including brackets\n",
    "    ]\n",
    "    \n",
    "    for pattern in list_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            list_text = match.group(0) if pattern == r'(\\[.*?\\])' else '[' + match.group(1) + ']'\n",
    "            break\n",
    "    else:\n",
    "        # If no pattern matches, assume the entire cleaned text is the list\n",
    "        list_text = text\n",
    "    \n",
    "    # Clean up the list text\n",
    "    list_text = list_text.strip()\n",
    "    \n",
    "    # Handle both single and double quotes\n",
    "    try:\n",
    "        # First try parsing as-is\n",
    "        return json.loads(list_text)\n",
    "    except json.JSONDecodeError:\n",
    "\n",
    "        # Try converting single quotes to double quotes\n",
    "        list_text_double_quotes = list_text.replace(\"'\", '\"')\n",
    "        return json.loads(list_text_double_quotes)\n",
    "\n",
    "\n",
    "def extract_news_relevant_fields(description_path: str, main_path: str):\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")\n",
    "        return []\n",
    "     \n",
    "    lm = setup_gemini_api(api_key)\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(description_path)\n",
    "    \n",
    "    prompt = f\"\"\"Using the following outline and list of data column descriptions, select only the columns that are useful for the outline.\n",
    "\n",
    "## outline\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Please return only a Python list of column names, like this:\n",
    "['player_name', 'match_score', 'duration', ...]\n",
    "\n",
    "Do not include explanations or any other text. Return only the list.\"\"\"\n",
    "     \n",
    "    result = lm.basic_request(prompt)\n",
    "    \n",
    "    print(f\"ğŸ” åŸå§‹å›æ‡‰: {result}\")\n",
    "    \n",
    "    selected_fields = parse_list_from_response(result)\n",
    "    \n",
    "    if selected_fields:\n",
    "        print(\"âœ… ç¯©é¸å‡ºçš„æ¬„ä½:\", selected_fields)\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè§£ææ¬„ä½åˆ—è¡¨\")\n",
    "    \n",
    "    return selected_fields\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fields = extract_news_relevant_fields(\"data_description.txt\", \"main.txt\")\n",
    "    print(\"\\næœ€çµ‚æ¬„ä½æ¸…å–®:\")\n",
    "    print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14f81b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields = ['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n",
    "df = pd.read_csv(\"set1.csv\")\n",
    "filtered_df = df[fields]\n",
    "filtered_df.to_csv(\"filtered_set1.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609780f4",
   "metadata": {},
   "source": [
    "æ›´æ”¹åŸå§‹çš„\"data_description.txt\"åˆ°\"filtered_data_description.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bdc0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ filtered_data_description.txt\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def extract_descriptions_for_fields(fields: List[str], desc_path: str, output_path: str):\n",
    "    description_text = read_text_file(desc_path)\n",
    "\n",
    "    field_desc = {}\n",
    "    for line in description_text.splitlines():\n",
    "        for field in fields:\n",
    "            if line.lower().startswith(field.lower() + \":\"):\n",
    "                field_desc[field] = line.strip()\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for field in fields:\n",
    "                f.write(field_desc.get(field, f\"{field}: [Description not found]\") + \"\\n\")\n",
    "        print(f\"âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯«å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "\n",
    "extract_descriptions_for_fields(fields, 'data_description.txt', \"filtered_data_description.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484f0a6",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "\n",
    "è—‰ç”±äººç‚ºè¼¸å…¥å•é¡Œèˆ‡æ–¹å‘æç¤ºï¼Œçµ¦LLMåšå®Œæ•´åˆ†æå•é¡Œèˆ‡æ–¹å‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d8455eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response saved to: analyze_response.txt\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "\n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "\n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{\n",
    "                'text': response.text,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "\n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def generate_chain_of_thought_response(main_path: str, desc_path: str, output_path: str):\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")\n",
    "        return\n",
    "\n",
    "    lm = setup_gemini_api(api_key)\n",
    "\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(desc_path)\n",
    "\n",
    "    chain_prompt = f\"\"\"\n",
    "You are a planning assistant.\n",
    "Analyze the following outline and column descriptions.\n",
    "\n",
    "## Outline & Ideas:\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Step-by-step:\n",
    "1. Reflect on the structure and meaning of the content.\n",
    "2. Formulate relevant and meanful questions or planning strategies.\n",
    "3. Be explicit and detailed, use Chain-of-Thought reasoning.\n",
    "4. Output all thoughts and questions in English only.\n",
    "\"\"\"\n",
    "\n",
    "    result = lm.basic_request(chain_prompt)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "        print(f\"âœ… Response saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to write output: {e}\")\n",
    "\n",
    "def main():\n",
    "    generate_chain_of_thought_response(\n",
    "        main_path=\"main.txt\",\n",
    "        desc_path=\"filtered_data_description.txt\",\n",
    "        output_path=\"analyze_response.txt\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b495c8",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "\n",
    "è«‹LLMæ ¹æ“š\"analyze_response.txt\"æ€è€ƒå¯ä»¥ä½¿ç”¨çš„operationä¸¦å°‡çµæœå­˜æ–¼ \"operations_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f5f0426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ operations_info.json\n",
      "\n",
      "âœ… æ“ä½œåç¨±é™£åˆ—:\n",
      "['write', 'select_row', 'select_column', 'group_by', 'aggregate', 'value_counts', 'crosstab', 'pivot_table', 'sort', 'calculate', 'merge', 'rolling_window', 'normalize', 'correlation', 'one_hot_encoding']\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "\n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "\n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{\n",
    "                'text': response.text,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "\n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def analyze_operations(analyze_path: str, output_json: str) -> List[str]:\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")\n",
    "        return []\n",
    "\n",
    "    lm = setup_gemini_api(api_key)\n",
    "    analysis = read_text_file(analyze_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a news journalist want to analyze data not forecaster.\n",
    "Based on the following text analysis, identify multiple useful table operations\n",
    "and describe the direct meaning of each operation.\n",
    "\n",
    "## Text Analysis:\n",
    "{analysis}\n",
    "\n",
    "---\n",
    "\n",
    "Please output a numbered list in this format:\n",
    "1. write: If the table is clear or small enough, generates text based on the tables using the LLM.\n",
    "2. select_row: Description\n",
    "3. select_column: Description\n",
    "4. operation_name: Description\n",
    "5. operation_name: Description\n",
    "...\n",
    "\n",
    "IMPORTANT: operation must contain select_row, select_column, and write in the first three operation.\n",
    "\n",
    "Give important operations and at most 15 operations.\n",
    "operation_name should be different and each operation can not be similar.\n",
    "operation can be apply on many columns is better.\n",
    "Description just give the original definition of the operation name and give some useful functions name in pandas.\n",
    "Only include operations and their descriptions. Be concise and clear.\n",
    "\"\"\"\n",
    "\n",
    "    response = lm.basic_request(prompt)\n",
    "\n",
    "    operations = []\n",
    "    operations_dict = {}\n",
    "\n",
    "    try:\n",
    "        for line in response.strip().split('\\n'):\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            if \".\" in line:\n",
    "                num, rest = line.split(\".\", 1)\n",
    "                if \":\" in rest:\n",
    "                    name, desc = rest.strip().split(\":\", 1)\n",
    "                    name = name.strip()\n",
    "                    desc = desc.strip()\n",
    "                    operations.append(name)\n",
    "                    operations_dict[num.strip()] = {\"operation\": name, \"description\": desc}\n",
    "\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(operations_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ {output_json}\")\n",
    "        return operations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å›æ‡‰è™•ç†å¤±æ•—: {e}\\nåŸå§‹å›æ‡‰:\\n{response}\")\n",
    "        return []\n",
    "\n",
    "ops = analyze_operations(\"analyze_response.txt\", \"operations_info.json\")\n",
    "print(\"\\nâœ… æ“ä½œåç¨±é™£åˆ—:\")\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9545745",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "\n",
    "ä½¿LLMè‡ªå‹•åˆ†ætableé¸å‡ºåˆé©çš„operationæ”¾å…¥æ“ä½œæ± (operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "153883e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading badminton match data...\n",
      "Identified 1 suitable operations:\n",
      "1. [1, 2, 3, 5, 7, 9, 10, 11, 12, 18, 19]\n",
      "\n",
      "Final operations array: ['[1, 2, 3, 5, 7, 9, 10, 11, 12, 18, 19]']\n"
     ]
    }
   ],
   "source": [
    "# badminton_analysis.py\n",
    "#æ­£å¼\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define a simple signature for the ChainOfThought\n",
    "class OperationSignature(dspy.Signature):\n",
    "    \"\"\"Identify suitable operations for analyzing badminton match data.\"\"\"\n",
    "    data_description = dspy.InputField(desc=\"Overview and sample of the dataset\")\n",
    "    column_descriptions = dspy.InputField(desc=\"Descriptions of each column in the dataset\")\n",
    "    rules = dspy.InputField(desc=\"Rules for selecting operations\")\n",
    "    operations_list = dspy.OutputField(desc=\"A list of suitable operations number\")\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "def parse_column_descriptions(description_text):\n",
    "    descriptions = {}\n",
    "    pattern = r'''\n",
    "        ^                # Line start\n",
    "        (\\w+)            # Column name\n",
    "        :\\s+             # Colon and space\n",
    "        (.+?)            # Description text\n",
    "        (?=\\n\\w+:\\s+|\\Z) # Lookahead for next column or end of file\n",
    "    '''\n",
    "    matches = re.findall(pattern, description_text, flags=re.M | re.X)\n",
    "    for col_name, desc in matches:\n",
    "        clean_desc = ' '.join(desc.split()).strip()\n",
    "        descriptions[col_name] = clean_desc\n",
    "    return descriptions\n",
    "\n",
    "class BadmintonOperationSelector(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.chain_of_thought = dspy.ChainOfThought(OperationSignature)\n",
    "\n",
    "    def forward(self, data_description, column_descriptions, rules):\n",
    "        result = self.chain_of_thought(\n",
    "            data_description=data_description,\n",
    "            column_descriptions=str(column_descriptions),\n",
    "            rules=str(rules)\n",
    "        )\n",
    "        return self.extract_operations_from_result(result.operations_list)\n",
    "\n",
    "    def extract_operations_from_result(self, operations_text):\n",
    "        operations = []\n",
    "        lines = operations_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line = re.sub(r'^[\\d\\.\\)\\-\\*]+\\s*', '', line)\n",
    "            if line and len(line) < 100:\n",
    "                operations.append(line)\n",
    "        return operations\n",
    "\n",
    "def analyze_badminton_match(data_path, column_desc_path, rules_path, api_key):\n",
    "    print(\"Reading badminton match data...\")\n",
    "    try:\n",
    "        match_data = read_badminton_data(data_path)\n",
    "        columns_desc_content = read_text_file(column_desc_path)\n",
    "        rules = read_json_file(rules_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return []\n",
    "\n",
    "    column_descriptions = parse_column_descriptions(columns_desc_content)\n",
    "    setup_gemini_api(api_key)\n",
    "\n",
    "    data_sample = match_data.to_string()\n",
    "    data_description = f\"\"\"\n",
    "    one match data:\n",
    "    {data_sample}\n",
    "\n",
    "    Data shape: {match_data.shape[0]} rows, {match_data.shape[1]} columns\n",
    "    Columns: {', '.join(match_data.columns)}\n",
    "    \"\"\"\n",
    "\n",
    "    selector = BadmintonOperationSelector()\n",
    "    operations = selector.forward(data_description, column_descriptions, rules)\n",
    "\n",
    "    print(f\"Identified {len(operations)} suitable operations:\")\n",
    "    for i, op in enumerate(operations, 1):\n",
    "        print(f\"{i}. {op}\")\n",
    "\n",
    "    return operations\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    os.environ['GOOGLE_API_KEY'] = api_key\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"set1.csv\"\n",
    "    column_desc_path = \"data_description.txt\"\n",
    "    rules_path = \"operation_description.json\"\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyDI6yAgr689NOqj2G34cgDr-aa5tv2aO8g\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"Error: Google API key not found. Please set the GOOGLE_API_KEY environment variable.\")\n",
    "    else:\n",
    "        operations = analyze_badminton_match(data_path, column_desc_path, rules_path, api_key)\n",
    "        print(\"\\nFinal operations array:\", operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db5ef4",
   "metadata": {},
   "source": [
    "å°‡æ‰€æŒ‘é¸å‡ºä¾†çš„æ“ä½œå¯«å…¥\"operations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7a99127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operations.json has been created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# å¾ JSON æª”æ¡ˆè®€å– \"operations\" åˆ†æ”¯\n",
    "with open(\"operation_description.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_data = json.load(f)\n",
    "    original_operations = all_data[\"operations\"]\n",
    "\n",
    "# ä½ æƒ³è¦æŒ‘é¸çš„ operation ç·¨è™Ÿï¼ˆæ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹é€™å€‹ listï¼‰\n",
    "selected_numbers = [1, 2, 3, 5, 7, 9, 10, 11, 12, 18, 19]\n",
    "\n",
    "# æ ¹æ“š selected_numbers é¸å‡ºå°æ‡‰æ“ä½œï¼Œä¸¦å¾ 1 é–‹å§‹é‡æ–°ç·¨è™Ÿ\n",
    "filtered_operations = []\n",
    "for new_number, original_number in enumerate(selected_numbers, start=1):\n",
    "    for op in original_operations:\n",
    "        if op[\"number\"] == original_number:\n",
    "            filtered_operations.append({\n",
    "                \"number\": new_number,\n",
    "                \"name\": op[\"name\"],\n",
    "                \"description\": op[\"description\"]\n",
    "            })\n",
    "            break\n",
    "\n",
    "# æ–°çš„ JSON çµæ§‹\n",
    "output_json = {\n",
    "    \"description\": \"Selected operations for badminton data analysis.\",\n",
    "    \"requirements\": [\n",
    "        \"The output must be based on the input data ; do not hallucinate.\",\n",
    "        \"Give me the list of numbers.\"\n",
    "    ],\n",
    "    \"operations\": filtered_operations\n",
    "}\n",
    "\n",
    "# å¯«å…¥ JSON æª”æ¡ˆ\n",
    "with open(\"operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"operations.json has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef6c0a",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "\n",
    "ç¯©é¸å‡ºæœ€åˆé©çš„1/2 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258834da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations.json æˆåŠŸè¼‰å…¥ 11 å€‹æ“ä½œ\n",
      "\n",
      "è¼‰å…¥çš„æ“ä½œè©³æƒ…:\n",
      "  1. ç·¨è™Ÿ: 1, åç¨±: View data\n",
      "      æè¿°: Inspect dataset preview and structure.\n",
      "  2. ç·¨è™Ÿ: 2, åç¨±: Add column\n",
      "      æè¿°: Add new features, e.g., rally length or is_win.\n",
      "  3. ç·¨è™Ÿ: 3, åç¨±: Delete column\n",
      "      æè¿°: Remove irrelevant or noisy fields.\n",
      "  4. ç·¨è™Ÿ: 4, åç¨±: Fill missing values\n",
      "      æè¿°: Replace NaN with default or computed values.\n",
      "  5. ç·¨è™Ÿ: 5, åç¨±: Column statistics\n",
      "      æè¿°: Compute stats like average rally time or shot count.\n",
      "  ... é‚„æœ‰ 6 å€‹æ“ä½œ\n",
      "\n",
      "æˆåŠŸè¼‰å…¥è³‡æ–™é›†: 315 è¡Œ, 30 åˆ—\n",
      "åŸå§‹æ“ä½œæ•¸é‡: 11\n",
      "åŸå§‹æ“ä½œæ¸…å–®:\n",
      "  1. 1. View data: Inspect dataset preview and structure.\n",
      "  2. 2. Add column: Add new features, e.g., rally length or is_win.\n",
      "  3. 3. Delete column: Remove irrelevant or noisy fields.\n",
      "  4. 4. Fill missing values: Replace NaN with default or computed values.\n",
      "  5. 5. Column statistics: Compute stats like average rally time or shot count.\n",
      "  6. 6. Sort data: Sort by duration, player, or score for ranking or filtering.\n",
      "  7. 7. Filter data: Use logical conditions to narrow down the dataset.\n",
      "  8. 8. Group data: Cluster data by player, match, or shot type for summarization.\n",
      "  9. 9. Aggregate data: Summarize grouped data via average, count, etc.\n",
      "  10. 10. Check/handle missing values: Diagnose and manage incomplete data.\n",
      "  11. 11. Count: Count unique values or occurrences in columns.\n",
      "\n",
      "ä½¿ç”¨ Gemini LLM éæ¿¾æ“ä½œ (ç§»é™¤ 50%)...\n",
      "æ–¹æ³•1: ä½¿ç”¨è‡ªå®šç¾©éæ¿¾å™¨ + Gemini...\n",
      "response: åˆ†ææ€è·¯ï¼š\n",
      "\n",
      "é¦–å…ˆï¼Œå¯©è¦–æ¯å€‹æ“ä½œåœ¨ç¾½çƒæ¯”è³½è³‡æ–™åˆ†æä¸­çš„ä½œç”¨å’Œå¿…è¦æ€§ï¼Œçµåˆè³‡æ–™é›†çš„æ¬„ä½è³‡è¨Šé€²è¡Œåˆ¤æ–·ã€‚\n",
      "\n",
      "*   **1. View data (æŸ¥çœ‹è³‡æ–™)**ï¼šé€™æ˜¯ä»»ä½•è³‡æ–™åˆ†æçš„ç¬¬ä¸€æ­¥ï¼Œå¿…é ˆä¿ç•™ï¼Œç”¨æ–¼åˆæ­¥äº†è§£è³‡æ–™é›†ã€‚\n",
      "\n",
      "*   **2. Add column (æ–°å¢æ¬„ä½)**ï¼šéå¸¸æœ‰ç”¨ï¼Œå¯ä»¥åŸºæ–¼ç¾æœ‰æ¬„ä½å‰µå»ºæ–°çš„ç‰¹å¾µï¼Œä¾‹å¦‚è¨ˆç®—çƒé€Ÿã€æ“Šçƒè§’åº¦ã€å›åˆæŒçºŒæ™‚é–“ï¼Œç”šè‡³ä½¿ç”¨ç¾æœ‰çš„ `win_reason` å’Œ `lose_reason` æ¬„ä½å‰µå»ºä¸€å€‹ `is_win` æ¬„ä½ã€‚\n",
      "\n",
      "*   **3. Delete column (åˆªé™¤æ¬„ä½)**ï¼š å¦‚æœå­˜åœ¨ç„¡æ„ç¾©æˆ–é«˜åº¦å†—é¤˜çš„æ¬„ä½ï¼Œå‰‡å¾ˆæœ‰ç”¨ï¼Œä½†éœ€è¦å°å¿ƒæ“ä½œï¼Œé¿å…èª¤åˆªé‡è¦è³‡è¨Šã€‚`db` æ¬„ä½åªæœ‰ä¸€å€‹å€¼ 0ï¼Œæ²’æœ‰æä¾›ä»»ä½•è³‡è¨Šï¼Œå¯ä»¥åˆªé™¤ã€‚\n",
      "\n",
      "*   **4. Fill missing values (å¡«è£œç¼ºå¤±å€¼)**ï¼šé‘‘æ–¼ `lose_reason`, `win_reason`, å’Œ `getpoint_player` æ¬„ä½å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆnanï¼‰ï¼Œæ­¤æ“ä½œéå¸¸é‡è¦ï¼Œå¯ä»¥é¿å…å¾ŒçºŒåˆ†æç”¢ç”ŸéŒ¯èª¤ã€‚\n",
      "\n",
      "*   **5. Column statistics (æ¬„ä½çµ±è¨ˆ)**ï¼šå¯ä»¥æä¾›é—œæ–¼æ¯å€‹æ¬„ä½çš„åŸºæœ¬çµ±è¨ˆè³‡è¨Šï¼Œä¾‹å¦‚å¹³å‡å€¼ã€æ¨™æº–å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ç­‰ï¼Œæœ‰åŠ©æ–¼ç†è§£è³‡æ–™çš„åˆ†ä½ˆå’Œç‰¹å¾µï¼Œæœ‰åƒ¹å€¼ã€‚\n",
      "\n",
      "*   **6. Sort data (æ’åºè³‡æ–™)**ï¼šæ ¹æ“šæŒ‡å®šçš„æ¬„ä½ï¼ˆä¾‹å¦‚æ™‚é–“ã€çƒå“¡ã€å¾—åˆ†ï¼‰å°è³‡æ–™é€²è¡Œæ’åºï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ°è­˜åˆ¥è¶¨å‹¢å’Œæ¨¡å¼ï¼Œå…·æœ‰ä¸€å®šçš„å¯¦ç”¨æ€§ï¼Œä¾‹å¦‚æŒ‰æ™‚é–“æ’åºè§€å¯Ÿæ¯”è³½é€²ç¨‹ã€‚\n",
      "\n",
      "*   **7. Filter data (ç¯©é¸è³‡æ–™)**ï¼šå…è¨±åŸºæ–¼ç‰¹å®šæ¢ä»¶é¸æ“‡è³‡æ–™å­é›†ï¼Œä¾‹å¦‚åªé¸æ“‡ç‰¹å®šçƒå“¡çš„è³‡æ–™ï¼Œæˆ–æ˜¯ç¯©é¸å‡ºç‰¹å®šé¡å‹çš„æ“Šçƒï¼Œéå¸¸æœ‰ç”¨ã€‚\n",
      "\n",
      "*   **8. Group data (åˆ†çµ„è³‡æ–™)**ï¼šå°‡è³‡æ–™æŒ‰ç‰¹å®šæ¬„ä½ï¼ˆä¾‹å¦‚çƒå“¡ã€æ¯”è³½ã€æ“Šçƒé¡å‹ï¼‰é€²è¡Œåˆ†çµ„ï¼Œä»¥ä¾¿é€²è¡Œå½™ç¸½å’Œæ¯”è¼ƒï¼Œå¸¸ç”¨æ–¼åˆ†æä¸åŒçƒå“¡çš„è¡¨ç¾å·®ç•°ã€ä¸åŒæ“Šçƒé¡å‹çš„æ•ˆæœç­‰ã€‚\n",
      "\n",
      "*   **9. Aggregate data (å½™ç¸½è³‡æ–™)**ï¼šå°åˆ†çµ„å¾Œçš„è³‡æ–™é€²è¡Œå½™ç¸½è¨ˆç®—ï¼Œä¾‹å¦‚è¨ˆç®—æ¯å€‹çƒå“¡çš„å¹³å‡å¾—åˆ†ã€æ¯ç¨®æ“Šçƒé¡å‹çš„æˆåŠŸç‡ç­‰ã€‚ é€šå¸¸å’Œ group data ä¸€èµ·ä½¿ç”¨ã€‚\n",
      "\n",
      "*   **10. Check/handle missing values (æª¢æŸ¥/è™•ç†ç¼ºå¤±å€¼)**ï¼šæœ¬è³ªä¸Šèˆ‡æ“ä½œ 4 ç›¸åŒï¼Œä½œç”¨é‡è¤‡ï¼Œå¯ä»¥åˆªé™¤ã€‚\n",
      "\n",
      "*   **11. Count (è¨ˆæ•¸)**ï¼šçµ±è¨ˆå„æ¬„ä½ä¸­å”¯ä¸€å€¼çš„æ•¸é‡ï¼Œæˆ–è€…ç‰¹å®šå€¼çš„å‡ºç¾æ¬¡æ•¸ï¼Œä¾‹å¦‚ï¼Œçµ±è¨ˆæ¯ç¨®æ“Šçƒé¡å‹çš„æ¬¡æ•¸ï¼Œæˆ–æ˜¯æ¯å€‹çƒå“¡çš„å¾—åˆ†æ¬¡æ•¸ï¼Œæ¯”è¼ƒå¯¦ç”¨ã€‚\n",
      "\n",
      "åŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œä»¥ä¸‹æ“ä½œæ‡‰è©²ç§»é™¤ï¼š\n",
      "\n",
      "*   **3. Delete column**: é›–ç„¶ `db` æ¬„ä½å¯ä»¥åˆªé™¤ï¼Œä½†é€™å€‹æ“ä½œæœ¬èº«ä¸¦ä¸æ˜¯çµ•å°å¿…é ˆçš„ï¼Œå¯ä»¥å…ˆä¸åŸ·è¡Œã€‚\n",
      "*   **6. Sort data**: é›–ç„¶å…·æœ‰ä¸€å®šçš„å¯¦ç”¨æ€§ï¼Œä½†ä¸å¦‚å…¶ä»–æ“ä½œé‡è¦ã€‚å¦‚æœç›®æ¨™ä¸æ˜¯æŒ‰æ™‚é–“é †åºåˆ†ææ¯”è³½éç¨‹ï¼Œå¯ä»¥å…ˆä¸åŸ·è¡Œã€‚\n",
      "*   **8. Group data**: å¦‚æœæ²’æœ‰æ˜ç¢ºçš„åˆ†æç›®æ¨™ï¼Œä¾‹å¦‚è¦åˆ†æç‰¹å®šçƒå“¡æˆ–æ¯”è³½ï¼Œé€™å€‹æ“ä½œçš„åƒ¹å€¼æœƒé™ä½ã€‚\n",
      "*   **9. Aggregate data**: éœ€è¦å…ˆåˆ†çµ„è³‡æ–™æ‰èƒ½é€²è¡Œå½™ç¸½ï¼Œå› æ­¤ç§»é™¤ group data å¾Œï¼Œé€™å€‹æ“ä½œä¹Ÿå¤±å»æ„ç¾©ã€‚\n",
      "*   **10. Check/handle missing values**: èˆ‡æ“ä½œ 4 é‡è¤‡ã€‚\n",
      "\n",
      "ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
      "[1, 2, 4, 5, 7, 11]\n",
      "\n",
      "Debug - æ­£åœ¨åˆ†æå›æ‡‰æ–‡æœ¬...\n",
      "Debug - æª¢æŸ¥è¡Œ: åˆ†ææ€è·¯ï¼š\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: é¦–å…ˆï¼Œå¯©è¦–æ¯å€‹æ“ä½œåœ¨ç¾½çƒæ¯”è³½è³‡æ–™åˆ†æä¸­çš„ä½œç”¨å’Œå¿…è¦æ€§ï¼Œçµåˆè³‡æ–™é›†çš„æ¬„ä½è³‡è¨Šé€²è¡Œåˆ¤æ–·ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **1. View data (æŸ¥çœ‹è³‡æ–™)**ï¼šé€™æ˜¯ä»»ä½•è³‡æ–™åˆ†æçš„ç¬¬ä¸€æ­¥ï¼Œå¿…é ˆä¿ç•™ï¼Œç”¨æ–¼åˆæ­¥äº†è§£è³‡æ–™é›†ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **2. Add column (æ–°å¢æ¬„ä½)**ï¼šéå¸¸æœ‰ç”¨ï¼Œå¯ä»¥åŸºæ–¼ç¾æœ‰æ¬„ä½å‰µå»ºæ–°çš„ç‰¹å¾µï¼Œä¾‹å¦‚è¨ˆç®—çƒé€Ÿã€æ“Šçƒè§’åº¦ã€å›åˆæŒçºŒæ™‚é–“ï¼Œç”šè‡³ä½¿ç”¨ç¾æœ‰çš„ `win_reason` å’Œ `lose_reason` æ¬„ä½å‰µå»ºä¸€å€‹ `is_win` æ¬„ä½ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **3. Delete column (åˆªé™¤æ¬„ä½)**ï¼š å¦‚æœå­˜åœ¨ç„¡æ„ç¾©æˆ–é«˜åº¦å†—é¤˜çš„æ¬„ä½ï¼Œå‰‡å¾ˆæœ‰ç”¨ï¼Œä½†éœ€è¦å°å¿ƒæ“ä½œï¼Œé¿å…èª¤åˆªé‡è¦è³‡è¨Šã€‚`db` æ¬„ä½åªæœ‰ä¸€å€‹å€¼ 0ï¼Œæ²’æœ‰æä¾›ä»»ä½•è³‡è¨Šï¼Œå¯ä»¥åˆªé™¤ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **4. Fill missing values (å¡«è£œç¼ºå¤±å€¼)**ï¼šé‘‘æ–¼ `lose_reason`, `win_reason`, å’Œ `getpoint_player` æ¬„ä½å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆnanï¼‰ï¼Œæ­¤æ“ä½œéå¸¸é‡è¦ï¼Œå¯ä»¥é¿å…å¾ŒçºŒåˆ†æç”¢ç”ŸéŒ¯èª¤ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **5. Column statistics (æ¬„ä½çµ±è¨ˆ)**ï¼šå¯ä»¥æä¾›é—œæ–¼æ¯å€‹æ¬„ä½çš„åŸºæœ¬çµ±è¨ˆè³‡è¨Šï¼Œä¾‹å¦‚å¹³å‡å€¼ã€æ¨™æº–å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ç­‰ï¼Œæœ‰åŠ©æ–¼ç†è§£è³‡æ–™çš„åˆ†ä½ˆå’Œç‰¹å¾µï¼Œæœ‰åƒ¹å€¼ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **6. Sort data (æ’åºè³‡æ–™)**ï¼šæ ¹æ“šæŒ‡å®šçš„æ¬„ä½ï¼ˆä¾‹å¦‚æ™‚é–“ã€çƒå“¡ã€å¾—åˆ†ï¼‰å°è³‡æ–™é€²è¡Œæ’åºï¼Œä»¥ä¾¿æ›´å®¹æ˜“åœ°è­˜åˆ¥è¶¨å‹¢å’Œæ¨¡å¼ï¼Œå…·æœ‰ä¸€å®šçš„å¯¦ç”¨æ€§ï¼Œä¾‹å¦‚æŒ‰æ™‚é–“æ’åºè§€å¯Ÿæ¯”è³½é€²ç¨‹ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **7. Filter data (ç¯©é¸è³‡æ–™)**ï¼šå…è¨±åŸºæ–¼ç‰¹å®šæ¢ä»¶é¸æ“‡è³‡æ–™å­é›†ï¼Œä¾‹å¦‚åªé¸æ“‡ç‰¹å®šçƒå“¡çš„è³‡æ–™ï¼Œæˆ–æ˜¯ç¯©é¸å‡ºç‰¹å®šé¡å‹çš„æ“Šçƒï¼Œéå¸¸æœ‰ç”¨ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **8. Group data (åˆ†çµ„è³‡æ–™)**ï¼šå°‡è³‡æ–™æŒ‰ç‰¹å®šæ¬„ä½ï¼ˆä¾‹å¦‚çƒå“¡ã€æ¯”è³½ã€æ“Šçƒé¡å‹ï¼‰é€²è¡Œåˆ†çµ„ï¼Œä»¥ä¾¿é€²è¡Œå½™ç¸½å’Œæ¯”è¼ƒï¼Œå¸¸ç”¨æ–¼åˆ†æä¸åŒçƒå“¡çš„è¡¨ç¾å·®ç•°ã€ä¸åŒæ“Šçƒé¡å‹çš„æ•ˆæœç­‰ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **9. Aggregate data (å½™ç¸½è³‡æ–™)**ï¼šå°åˆ†çµ„å¾Œçš„è³‡æ–™é€²è¡Œå½™ç¸½è¨ˆç®—ï¼Œä¾‹å¦‚è¨ˆç®—æ¯å€‹çƒå“¡çš„å¹³å‡å¾—åˆ†ã€æ¯ç¨®æ“Šçƒé¡å‹çš„æˆåŠŸç‡ç­‰ã€‚ é€šå¸¸å’Œ group data ä¸€èµ·ä½¿ç”¨ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **10. Check/handle missing values (æª¢æŸ¥/è™•ç†ç¼ºå¤±å€¼)**ï¼šæœ¬è³ªä¸Šèˆ‡æ“ä½œ 4 ç›¸åŒï¼Œä½œç”¨é‡è¤‡ï¼Œå¯ä»¥åˆªé™¤ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **11. Count (è¨ˆæ•¸)**ï¼šçµ±è¨ˆå„æ¬„ä½ä¸­å”¯ä¸€å€¼çš„æ•¸é‡ï¼Œæˆ–è€…ç‰¹å®šå€¼çš„å‡ºç¾æ¬¡æ•¸ï¼Œä¾‹å¦‚ï¼Œçµ±è¨ˆæ¯ç¨®æ“Šçƒé¡å‹çš„æ¬¡æ•¸ï¼Œæˆ–æ˜¯æ¯å€‹çƒå“¡çš„å¾—åˆ†æ¬¡æ•¸ï¼Œæ¯”è¼ƒå¯¦ç”¨ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: åŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œä»¥ä¸‹æ“ä½œæ‡‰è©²ç§»é™¤ï¼š\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: *   **3. Delete column**: é›–ç„¶ `db` æ¬„ä½å¯ä»¥åˆªé™¤ï¼Œä½†é€™å€‹æ“ä½œæœ¬èº«ä¸¦ä¸æ˜¯çµ•å°å¿…é ˆçš„ï¼Œå¯ä»¥å…ˆä¸åŸ·è¡Œã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: *   **6. Sort data**: é›–ç„¶å…·æœ‰ä¸€å®šçš„å¯¦ç”¨æ€§ï¼Œä½†ä¸å¦‚å…¶ä»–æ“ä½œé‡è¦ã€‚å¦‚æœç›®æ¨™ä¸æ˜¯æŒ‰æ™‚é–“é †åºåˆ†ææ¯”è³½éç¨‹ï¼Œå¯ä»¥å…ˆä¸åŸ·è¡Œã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: *   **8. Group data**: å¦‚æœæ²’æœ‰æ˜ç¢ºçš„åˆ†æç›®æ¨™ï¼Œä¾‹å¦‚è¦åˆ†æç‰¹å®šçƒå“¡æˆ–æ¯”è³½ï¼Œé€™å€‹æ“ä½œçš„åƒ¹å€¼æœƒé™ä½ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: *   **9. Aggregate data**: éœ€è¦å…ˆåˆ†çµ„è³‡æ–™æ‰èƒ½é€²è¡Œå½™ç¸½ï¼Œå› æ­¤ç§»é™¤ group data å¾Œï¼Œé€™å€‹æ“ä½œä¹Ÿå¤±å»æ„ç¾©ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: *   **10. Check/handle missing values**: èˆ‡æ“ä½œ 4 é‡è¤‡ã€‚\n",
      "Debug - æª¢æŸ¥è¡Œ: \n",
      "Debug - æª¢æŸ¥è¡Œ: ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
      "Debug - æ‰¾åˆ°æ¨™é¡Œè¡Œï¼Œé–‹å§‹æå–\n",
      "Debug - æª¢æŸ¥è¡Œ: [1, 2, 4, 5, 7, 11]\n",
      "Debug - æ‰¾åˆ°æ–¹æ‹¬è™Ÿå…§å®¹: 1, 2, 4, 5, 7, 11\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 1\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 2\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 4\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 5\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 7\n",
      "Debug - æå–åˆ°ç·¨è™Ÿ: 11\n",
      "Debug - åŸå§‹æå–çš„ç·¨è™Ÿ: ['1', '2', '4', '5', '7', '11']\n",
      "Debug - å¯ç”¨çš„ç·¨è™Ÿ: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "Debug - é©—è­‰å¾Œçš„æœ‰æ•ˆç·¨è™Ÿ: ['1', '2', '4', '5', '7', '11']\n",
      "è‡ªå®šç¾©éæ¿¾å™¨æ–¹æ³•ä¿ç•™äº† 6 å€‹æ“ä½œç·¨è™Ÿ\n",
      "\n",
      "è‡ªå®šç¾©éæ¿¾å™¨å›æ‡‰:\n",
      "==============================\n",
      "åˆ†ææ€è·¯ï¼š\n",
      "\n",
      "é¦–å…ˆï¼Œå¯©è¦–æ¯å€‹æ“ä½œåœ¨ç¾½çƒæ¯”è³½è³‡æ–™åˆ†æä¸­çš„ä½œç”¨å’Œå¿…è¦æ€§ï¼Œçµåˆè³‡æ–™é›†çš„æ¬„ä½è³‡è¨Šé€²è¡Œåˆ¤æ–·ã€‚\n",
      "\n",
      "*   **1. View data (æŸ¥çœ‹è³‡æ–™)**ï¼šé€™æ˜¯ä»»ä½•è³‡æ–™åˆ†æçš„ç¬¬ä¸€æ­¥ï¼Œå¿…é ˆä¿ç•™ï¼Œç”¨æ–¼åˆæ­¥äº†è§£è³‡æ–™é›†ã€‚\n",
      "\n",
      "*   **2. Add column (æ–°å¢æ¬„ä½)**ï¼šéå¸¸æœ‰ç”¨ï¼Œå¯ä»¥åŸºæ–¼ç¾æœ‰æ¬„ä½å‰µå»ºæ–°çš„ç‰¹å¾µï¼Œä¾‹å¦‚è¨ˆç®—çƒé€Ÿã€æ“Šçƒè§’åº¦ã€å›åˆæŒçºŒæ™‚é–“ï¼Œç”šè‡³ä½¿ç”¨ç¾æœ‰çš„ `win_reason` å’Œ `lose_reason` æ¬„ä½å‰µå»ºä¸€å€‹ `is_win` æ¬„ä½ã€‚\n",
      "\n",
      "*   **3. Delete column (åˆªé™¤æ¬„ä½)**ï¼š å¦‚æœå­˜åœ¨ç„¡æ„ç¾©æˆ–é«˜åº¦å†—é¤˜çš„æ¬„ä½ï¼Œå‰‡å¾ˆæœ‰ç”¨ï¼Œä½†éœ€è¦å°å¿ƒæ“ä½œï¼Œé¿å…èª¤åˆªé‡è¦è³‡è¨Šã€‚`db` æ¬„ä½åªæœ‰ä¸€å€‹å€¼ 0ï¼Œæ²’æœ‰æä¾›ä»»ä½•è³‡è¨Šï¼Œå¯ä»¥åˆªé™¤ã€‚\n",
      "\n",
      "*   **4. Fill missing values (å¡«è£œç¼ºå¤±å€¼)**ï¼šé‘‘æ–¼ `lose_reason`, `win_reason`, å’Œ `getpoint_player` æ¬„ä½å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆnanï¼‰ï¼Œæ­¤æ“ä½œéå¸¸é‡è¦ï¼Œå¯ä»¥é¿å…å¾ŒçºŒåˆ†æç”¢ç”ŸéŒ¯èª¤ã€‚\n",
      "\n",
      "*   **5. Column statistics (æ¬„ä½çµ±è¨ˆ)**ï¼š...\n",
      "==============================\n",
      "æ–¹æ³•2: ç›´æ¥ä½¿ç”¨ Gemini API...\n",
      "ç›´æ¥ API æ–¹æ³•ä¿ç•™äº† 6 å€‹æ“ä½œç·¨è™Ÿ\n",
      "\n",
      "Gemini å›æ‡‰:\n",
      "==================================================\n",
      "åˆ†ææ€è·¯ï¼š\n",
      "\n",
      "1. **ä¿ç•™åŸºç¤è³‡æ–™æª¢è¦– (1):** é€™æ˜¯åˆæ­¥ç†è§£è³‡æ–™é›†çš„å¿…è¦æ­¥é©Ÿï¼Œä¸è«–å¾ŒçºŒè¦é€²è¡Œä»€éº¼åˆ†æï¼Œéƒ½éœ€è¦å…ˆäº†è§£è³‡æ–™çš„çµæ§‹èˆ‡å…§å®¹ã€‚\n",
      "\n",
      "2. **ä¿ç•™æ–°å¢æ¬„ä½ (2):** é€™å€‹æ“ä½œæä¾›äº†å¾ˆé«˜çš„å½ˆæ€§ï¼Œå¯ä»¥åŸºæ–¼ç¾æœ‰æ¬„ä½å‰µé€ æ›´æœ‰æ„ç¾©çš„ç‰¹å¾µï¼Œä¾‹å¦‚è¨ˆç®—çƒå“¡çš„æ“Šçƒæ•ˆç‡ã€åˆ¤æ–·æ˜¯å¦å¾—åˆ†ç­‰ã€‚\n",
      "\n",
      "3. **ä¿ç•™å¡«è£œéºæ¼å€¼ (4):** éºæ¼å€¼æ˜¯å¸¸è¦‹çš„è³‡æ–™å•é¡Œï¼Œè‹¥è¦é€²è¡Œæœ‰æ•ˆçš„çµ±è¨ˆæˆ–å»ºæ¨¡ï¼Œéœ€è¦å…ˆé€²è¡Œè™•ç†ã€‚\n",
      "\n",
      "4. **ä¿ç•™è³‡æ–™ç¯©é¸ (7):** ç¯©é¸èƒ½å¤ è®“æˆ‘å€‘é‡å°ç‰¹å®šæƒ…å¢ƒé€²è¡Œåˆ†æï¼Œä¾‹å¦‚åªè§€å¯Ÿç‰¹å®šçƒå“¡çš„è¡¨ç¾ã€æˆ–ç‰¹å®šåˆ†æ•¸æ®µçš„ä¾†å›çƒã€‚\n",
      "\n",
      "5. **ä¿ç•™è³‡æ–™åˆ†çµ„ (8):** åˆ†çµ„æ˜¯é€²è¡Œæ•¸æ“šåˆ†æçš„é‡è¦æ‰‹æ®µï¼Œé€éæŒ‰çƒå“¡ã€æ¯”è³½æˆ–å…¶ä»–é—œéµè®Šæ•¸é€²è¡Œåˆ†çµ„ï¼Œå¯ä»¥æ´å¯Ÿæ•¸æ“šçš„å„å€‹é¢å‘ã€‚\n",
      "\n",
      "6. **ä¿ç•™è¨ˆæ•¸ (11):** é€™å€‹æ“ä½œå¯ä»¥å¿«é€Ÿäº†è§£å„å€‹é¡åˆ¥çš„åˆ†å¸ƒç‹€æ³ï¼Œä¾‹å¦‚å„é¡å‹æ“Šçƒçš„æ¬¡æ•¸ã€æ¯å€‹çƒå“¡å¾—åˆ†çš„æ¬¡æ•¸ç­‰ç­‰ï¼Œæœ‰åŠ©æ–¼åˆæ­¥äº†è§£è³‡æ–™çš„ç‰¹æ€§ã€‚\n",
      "\n",
      "*   **ç§»é™¤åŸå› ï¼š**\n",
      "\n",
      "    *   **æ“ä½œ 3ï¼ˆåˆªé™¤æ¬„ä½ï¼‰ï¼š** åœ¨é€™å€‹éšæ®µï¼Œé™¤éæœ‰æ˜é¡¯ç„¡ç”¨æˆ–æ´©æ¼ç›®æ¨™è®Šæ•¸çš„æ¬„ä½ï¼Œå¦å‰‡æ‡‰ç›¡å¯èƒ½ä¿ç•™æ‰€æœ‰æ¬„ä½ï¼Œä»¥ä¾¿å¾ŒçºŒç‰¹å¾µå·¥ç¨‹ã€‚\n",
      "    *   **æ“ä½œ 5ï¼ˆæ¬„ä½çµ±è¨ˆï¼‰ï¼š** æ“ä½œ 9ï¼ˆåŒ¯ç¸½æ•¸æ“šï¼‰å’Œ 11ï¼ˆè¨ˆæ•¸ï¼‰å¯ä»¥åŒ…å«çµ±è¨ˆåˆ†æã€‚æ­¤æ“ä½œèˆ‡æ“ä½œ 9 å’Œ 11 é«˜åº¦ç›¸ä¼¼ã€‚\n",
      "    *   **æ“ä½œ 6ï¼ˆæ’åºè³‡æ–™ï¼‰ï¼š** æ’åºæœ¬èº«ä¸¦ä¸æ˜¯åˆ†æçš„æ ¸å¿ƒï¼Œæ›´å¤šæ˜¯è¼”åŠ©æª¢è¦–è³‡æ–™çš„æ–¹å¼ï¼Œåœ¨å…¶ä»–æ“ä½œä¸­ä¹Ÿå¸¸å¸¸æœƒç”¨åˆ°æ’åºã€‚\n",
      "    *   **æ“ä½œ 9ï¼ˆåŒ¯ç¸½è³‡æ–™ï¼‰ï¼š** é€™å€‹æ“ä½œåŠŸèƒ½é‡è¤‡ï¼Œå› ç‚ºçµ±è¨ˆåˆ†æå¯ä»¥é€éåˆ†çµ„å’Œæ–°å¢æ¬„ä½ä¾†å®Œæˆã€‚\n",
      "    *   **æ“ä½œ 10ï¼ˆæª¢æŸ¥/è™•ç†éºæ¼å€¼ï¼‰ï¼š** é›–ç„¶æª¢æŸ¥éºæ¼å€¼å¾ˆé‡è¦ï¼Œä½†æ›´é‡è¦çš„æ˜¯å¯¦éš›å¡«è£œéºæ¼å€¼ (æ“ä½œ 4)ã€‚å–®ç´”æª¢æŸ¥è€Œä¸è™•ç†çš„å¯¦ç”¨åƒ¹å€¼è¼ƒä½ã€‚\n",
      "\n",
      "ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
      "[1, 2, 4, 7, 8, 11]\n",
      "\n",
      "==================================================\n",
      "\n",
      "ä½¿ç”¨ç›´æ¥ API æ–¹æ³•çš„çµæœ\n",
      "\n",
      "æœ€çµ‚ä¿ç•™çš„æ“ä½œç·¨è™Ÿ (6 å€‹):\n",
      "\n",
      "æ“ä½œéæ¿¾å®Œæˆï¼å¾ 11 å€‹æ“ä½œæ¸›å°‘åˆ° 6 å€‹æ“ä½œã€‚\n",
      "ä¿ç•™çš„æ“ä½œç·¨è™Ÿæ¸…å–®: ['1', '2', '4', '7', '8', '11']\n",
      "selected_numbers: [1, 2, 4, 7, 8, 11]\n",
      "original_operations: [{'number': 1, 'name': 'View data', 'description': 'Inspect dataset preview and structure.'}, {'number': 2, 'name': 'Add column', 'description': 'Add new features, e.g., rally length or is_win.'}, {'number': 3, 'name': 'Delete column', 'description': 'Remove irrelevant or noisy fields.'}, {'number': 4, 'name': 'Fill missing values', 'description': 'Replace NaN with default or computed values.'}, {'number': 5, 'name': 'Column statistics', 'description': 'Compute stats like average rally time or shot count.'}, {'number': 6, 'name': 'Sort data', 'description': 'Sort by duration, player, or score for ranking or filtering.'}, {'number': 7, 'name': 'Filter data', 'description': 'Use logical conditions to narrow down the dataset.'}, {'number': 8, 'name': 'Group data', 'description': 'Cluster data by player, match, or shot type for summarization.'}, {'number': 9, 'name': 'Aggregate data', 'description': 'Summarize grouped data via average, count, etc.'}, {'number': 10, 'name': 'Check/handle missing values', 'description': 'Diagnose and manage incomplete data.'}, {'number': 11, 'name': 'Count', 'description': 'Count unique values or occurrences in columns.'}]\n",
      "filtered_operations.json has been created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Define Gemini LLM class (same as before)\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "    \n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "        \n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "        \n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{\n",
    "                'text': response.text,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "    \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "# Set up Gemini API\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def load_operations_from_json(json_file_path):\n",
    "    \"\"\"\n",
    "    Load operations from JSON file\n",
    "    Returns a list of operation dictionaries and a formatted string list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        operations_data = data.get('operations', [])\n",
    "        \n",
    "        # Create formatted operation strings for LLM processing\n",
    "        operation_strings = []\n",
    "        operation_details = []\n",
    "        \n",
    "        for op in operations_data:\n",
    "            number = op.get('number', '')\n",
    "            name = op.get('name', '')\n",
    "            description = op.get('description', '')\n",
    "            \n",
    "            # Format as: \"number. name: description\"\n",
    "            if number and name and description:\n",
    "                formatted_op = f\"{number}. {name}: {description}\"\n",
    "                operation_strings.append(formatted_op)\n",
    "                operation_details.append({\n",
    "                    'number': number,\n",
    "                    'name': name,\n",
    "                    'description': description,\n",
    "                    'formatted': formatted_op\n",
    "                })\n",
    "        \n",
    "        print(f\"å¾ {json_file_path} æˆåŠŸè¼‰å…¥ {len(operation_strings)} å€‹æ“ä½œ\")\n",
    "        return operation_details, operation_strings\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_file_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"éŒ¯èª¤: {json_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return [], []\n",
    "\n",
    "class OperationFilter:\n",
    "    def __init__(self, gemini_api_key):\n",
    "        self.gemini_lm = Gemini(api_key=gemini_api_key)\n",
    "    \n",
    "    def forward(self, operations_list, operation_details, data_info, removal_percentage=0.25):\n",
    "        \"\"\"\n",
    "        Use Gemini to filter out inappropriate operations\n",
    "        \"\"\"\n",
    "        operations_count = len(operations_list)\n",
    "        operations_to_remove = int(operations_count * removal_percentage)\n",
    "        operations_to_keep = operations_count - operations_to_remove\n",
    "        \n",
    "        # Create numbered list of operations for easier reference\n",
    "        numbered_operations = \"\\n\".join([f\"{i+1}. {op}\" for i, op in enumerate(operations_list)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        æˆ‘æœ‰ {operations_count} å€‹ç”¨æ–¼åˆ†æç¾½çƒæ¯”è³½è³‡æ–™çš„æ“ä½œï¼š\n",
    "\n",
    "        æ“ä½œæ¸…å–®ï¼š\n",
    "        {numbered_operations}\n",
    "\n",
    "        è³‡æ–™é›†è³‡è¨Šï¼š\n",
    "        {data_info}\n",
    "\n",
    "        æˆ‘éœ€è¦ç§»é™¤ {operations_to_remove} å€‹æœ€ä¸é©åˆæˆ–æœ€ä¸ç›¸é—œçš„æ“ä½œï¼ˆç´„ {removal_percentage*100:.0f}%ï¼‰ã€‚\n",
    "\n",
    "        è«‹åŸºæ–¼ä»¥ä¸‹æ¨™æº–åˆ†ææ¯å€‹æ“ä½œä¸¦è­˜åˆ¥æ‡‰è©²ç§»é™¤å“ªäº›æ“ä½œï¼š\n",
    "        1. èˆ‡å¯¦éš›å¯ç”¨è³‡æ–™æ¬„ä½çš„ç›¸é—œæ€§\n",
    "        2. åœ¨çµ¦å®šè³‡æ–™é›†çµæ§‹ä¸‹çš„å¯è¡Œæ€§  \n",
    "        3. å°ç¾½çƒæ¯”è³½åˆ†æçš„å¯¦ç”¨åƒ¹å€¼\n",
    "        4. é¿å…é‡è¤‡æˆ–éå¸¸ç›¸ä¼¼çš„æ“ä½œ\n",
    "\n",
    "        è«‹ä¿ç•™ {operations_to_keep} å€‹æœ€é©åˆçš„æ“ä½œä¸¦æ¸…æ¥šåˆ—å‡ºå®ƒå€‘çš„ç·¨è™Ÿã€‚\n",
    "        \n",
    "        è«‹é€æ­¥æ€è€ƒç‚ºä»€éº¼æŸäº›æ“ä½œæ‡‰è©²è¢«ç§»é™¤ï¼Œç„¶å¾Œæä¾›è¦ä¿ç•™æ“ä½œçš„ç·¨è™Ÿæ¸…å–®ã€‚\n",
    "\n",
    "        è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "        åˆ†ææ€è·¯ï¼š\n",
    "        [ä½ çš„åˆ†æ]\n",
    "\n",
    "        ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
    "        [ç·¨è™Ÿ1, ç·¨è™Ÿ2, ç·¨è™Ÿ3, ...]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.gemini_lm.basic_request(prompt)\n",
    "        print(f\"response: {response}\")\n",
    "        # Extract the kept operation numbers from the result\n",
    "        kept_operation_numbers = self.extract_kept_operation_numbers(response, operation_details)\n",
    "        \n",
    "        return kept_operation_numbers, response\n",
    "    \n",
    "    def extract_kept_operation_numbers(self, filtered_text, operation_details):\n",
    "        \"\"\"\n",
    "        Extract the operation numbers that should be kept from the LLM response\n",
    "        \"\"\"\n",
    "        kept_numbers = []\n",
    "        \n",
    "        print(f\"Debug - æ­£åœ¨åˆ†æå›æ‡‰æ–‡æœ¬...\")\n",
    "        \n",
    "        # Look for the section with kept operation numbers\n",
    "        lines = filtered_text.split('\\n')\n",
    "        \n",
    "        # Find the start of the operations list\n",
    "        start_extracting = False\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            print(f\"Debug - æª¢æŸ¥è¡Œ: {line}\")\n",
    "            \n",
    "            # Look for section headers that indicate the start of the kept operations list\n",
    "            if any(keyword in line.lower() for keyword in ['ä¿ç•™çš„æ“ä½œç·¨è™Ÿ', 'ä¿ç•™æ“ä½œç·¨è™Ÿ', 'ç·¨è™Ÿ', 'numbers']):\n",
    "                start_extracting = True\n",
    "                print(f\"Debug - æ‰¾åˆ°æ¨™é¡Œè¡Œï¼Œé–‹å§‹æå–\")\n",
    "                # æª¢æŸ¥æ¨™é¡Œè¡Œæœ¬èº«æ˜¯å¦åŒ…å«æ•¸å­—\n",
    "                bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "                if bracket_match:\n",
    "                    numbers_text = bracket_match.group(1)\n",
    "                    print(f\"Debug - åœ¨æ¨™é¡Œè¡Œæ‰¾åˆ°æ–¹æ‹¬è™Ÿå…§å®¹: {numbers_text}\")\n",
    "                    # Split by comma and extract numbers\n",
    "                    for item in numbers_text.split(','):\n",
    "                        number = re.search(r'(\\d+)', item.strip())\n",
    "                        if number:\n",
    "                            kept_numbers.append(number.group(1))\n",
    "                            print(f\"Debug - æå–åˆ°ç·¨è™Ÿ: {number.group(1)}\")\n",
    "                    if kept_numbers:  # å¦‚æœåœ¨æ¨™é¡Œè¡Œæ‰¾åˆ°æ•¸å­—ï¼Œå°±ä¸éœ€è¦ç¹¼çºŒäº†\n",
    "                        break\n",
    "                continue\n",
    "            \n",
    "            if start_extracting and line:\n",
    "                # Try to extract numbers from various formats\n",
    "                # Format 1: [ç·¨è™Ÿ1, ç·¨è™Ÿ2, ...]\n",
    "                bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "                if bracket_match:\n",
    "                    numbers_text = bracket_match.group(1)\n",
    "                    print(f\"Debug - æ‰¾åˆ°æ–¹æ‹¬è™Ÿå…§å®¹: {numbers_text}\")\n",
    "                    # Split by comma and extract numbers\n",
    "                    for item in numbers_text.split(','):\n",
    "                        number = re.search(r'(\\d+)', item.strip())\n",
    "                        if number:\n",
    "                            kept_numbers.append(number.group(1))\n",
    "                            print(f\"Debug - æå–åˆ°ç·¨è™Ÿ: {number.group(1)}\")\n",
    "                    break\n",
    "                \n",
    "                # Format 2: Numbered list or comma-separated numbers\n",
    "                numbers = re.findall(r'\\b(\\d+)\\b', line)\n",
    "                if numbers:\n",
    "                    kept_numbers.extend(numbers)\n",
    "                    print(f\"Debug - å¾è¡Œä¸­æå–åˆ°ç·¨è™Ÿ: {numbers}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Debug - åŸå§‹æå–çš„ç·¨è™Ÿ: {kept_numbers}\")\n",
    "        \n",
    "        # Remove duplicates and validate against available operations\n",
    "        valid_numbers = []\n",
    "        available_numbers = [str(detail['number']) for detail in operation_details]  # ç¢ºä¿éƒ½æ˜¯å­—ç¬¦ä¸²\n",
    "        \n",
    "        print(f\"Debug - å¯ç”¨çš„ç·¨è™Ÿ: {available_numbers}\")\n",
    "        \n",
    "        for num in kept_numbers:\n",
    "            num_str = str(num)  # ç¢ºä¿æ˜¯å­—ç¬¦ä¸²\n",
    "            if num_str in available_numbers and num_str not in valid_numbers:\n",
    "                valid_numbers.append(num_str)\n",
    "        \n",
    "        print(f\"Debug - é©—è­‰å¾Œçš„æœ‰æ•ˆç·¨è™Ÿ: {valid_numbers}\")\n",
    "        return valid_numbers\n",
    "\n",
    "def filter_operations_direct_gemini(api_key, operations_list, operation_details, data_sample, data_info, removal_percentage=0.25):\n",
    "    \"\"\"\n",
    "    Use Gemini API directly to filter operations and return operation numbers\n",
    "    \"\"\"\n",
    "    gemini_lm = Gemini(api_key=api_key)\n",
    "    \n",
    "    operations_count = len(operations_list)\n",
    "    operations_to_remove = int(operations_count * removal_percentage)\n",
    "    operations_to_keep = operations_count - operations_to_remove\n",
    "    \n",
    "    # Create numbered list of operations for easier reference\n",
    "    numbered_operations = \"\\n\".join([f\"{i+1}. {op}\" for i, op in enumerate(operations_list)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    æˆ‘æœ‰ä¸€å€‹ç¾½çƒæ¯”è³½çš„è³‡æ–™é›†å’Œ {operations_count} å€‹åˆ†ææ“ä½œã€‚\n",
    "\n",
    "    è³‡æ–™æ¨£æœ¬:\n",
    "    {data_sample}\n",
    "\n",
    "    è³‡æ–™é›†è³‡è¨Š:\n",
    "    {data_info}\n",
    "\n",
    "    æ“ä½œæ¸…å–®:\n",
    "    {numbered_operations}\n",
    "\n",
    "    è«‹å¹«æˆ‘åˆ†æä¸¦ç§»é™¤ {operations_to_remove} å€‹æœ€ä¸åˆé©çš„æ“ä½œï¼ˆç´„ {removal_percentage*100:.0f}%ï¼‰ï¼Œä¿ç•™ {operations_to_keep} å€‹æœ€é©åˆçš„æ“ä½œã€‚\n",
    "\n",
    "    è«‹è€ƒæ…®ä»¥ä¸‹æ¨™æº–ä¾†æ±ºå®šç§»é™¤å“ªäº›æ“ä½œï¼š\n",
    "    1. èˆ‡å¯¦éš›è³‡æ–™æ¬„ä½çš„ç›¸é—œæ€§\n",
    "    2. åœ¨çµ¦å®šè³‡æ–™é›†çµæ§‹ä¸‹çš„å¯è¡Œæ€§\n",
    "    3. å°ç¾½çƒæ¯”è³½åˆ†æçš„å¯¦ç”¨åƒ¹å€¼\n",
    "    4. é¿å…é‡è¤‡æˆ–éæ–¼ç›¸ä¼¼çš„æ“ä½œ\n",
    "\n",
    "    è«‹å…ˆèªªæ˜ä½ çš„åˆ†ææ€è·¯ï¼Œç„¶å¾Œ**åªæä¾›è¦ä¿ç•™æ“ä½œçš„ç·¨è™Ÿ**ï¼ˆå¾æ“ä½œæè¿°é–‹é ­æå–çš„ç·¨è™Ÿï¼‰ã€‚\n",
    "\n",
    "    è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "\n",
    "    åˆ†ææ€è·¯ï¼š\n",
    "    [ä½ çš„åˆ†æ]\n",
    "\n",
    "    ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
    "    [ç·¨è™Ÿ1, ç·¨è™Ÿ2, ç·¨è™Ÿ3, ...]\n",
    "\n",
    "    ç¢ºä¿åªåˆ—å‡ºè¦ä¿ç•™çš„ {operations_to_keep} å€‹æ“ä½œçš„ç·¨è™Ÿã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    response = gemini_lm.basic_request(prompt)\n",
    "    \n",
    "    # Extract kept operation numbers from the response\n",
    "    kept_operation_numbers = extract_operation_numbers_from_response(response, operation_details)\n",
    "    \n",
    "    return kept_operation_numbers, response\n",
    "\n",
    "def extract_operation_numbers_from_response(response, operation_details):\n",
    "    \"\"\"\n",
    "    Extract the operation numbers to keep from Gemini's response\n",
    "    \"\"\"\n",
    "    kept_numbers = []\n",
    "    \n",
    "    #print(f\"Debug - æ­£åœ¨åˆ†æå›æ‡‰æ–‡æœ¬...\")\n",
    "    \n",
    "    # Look for the section with kept operation numbers\n",
    "    lines = response.split('\\n')\n",
    "    \n",
    "    # Find the start of the operations list\n",
    "    start_extracting = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "       # print(f\"Debug - æª¢æŸ¥è¡Œ: {line}\")\n",
    "        \n",
    "        # Look for section headers that indicate the start of the kept operations list\n",
    "        if any(keyword in line.lower() for keyword in ['ä¿ç•™çš„æ“ä½œç·¨è™Ÿ', 'ä¿ç•™æ“ä½œç·¨è™Ÿ', 'ç·¨è™Ÿ', 'numbers']):\n",
    "            start_extracting = True\n",
    "            #print(f\"Debug - æ‰¾åˆ°æ¨™é¡Œè¡Œï¼Œé–‹å§‹æå–\")\n",
    "            # æª¢æŸ¥æ¨™é¡Œè¡Œæœ¬èº«æ˜¯å¦åŒ…å«æ•¸å­—\n",
    "            bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if bracket_match:\n",
    "                numbers_text = bracket_match.group(1)\n",
    "                #print(f\"Debug - åœ¨æ¨™é¡Œè¡Œæ‰¾åˆ°æ–¹æ‹¬è™Ÿå…§å®¹: {numbers_text}\")\n",
    "                # Split by comma and extract numbers\n",
    "                for item in numbers_text.split(','):\n",
    "                    number = re.search(r'(\\d+)', item.strip())\n",
    "                    if number:\n",
    "                        kept_numbers.append(number.group(1))\n",
    "                        #print(f\"Debug - æå–åˆ°ç·¨è™Ÿ: {number.group(1)}\")\n",
    "                if kept_numbers:  # å¦‚æœåœ¨æ¨™é¡Œè¡Œæ‰¾åˆ°æ•¸å­—ï¼Œå°±ä¸éœ€è¦ç¹¼çºŒäº†\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        if start_extracting and line:\n",
    "            # Try to extract numbers from various formats\n",
    "            # Format 1: [ç·¨è™Ÿ1, ç·¨è™Ÿ2, ...]\n",
    "            bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if bracket_match:\n",
    "                numbers_text = bracket_match.group(1)\n",
    "                #print(f\"Debug - æ‰¾åˆ°æ–¹æ‹¬è™Ÿå…§å®¹: {numbers_text}\")\n",
    "                # Split by comma and extract numbers\n",
    "                for item in numbers_text.split(','):\n",
    "                    number = re.search(r'(\\d+)', item.strip())\n",
    "                    if number:\n",
    "                        kept_numbers.append(number.group(1))\n",
    "                       # print(f\"Debug - æå–åˆ°ç·¨è™Ÿ: {number.group(1)}\")\n",
    "                break\n",
    "            \n",
    "            # Format 2: Numbered list or comma-separated numbers\n",
    "            numbers = re.findall(r'\\b(\\d+)\\b', line)\n",
    "            if numbers:\n",
    "                kept_numbers.extend(numbers)\n",
    "                #print(f\"Debug - å¾è¡Œä¸­æå–åˆ°ç·¨è™Ÿ: {numbers}\")\n",
    "                break\n",
    "    \n",
    "    #print(f\"Debug - åŸå§‹æå–çš„ç·¨è™Ÿ: {kept_numbers}\")\n",
    "    \n",
    "    # Remove duplicates and validate against available operations\n",
    "    valid_numbers = []\n",
    "    available_numbers = [str(detail['number']) for detail in operation_details]  # ç¢ºä¿éƒ½æ˜¯å­—ç¬¦ä¸²\n",
    "    \n",
    "    #print(f\"Debug - å¯ç”¨çš„ç·¨è™Ÿ: {available_numbers}\")\n",
    "    \n",
    "    for num in kept_numbers:\n",
    "        num_str = str(num)  # ç¢ºä¿æ˜¯å­—ç¬¦ä¸²\n",
    "        if num_str in available_numbers and num_str not in valid_numbers:\n",
    "            valid_numbers.append(num_str)\n",
    "    \n",
    "    #print(f\"Debug - é©—è­‰å¾Œçš„æœ‰æ•ˆç·¨è™Ÿ: {valid_numbers}\")\n",
    "    return valid_numbers\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataset\n",
    "    \"\"\"\n",
    "    summary = f\"\"\"\n",
    "    è³‡æ–™é›†æ¦‚è¦:\n",
    "    - ç¸½è¡Œæ•°: {dataframe.shape[0]}\n",
    "    - ç¸½åˆ—æ•°: {dataframe.shape[1]}\n",
    "    - æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\n",
    "    \n",
    "    å„æ¬„ä½è³‡è¨Š:\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        col_info = f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            unique_values = dataframe[col].unique()[:10]  # Show first 10 unique values\n",
    "            col_info += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, unique_values))}\"\n",
    "        else:\n",
    "            col_info += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\"\n",
    "        \n",
    "        summary += col_info + \"\\n\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def filter_badminton_operations(operations_list, operation_details, dataframe, api_key, removal_percentage=0.25):\n",
    "    \"\"\"\n",
    "    Main function to filter operations using Gemini LLM and return operation numbers\n",
    "    \"\"\"\n",
    "    print(f\"åŸå§‹æ“ä½œæ•¸é‡: {len(operations_list)}\")\n",
    "    print(\"åŸå§‹æ“ä½œæ¸…å–®:\")\n",
    "    for i, op in enumerate(operations_list, 1):\n",
    "        print(f\"  {i}. {op}\")\n",
    "    \n",
    "    # Get data summary\n",
    "    data_summary = get_data_summary(dataframe)\n",
    "    data_sample = dataframe.to_string()  # Use only first 5 rows for sample\n",
    "    \n",
    "    print(f\"\\nä½¿ç”¨ Gemini LLM éæ¿¾æ“ä½œ (ç§»é™¤ {removal_percentage*100:.0f}%)...\")\n",
    "    \n",
    "    # Method 1: Use custom filter class with Gemini\n",
    "    try:\n",
    "        print(\"æ–¹æ³•1: ä½¿ç”¨è‡ªå®šç¾©éæ¿¾å™¨ + Gemini...\")\n",
    "        filter_module = OperationFilter(api_key)\n",
    "        dspy_filtered_numbers, dspy_response = filter_module.forward(\n",
    "            operations_list, \n",
    "            operation_details,\n",
    "            data_summary, \n",
    "            removal_percentage\n",
    "        )\n",
    "        print(f\"è‡ªå®šç¾©éæ¿¾å™¨æ–¹æ³•ä¿ç•™äº† {len(dspy_filtered_numbers)} å€‹æ“ä½œç·¨è™Ÿ\")\n",
    "        \n",
    "        print(\"\\nè‡ªå®šç¾©éæ¿¾å™¨å›æ‡‰:\")\n",
    "        print(\"=\"*30)\n",
    "        print(dspy_response[:500] + \"...\" if len(dspy_response) > 500 else dspy_response)\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è‡ªå®šç¾©éæ¿¾å™¨æ–¹æ³•å¤±æ•—: {e}\")\n",
    "        dspy_filtered_numbers = []\n",
    "    \n",
    "    # Method 2: Direct Gemini API call\n",
    "    try:\n",
    "        print(\"æ–¹æ³•2: ç›´æ¥ä½¿ç”¨ Gemini API...\")\n",
    "        direct_filtered_numbers, gemini_response = filter_operations_direct_gemini(\n",
    "            api_key, \n",
    "            operations_list, \n",
    "            operation_details,\n",
    "            data_sample, \n",
    "            data_summary, \n",
    "            removal_percentage\n",
    "        )\n",
    "        print(f\"ç›´æ¥ API æ–¹æ³•ä¿ç•™äº† {len(direct_filtered_numbers)} å€‹æ“ä½œç·¨è™Ÿ\")\n",
    "        \n",
    "        print(\"\\nGemini å›æ‡‰:\")\n",
    "        print(\"=\"*50)\n",
    "        print(gemini_response)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ç›´æ¥ API æ–¹æ³•å¤±æ•—: {e}\")\n",
    "        direct_filtered_numbers = []\n",
    "    \n",
    "    # Combine results (prefer direct method if both work, fallback to custom filter)\n",
    "    if direct_filtered_numbers:\n",
    "        final_operation_numbers = direct_filtered_numbers\n",
    "        print(f\"\\nä½¿ç”¨ç›´æ¥ API æ–¹æ³•çš„çµæœ\")\n",
    "    elif dspy_filtered_numbers:\n",
    "        final_operation_numbers = dspy_filtered_numbers\n",
    "        print(f\"\\nä½¿ç”¨è‡ªå®šç¾©éæ¿¾å™¨æ–¹æ³•çš„çµæœ\")\n",
    "    else:\n",
    "        # Fallback: keep random subset of operation numbers\n",
    "        target_count = int(len(operations_list) * (1 - removal_percentage))\n",
    "        final_operation_numbers = [detail['number'] for detail in operation_details[:target_count]]\n",
    "        print(f\"\\nå…©ç¨®æ–¹æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨å‰ {target_count} å€‹æ“ä½œç·¨è™Ÿä½œç‚ºå‚™æ¡ˆ\")\n",
    "    \n",
    "    print(f\"\\næœ€çµ‚ä¿ç•™çš„æ“ä½œç·¨è™Ÿ ({len(final_operation_numbers)} å€‹):\")\n",
    "    for i, number in enumerate(final_operation_numbers, 1):\n",
    "        # Find the operation details for display\n",
    "        for detail in operation_details:\n",
    "            if detail['number'] == number:\n",
    "                print(f\"  {i}. ç·¨è™Ÿ {number}: {detail['name']}\")\n",
    "                break\n",
    "    \n",
    "    return final_operation_numbers\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load operations from JSON file\n",
    "    json_file_path = \"operations.json\"\n",
    "    operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "    \n",
    "    if not operation_strings:\n",
    "        print(\"ç„¡æ³•è¼‰å…¥æ“ä½œï¼Œç¨‹å¼çµæŸ\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"\\nè¼‰å…¥çš„æ“ä½œè©³æƒ…:\")\n",
    "    for i, detail in enumerate(operation_details[:5], 1):  # Show first 5 as example\n",
    "        print(f\"  {i}. ç·¨è™Ÿ: {detail['number']}, åç¨±: {detail['name']}\")\n",
    "        print(f\"      æè¿°: {detail['description']}\")\n",
    "    \n",
    "    if len(operation_details) > 5:\n",
    "        print(f\"  ... é‚„æœ‰ {len(operation_details) - 5} å€‹æ“ä½œ\")\n",
    "    \n",
    "    # Load badminton data\n",
    "    try:\n",
    "        example_df = pd.read_csv('set1.csv')\n",
    "        print(f\"\\næˆåŠŸè¼‰å…¥è³‡æ–™é›†: {example_df.shape[0]} è¡Œ, {example_df.shape[1]} åˆ—\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"éŒ¯èª¤: æ‰¾ä¸åˆ° set1.csv æ–‡ä»¶\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥è³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Your Gemini API key\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    \n",
    "    # Filter operations and get operation numbers\n",
    "    filtered_operation_numbers = filter_badminton_operations(\n",
    "        operation_strings,  # Use the formatted strings for LLM processing\n",
    "        operation_details,  # Pass the operation details for number extraction\n",
    "        example_df, \n",
    "        api_key, \n",
    "        removal_percentage=0.2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\næ“ä½œéæ¿¾å®Œæˆï¼å¾ {len(operation_strings)} å€‹æ“ä½œæ¸›å°‘åˆ° {len(filtered_operation_numbers)} å€‹æ“ä½œã€‚\")\n",
    "    print(f\"ä¿ç•™çš„æ“ä½œç·¨è™Ÿæ¸…å–®: {filtered_operation_numbers}\")\n",
    "\n",
    "    # å¾ JSON æª”æ¡ˆè®€å– \"operations\" åˆ†æ”¯\n",
    "    with open(\"operations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = json.load(f)\n",
    "        original_operations = all_data[\"operations\"]\n",
    "\n",
    "    # ä½ æƒ³è¦æŒ‘é¸çš„ operation ç·¨è™Ÿï¼ˆæ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹é€™å€‹ listï¼‰\n",
    "    selected_numbers = [int(num) for num in filtered_operation_numbers]\n",
    "    print(f\"selected_numbers: {selected_numbers}\")\n",
    "    print(f\"original_operations: {original_operations}\")\n",
    "    # æ ¹æ“š selected_numbers é¸å‡ºå°æ‡‰æ“ä½œï¼Œä¸¦å¾ 1 é–‹å§‹é‡æ–°ç·¨è™Ÿ\n",
    "    filtered_operations = []\n",
    "    for new_number, original_number in enumerate(selected_numbers, start=1):\n",
    "        for op in original_operations:\n",
    "            if op[\"number\"] == original_number:\n",
    "                filtered_operations.append({\n",
    "                    \"number\": new_number,\n",
    "                    \"name\": op[\"name\"],\n",
    "                    \"description\": op[\"description\"]\n",
    "                })\n",
    "                break\n",
    "    #print(f\"filtered_operations: {filtered_operations}\")\n",
    "    # æ–°çš„ JSON çµæ§‹\n",
    "    output_json = {\n",
    "        \"description\": \"Selected operations for badminton data analysis.\",\n",
    "        \"requirements\": [\n",
    "            \"The output must be based on the input data ; do not hallucinate.\",\n",
    "            \"Give me the list of numbers.\"\n",
    "        ],\n",
    "        \"operations\": filtered_operations\n",
    "    }\n",
    "\n",
    "    # å¯«å…¥ JSON æª”æ¡ˆ\n",
    "    with open(\"filtered_operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"filtered_operations.json has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf4009",
   "metadata": {},
   "source": [
    "# STEP 6\n",
    "\n",
    "æ ¹æ“šçœŸå¯¦tableåªä¿ç•™é‡è¦70%æ“ä½œï¼Œä¿ç•™'write' 'select_col' 'select_row'ä¸‰å€‹é‡è¦æ“ä½œï¼Œåˆ°'selected_operations.json'\n",
    "\n",
    "æ“ä½œæå–å·²å®Œæˆ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da8caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations_info.json æˆåŠŸè¼‰å…¥ 15 å€‹æ“ä½œ\n",
      "operation_length: 15\n",
      "[1, 2, 4, 5, 3, 6, 7, 9, 10, 8, 13, 12, 14, 15, 11]\n",
      "response: å¥½çš„ï¼Œæˆ‘å°‡æ ¹æ“šæ‚¨çš„è¦æ±‚ï¼Œä½¿ç”¨éˆå¼æ€è€ƒçš„æ–¹å¼ï¼Œåˆ†æä¸¦æ’åºæ‚¨æä¾›çš„ 15 å€‹åˆ†ææ“ä½œï¼Œä¸¦è¼¸å‡ºä¸€å€‹ç”±æ“ä½œç·¨è™Ÿçµ„æˆçš„é™£åˆ—ï¼ŒæŒ‰ç…§é‡è¦æ€§ç”±é«˜åˆ°ä½æ’åˆ—ã€‚\n",
      "\n",
      "**éˆå¼æ€è€ƒåˆ†æï¼š**\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ä½œç‚ºä¸€å€‹è³‡æ·±ç¾½æ¯›çƒæ–°èè¨˜è€…ï¼Œè®€è€…æœ€æƒ³çœ‹åˆ°ä»€éº¼æ¨£çš„è³‡è¨Šï¼Œä»¥åŠå“ªäº›åˆ†ææ“ä½œèƒ½æä¾›é€™äº›è³‡è¨Šã€‚\n",
      "\n",
      "1.  **æ¯”è³½çµæœå’ŒåŸºæœ¬è³‡è¨Šï¼š** è®€è€…é¦–å…ˆé—œå¿ƒçš„æ˜¯èª°è´äº†ï¼Œä»¥åŠæ¯”è³½çš„ç¸½é«”æƒ…æ³ã€‚\n",
      "\n",
      "2.  **é—œéµçƒå“¡è¡¨ç¾å’Œæˆ°è¡“ï¼š** è®€è€…æœƒæƒ³çŸ¥é“é—œéµçƒå“¡åœ¨æ¯”è³½ä¸­çš„è¡¨ç¾å¦‚ä½•ï¼Œä½¿ç”¨äº†å“ªäº›æˆ°è¡“ï¼Œä»¥åŠé€™äº›æˆ°è¡“æ˜¯å¦æœ‰æ•ˆã€‚\n",
      "\n",
      "3.  **å¤±èª¤åˆ†æï¼š** åˆ†æå¤±èª¤çš„åŸå› å¯ä»¥å¹«åŠ©è®€è€…äº†è§£æ¯”è³½çš„è½‰æŠ˜é»å’Œçƒå“¡çš„å¼±é»ã€‚\n",
      "\n",
      "4.  **é€²éšåˆ†æï¼š** çµåˆæ™‚é–“ã€å›åˆå¾—åˆ†ç­‰å› ç´ ï¼Œåˆ†ææ¯”è³½çš„ç¯€å¥ã€çƒå“¡çš„é«”èƒ½è®ŠåŒ–ç­‰ã€‚\n",
      "\n",
      "åŸºæ–¼ä»¥ä¸Šæ€è€ƒï¼Œæˆ‘å°‡å„å€‹æ“ä½œçš„é‡è¦æ€§æ’åºå¦‚ä¸‹ï¼š\n",
      "\n",
      "*   **æ¥µé«˜é‡è¦æ€§ï¼š**\n",
      "\n",
      "    *   **1. write:** é€™å€‹æ“ä½œèƒ½å°‡åˆ†æçµæœè½‰åŒ–ç‚ºæ–°èå ±å°ï¼Œæ˜¯æœ€çµ‚å‘ˆç¾çµ¦è®€è€…çš„å½¢å¼ã€‚\n",
      "    *   **2. select_row:** é¸æ“‡ç‰¹å®šæ¢ä»¶çš„è¡Œï¼Œä¾‹å¦‚ç‰¹å®šçƒå“¡çš„æ•¸æ“šï¼Œæ˜¯æ·±å…¥åˆ†æçš„åŸºç¤ã€‚\n",
      "    *   **4. group_by:** å°‡æ•¸æ“šæŒ‰ç…§ä¸åŒçš„é¡å‹åˆ†çµ„ï¼Œä¾‹å¦‚æŒ‰ç…§çƒå“¡ã€å›åˆç­‰åˆ†çµ„ï¼Œæ˜¯é€²è¡Œçµ±è¨ˆåˆ†æçš„å‰æã€‚\n",
      "    *   **5. aggregate:** çµ±è¨ˆä¸åŒçµ„åˆ¥çš„æ•¸æ“šï¼Œä¾‹å¦‚çµ±è¨ˆæ¯å€‹çƒå“¡çš„å¾—åˆ†ã€å¤±èª¤ç­‰ï¼Œæä¾›é—œéµè³‡è¨Šã€‚\n",
      "*   **é«˜é‡è¦æ€§ï¼š**\n",
      "\n",
      "    *   **3. select_column:** é¸æ“‡ç‰¹å®šçš„åˆ—ï¼Œä¾‹å¦‚é¸æ“‡çƒçš„é¡å‹ã€å¤±èª¤åŸå› ç­‰ï¼Œæ˜¯èšç„¦åˆ†æçš„å¿…è¦æ­¥é©Ÿã€‚\n",
      "    *   **6. value_counts:** çµ±è¨ˆå„ç¨®é¡å‹çš„çƒã€å¤±èª¤åŸå› ç­‰çš„å‡ºç¾æ¬¡æ•¸ï¼Œæä¾›ç›´è§€çš„æ•¸æ“šã€‚\n",
      "    *   **7. crosstab:** äº¤å‰åˆ†æä¸åŒå› ç´ ä¹‹é–“çš„é—œä¿‚ï¼Œä¾‹å¦‚çƒçš„é¡å‹å’Œå¾—åˆ†çƒå“¡ä¹‹é–“çš„é—œä¿‚ã€‚\n",
      "*   **ä¸­ç­‰é‡è¦æ€§ï¼š**\n",
      "\n",
      "    *   **9. sort:** æŒ‰ç…§æ™‚é–“æ’åºï¼Œå¯ä»¥åˆ†ææ¯”è³½çš„é€²ç¨‹ã€‚\n",
      "    *   **10. calculate:** è¨ˆç®—æ–°çš„æ¬„ä½ï¼Œä¾‹å¦‚è¨ˆç®—å›åˆé•·åº¦ï¼Œå¯ä»¥æä¾›æ›´æ·±å…¥çš„åˆ†æã€‚\n",
      "    *   **8. pivot_table:** å‰µå»ºé€è¦–è¡¨ï¼Œå¯ä»¥æ›´éˆæ´»åœ°ç¸½çµæ•¸æ“šã€‚\n",
      "*   **ä½é‡è¦æ€§ï¼š**\n",
      "\n",
      "    *   **13. normalize:** æ­£è¦åŒ–çƒçš„é¡å‹åˆ†ä½ˆï¼Œå¯ä»¥åˆ†ææˆ°è¡“çš„è®ŠåŒ–ï¼Œä½†å¯èƒ½ä¸å¤ ç›´è§€ã€‚\n",
      "    *   **12. rolling_window:** è¨ˆç®—æ™‚é–“çš„æ»¾å‹•å¹³å‡ï¼Œå¯ä»¥åˆ†ææ¯”è³½çš„ç¯€å¥è®ŠåŒ–ï¼Œä½†å¯èƒ½å°ä¸€èˆ¬è®€è€…ä¾†èªªå¤ªéæŠ€è¡“æ€§ã€‚\n",
      "    *   **14. correlation:** è¨ˆç®—æ¬„ä½ä¹‹é–“çš„ç›¸é—œæ€§ï¼Œå¯èƒ½ç™¼ç¾ä¸€äº›æœ‰è¶£çš„é—œä¿‚ï¼Œä½†å¯èƒ½ä¸å¤ ç›´æ¥ã€‚\n",
      "    *   **15. one_hot_encoding:** å°‡åˆ†é¡è®Šæ•¸è½‰æ›ç‚ºäºŒå…ƒçŸ©é™£ï¼Œé€šå¸¸ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œå°æ–°èå ±å°çš„åƒ¹å€¼è¼ƒä½ã€‚\n",
      "    *   **11. merge:** åˆä½µå…©å€‹è¡¨æ ¼ï¼Œé€™å€‹æ“ä½œåœ¨é€™å€‹è³‡æ–™é›†ä¸­å¯èƒ½ä¸å¤ªå¸¸ç”¨ã€‚\n",
      "\n",
      "**æ’åºå¾Œçš„é™£åˆ—ï¼š**\n",
      "\n",
      "```\n",
      "[1, 2, 4, 5, 3, 6, 7, 9, 10, 8, 13, 12, 14, 15, 11]\n",
      "```\n",
      "\n",
      "æ’åºæ“ä½œé‡è¦æ€§ç·¨è™Ÿ: [1, 2, 4, 5, 3, 6, 7, 9, 10, 8, 13, 12, 14, 15, 11]\n",
      "operation_details: [{'number': '1', 'name': 'write', 'description': 'If the table is clear or small enough, generates text based on the tables using the LLM.', 'formatted': '1. write: If the table is clear or small enough, generates text based on the tables using the LLM.'}, {'number': '2', 'name': 'select_row', 'description': \"Subsets the table to rows matching a condition. Example: `df[df['getpoint_player'] == 'PlayerA']`.\", 'formatted': \"2. select_row: Subsets the table to rows matching a condition. Example: `df[df['getpoint_player'] == 'PlayerA']`.\"}, {'number': '3', 'name': 'select_column', 'description': \"Selects one or more columns from the table. Example: `df[['type', 'lose_reason']]`.\", 'formatted': \"3. select_column: Selects one or more columns from the table. Example: `df[['type', 'lose_reason']]`.\"}, {'number': '4', 'name': 'group_by', 'description': \"Groups rows based on one or more columns, enabling aggregate calculations. Example: `df.groupby('type')['getpoint_player'].count()`.\", 'formatted': \"4. group_by: Groups rows based on one or more columns, enabling aggregate calculations. Example: `df.groupby('type')['getpoint_player'].count()`.\"}, {'number': '5', 'name': 'aggregate', 'description': \"Calculates summary statistics for each group. Example: `df.groupby('type')['getpoint_player'].agg(['count', 'mean'])`.\", 'formatted': \"5. aggregate: Calculates summary statistics for each group. Example: `df.groupby('type')['getpoint_player'].agg(['count', 'mean'])`.\"}, {'number': '6', 'name': 'value_counts', 'description': \"Counts the occurrences of each unique value in a column. Example: `df['lose_reason'].value_counts()`.\", 'formatted': \"6. value_counts: Counts the occurrences of each unique value in a column. Example: `df['lose_reason'].value_counts()`.\"}, {'number': '7', 'name': 'crosstab', 'description': \"Computes a cross-tabulation of two or more factors. Example: `pd.crosstab(df['type'], df['getpoint_player'])`.\", 'formatted': \"7. crosstab: Computes a cross-tabulation of two or more factors. Example: `pd.crosstab(df['type'], df['getpoint_player'])`.\"}, {'number': '8', 'name': 'pivot_table', 'description': \"Creates a pivot table summarizing data. Example: `pd.pivot_table(df, values='rally', index='type', columns='getpoint_player', aggfunc='mean')`.\", 'formatted': \"8. pivot_table: Creates a pivot table summarizing data. Example: `pd.pivot_table(df, values='rally', index='type', columns='getpoint_player', aggfunc='mean')`.\"}, {'number': '9', 'name': 'sort', 'description': \"Orders the table by one or more columns. Example: `df.sort_values(by='time')`.\", 'formatted': \"9. sort: Orders the table by one or more columns. Example: `df.sort_values(by='time')`.\"}, {'number': '10', 'name': 'calculate', 'description': \"Performs mathematical operations on columns to create new columns. Example: `df['rally_length'] = df['time'].diff()`.\", 'formatted': \"10. calculate: Performs mathematical operations on columns to create new columns. Example: `df['rally_length'] = df['time'].diff()`.\"}, {'number': '11', 'name': 'merge', 'description': 'Joins two tables based on a common column.', 'formatted': '11. merge: Joins two tables based on a common column.'}, {'number': '12', 'name': 'rolling_window', 'description': 'Calculate the rolling mean of the \"time\" column to check the change of the speed.', 'formatted': '12. rolling_window: Calculate the rolling mean of the \"time\" column to check the change of the speed.'}, {'number': '13', 'name': 'normalize', 'description': 'Normalizes the distribution of \"type\" columns based on roundscore to check the change of the shot distribution.', 'formatted': '13. normalize: Normalizes the distribution of \"type\" columns based on roundscore to check the change of the shot distribution.'}, {'number': '14', 'name': 'correlation', 'description': 'Computes the correlation between two columns.', 'formatted': '14. correlation: Computes the correlation between two columns.'}, {'number': '15', 'name': 'one_hot_encoding', 'description': \"Converts categorical variables into a binary matrix representation. Example: `pd.get_dummies(df, columns=['type'])`.\", 'formatted': \"15. one_hot_encoding: Converts categorical variables into a binary matrix representation. Example: `pd.get_dummies(df, columns=['type'])`.\"}]\n",
      "âœ… selected_operations.json has been created.\n"
     ]
    }
   ],
   "source": [
    "# æ­£å¼ç‰ˆ - ä½¿ç”¨ Gemini éæ¿¾ç¾½çƒæ¯”è³½åˆ†ææ“ä½œ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "\n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{ 'text': response.text, 'logprobs': None }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{ 'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\", 'logprobs': None }]\n",
    "\n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def load_operations_from_json(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        operations_data = data if isinstance(data, dict) else {}\n",
    "        operation_details = []\n",
    "        operation_strings = []\n",
    "\n",
    "        for key, op in operations_data.items():\n",
    "            name = op.get('operation', '')\n",
    "            desc = op.get('description', '')\n",
    "            if key and name and desc:\n",
    "                formatted_op = f\"{key}. {name}: {desc}\"\n",
    "                operation_strings.append(formatted_op)\n",
    "                operation_details.append({\n",
    "                    'number': key,\n",
    "                    'name': name,\n",
    "                    'description': desc,\n",
    "                    'formatted': formatted_op\n",
    "                })\n",
    "\n",
    "        print(f\"å¾ {json_file_path} æˆåŠŸè¼‰å…¥ {len(operation_strings)} å€‹æ“ä½œ\")\n",
    "        return operation_details, operation_strings\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥ operations_info.json æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    summary = f\"è³‡æ–™é›†æ¦‚è¦:\\n- ç¸½è¡Œæ•¸: {dataframe.shape[0]}\\n- ç¸½åˆ—æ•¸: {dataframe.shape[1]}\\n- æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\\n\\nå„æ¬„ä½è³‡è¨Š:\\n\"\n",
    "    for col in dataframe.columns:\n",
    "        summary += f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            summary += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, dataframe[col].unique()[:10]))}\\n\"\n",
    "        else:\n",
    "            summary += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\\n\"\n",
    "    return summary\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def extract_operation_numbers_from_response(response):\n",
    "    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ•°ç»„\n",
    "    pattern = r'```\\s*\\[([\\d,\\s]+)\\]\\s*```'\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        # æå–åŒ¹é…çš„æ•°ç»„å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',')]\n",
    "        print(operation_list)\n",
    "        return operation_list\n",
    "    else:\n",
    "        print(\"æœªæ‰¾åˆ°æ’åºæ•°ç»„\")\n",
    "    \n",
    "def filter_badminton_operations(operation_details, operation_strings, df, api_key, outline_path='outline.txt'):\n",
    "    gemini = Gemini(api_key=api_key)\n",
    "    data_summary = get_data_summary(df)\n",
    "    data_sample = df.to_string()\n",
    "    outline = read_text_file(outline_path)\n",
    "    print(f\"operation_length: {len(operation_strings)}\")\n",
    "    prompt = f\"\"\"\n",
    "    æˆ‘æœ‰ä¸€å€‹æ’°å¯«æ–°èçš„å¤§é‹¼èˆ‡æ¯”è³½çš„è³‡æ–™é›†å’Œ {len(operation_strings)} å€‹åˆ†ææ“ä½œï¼Œè«‹ä¾æ“šæ“ä½œé‡è¦æ€§æ’åº(ç”±é«˜åˆ°ä½)ã€‚\n",
    "\n",
    "    å¤§ç¶±:\n",
    "    {outline}\n",
    "\n",
    "    è³‡æ–™æ¨£æœ¬:\n",
    "    {data_sample}\n",
    "\n",
    "    è³‡æ–™é›†è³‡è¨Š:\n",
    "    {data_summary}\n",
    "\n",
    "    æ“ä½œæ¸…å–®:\n",
    "    {'\\n'.join(operation_strings)}\n",
    "\n",
    "    è«‹å…ˆæ ¹æ“šchain_of_thoughtåˆ†æï¼Œå°‡æ“ä½œç·¨è™Ÿæ ¹æ“šé‡è¦æ€§æ’åºï¼Œæ¯å€‹ç·¨è™Ÿåƒ…åœ¨é™£åˆ—ä¸­å‡ºç¾ä¸€æ¬¡ï¼Œé™£åˆ—é•·åº¦æ‡‰ç‚º{len(operation_strings)}:\n",
    "    [1, 2, 3, ...]\n",
    "    \"\"\"\n",
    "    response = gemini.basic_request(prompt)\n",
    "    #print(response)\n",
    "    return extract_operation_numbers_from_response(response), response\n",
    "\n",
    "def main():\n",
    "    json_file_path = \"operations_info.json\"\n",
    "    operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "    if not operation_details:\n",
    "        return\n",
    "    try:\n",
    "        df = pd.read_csv(\"filtered_set1.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥è³‡æ–™éŒ¯èª¤: {e}\")\n",
    "        return\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ GOOGLE_API_KEY æœªè¨­å®š\")\n",
    "        return\n",
    "    kept_numbers, response = filter_badminton_operations(operation_details, operation_strings, df, api_key, outline_path = 'main.txt')\n",
    "    print(f\"response: {response}\")\n",
    "    #kept_numbers = list(set(kept_numbers) | {1, 2, 3})\n",
    "    print(\"æ’åºæ“ä½œé‡è¦æ€§ç·¨è™Ÿ:\", kept_numbers)\n",
    "    kept_numbers = list(set(kept_numbers[:int(0.7*len(kept_numbers))]) | {1,2,3})\n",
    "    new_operations = []\n",
    "    print(f\"operation_details: {operation_details}\")\n",
    "    for new_id, num in enumerate(kept_numbers, 1):\n",
    "        for detail in operation_details:\n",
    "            if int(detail['number']) == int(num):\n",
    "                new_operations.append({\n",
    "                    'number': new_id,\n",
    "                    'name': detail['name'],\n",
    "                    'description': detail['description']\n",
    "                })\n",
    "                break\n",
    "    output_json = new_operations\n",
    "    \n",
    "    with open(\"selected_operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "    print(\"âœ… selected_operations.json has been created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d4081",
   "metadata": {},
   "source": [
    "æ–°å¢é‡è¦operations(select_row, select_col, write) åˆ°'selected_operations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fc64e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# è®€å– filtered_operation.json\n",
    "with open('filtered_operations.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# å–å¾—ç›®å‰æœ€å¤§ number\n",
    "existing_numbers = [op['number'] for op in data['operations']]\n",
    "max_number = max(existing_numbers)\n",
    "\n",
    "# æ–°å¢çš„ operations\n",
    "new_operations = [\n",
    "    {\n",
    "        \"number\": max_number + 1,\n",
    "        \"name\": \"select_row\",\n",
    "        \"description\": \"Selects rows based on their row indices.\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": max_number + 2,\n",
    "        \"name\": \"select_col\",\n",
    "        \"description\": \"Selects columns based on their column names.\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": max_number + 3,\n",
    "        \"name\": \"write\",\n",
    "        \"description\": \"If the table is small enough, generates text based on the tables using the LLM; represents the leaf node of the tree.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# å°‡æ–°æ“ä½œåŠ å…¥åŸå§‹è³‡æ–™\n",
    "data['operations'].extend(new_operations)\n",
    "\n",
    "# å¯«å› JSON æª”\n",
    "with open('selected_operations.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# è¼¸å‡ºæ‰€æœ‰æ“ä½œçš„ number åˆ—è¡¨\n",
    "all_numbers = [op['number'] for op in data['operations']]\n",
    "print(all_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8adeb2",
   "metadata": {},
   "source": [
    "# STEP 7\n",
    "\n",
    "æ ¹æ“šTable,å¾—åˆ°è¦åŸ·è¡Œçš„æ“ä½œèˆ‡åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23802814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Planner for Badminton Game Report\n",
      "==================================================\n",
      "æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "è¼‰å…¥è¡¨æ ¼æè¿°: 494 å­—ç¬¦\n",
      "è¼‰å…¥æ“ä½œæè¿°: 10 å€‹é …ç›®\n",
      "æ“ä½œæ± : ['write', 'select_row', 'select_column', 'group_by', 'aggregate', 'value_counts', 'crosstab', 'pivot_table', 'sort', 'calculate']\n",
      "æ“ä½œæ­·å²: ['root(None)']\n",
      "\n",
      "é–‹å§‹ç”Ÿæˆæ“ä½œå’Œåƒæ•¸...\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "\n",
      "==================================================\n",
      "GEMINI è¼¸å‡ºçµæœ:\n",
      "==================================================\n",
      "[select_column(type, lose_reason, getpoint_player), value_counts(type), value_counts(lose_reason)]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3,outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨Geminiç”Ÿæˆoperationså’Œarguments\n",
    "        \"\"\"\n",
    "        \n",
    "        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}.\n",
    "6. Only select Opertions from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path)}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                print(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                return response.text.strip()\n",
    "            else:\n",
    "                print(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return None\n",
    "        \n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    \n",
    "# è¨­ç½®APIå¯†é‘°\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"Content Planner for Badminton Game Report\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶\n",
    "#required_files = ['filtered_set1.csv', 'filtered_data_description.txt', 'filtered_operations.json']\n",
    "\n",
    "print(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "\n",
    "# è®€å–CSVæª”æ¡ˆ\n",
    "TABLES = pd.read_csv('filtered_set1.csv')\n",
    "tables_str = TABLES.to_string()\n",
    "print(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "\n",
    "# è®€å–è¡¨æ ¼æè¿°\n",
    "TABLE_DESCRIPTION = read_text_file(\"filtered_data_description.txt\")\n",
    "if not TABLE_DESCRIPTION:\n",
    "    TABLE_DESCRIPTION = \"No table description available\"\n",
    "print(f\"è¼‰å…¥è¡¨æ ¼æè¿°: {len(TABLE_DESCRIPTION)} å­—ç¬¦\")\n",
    "\n",
    "# è®€å–æ“ä½œæè¿°\n",
    "OPERATION_DESCRIPTION = read_json_file(\"selected_operations.json\")\n",
    "print(f\"è¼‰å…¥æ“ä½œæè¿°: {len(OPERATION_DESCRIPTION)} å€‹é …ç›®\")\n",
    "\n",
    "# è¨­ç½®å…¶ä»–è®Šæ•¸\n",
    "MAX_DEPTH = 5\n",
    "MAX_DEGREE = 5\n",
    "OPERATION_HISTORY = ['root(None)']\n",
    "Level = 0\n",
    "# å¾æ“ä½œæè¿°ä¸­æå–æ“ä½œæ± \n",
    "#if 'operations' in OPERATION_DESCRIPTION:\n",
    "#    OPERATION_POOL = [op['name'] for op in OPERATION_DESCRIPTION['operations']]\n",
    "#else:\n",
    "#    OPERATION_POOL = list(OPERATION_DESCRIPTION.keys())\n",
    "OPERATION_POOL = [op['name'] for op in OPERATION_DESCRIPTION]\n",
    "\n",
    "print(f\"æ“ä½œæ± : {OPERATION_POOL}\")\n",
    "print(f\"æ“ä½œæ­·å²: {OPERATION_HISTORY}\")\n",
    "    \n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–å…§å®¹è¦åŠƒå™¨\n",
    "planner = ContentPlanner(api_key)\n",
    "\n",
    "# ç”Ÿæˆæ“ä½œå’Œåƒæ•¸\n",
    "print(\"\\né–‹å§‹ç”Ÿæˆæ“ä½œå’Œåƒæ•¸...\")\n",
    "operations_and_arguments = planner.generate_operations(\n",
    "    tables=tables_str,\n",
    "    table_description=TABLE_DESCRIPTION,\n",
    "    operation_description=OPERATION_DESCRIPTION,\n",
    "    operation_history=OPERATION_HISTORY,\n",
    "    operation_pool=OPERATION_POOL,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    max_degree=MAX_DEGREE,\n",
    "    outline_path='analyze_response.txt'\n",
    ")\n",
    "OPERATION_HISTORY = OPERATION_HISTORY.append(operations_and_arguments)\n",
    "Level +=1\n",
    "\n",
    "if operations_and_arguments:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GEMINI è¼¸å‡ºçµæœ:\")\n",
    "    print(\"=\"*50)\n",
    "    print(operations_and_arguments)\n",
    "    print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "243e8c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[select_column(type, lose_reason, getpoint_player), value_counts(type), value_counts(lose_reason)]\n"
     ]
    }
   ],
   "source": [
    "print(operations_and_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317faf",
   "metadata": {},
   "source": [
    "è§£æLLM responseå…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc1223e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select_column(type, lose_reason, getpoint_player)', 'value_counts(type)', 'value_counts(lose_reason)']\n"
     ]
    }
   ],
   "source": [
    "# åŸå§‹å­—ç¬¦ä¸²\n",
    "test_str = operations_and_arguments\n",
    "\n",
    "# æå–æ–¹æ‹¬å·å†…çš„å†…å®¹\n",
    "start = test_str.find('[') + 1\n",
    "end = test_str.rfind(']')\n",
    "content = test_str[start:end].strip()\n",
    "\n",
    "elements = []\n",
    "current = []\n",
    "stack = 0\n",
    "\n",
    "# éå†å­—ç¬¦è¿›è¡Œè§£æ\n",
    "for char in content:\n",
    "    if char == '(':\n",
    "        stack += 1\n",
    "        current.append(char)\n",
    "    elif char == ')':\n",
    "        stack -= 1\n",
    "        current.append(char)\n",
    "    elif char == ',' and stack == 0:\n",
    "        elements.append(''.join(current).strip())\n",
    "        current = []\n",
    "    else:\n",
    "        current.append(char)\n",
    "\n",
    "# æ·»åŠ æœ€åä¸€ä¸ªå…ƒç´ \n",
    "if current:\n",
    "    elements.append(''.join(current).strip())\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12882b",
   "metadata": {},
   "source": [
    "# STEP 8\n",
    "\n",
    "æ ¹æ“šæ¬„ä½å‹æ…‹èˆ‡'operation_name' å’Œ 'operation_argument'ï¼Œè«‹LLMæ’°å¯«å¯ä»¥åŸ·è¡Œçš„æ“ä½œç¨‹å¼ç¢¼\n",
    "\n",
    "å–æ¬„ä½å‹æ…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe204f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          int64\n",
      "rally               int64\n",
      "time               object\n",
      "roundscore_A        int64\n",
      "roundscore_B        int64\n",
      "player             object\n",
      "type               object\n",
      "lose_reason        object\n",
      "getpoint_player    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_copy = pd.read_csv(\"filtered_set1.csv\")\n",
    "df = df_copy\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2986094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 315 entries, 0 to 314\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       315 non-null    int64 \n",
      " 1   rally            315 non-null    int64 \n",
      " 2   time             315 non-null    object\n",
      " 3   roundscore_A     315 non-null    int64 \n",
      " 4   roundscore_B     315 non-null    int64 \n",
      " 5   player           315 non-null    object\n",
      " 6   type             315 non-null    object\n",
      " 7   lose_reason      36 non-null     object\n",
      " 8   getpoint_player  36 non-null     object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 22.3+ KB\n",
      "ç”Ÿæˆçš„ç¨‹å¼ç¢¼ï¼š\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def select_column(df, type_col, lose_reason_col, getpoint_player_col):\n",
      "  \"\"\"\n",
      "  å¾ DataFrame ä¸­é¸æ“‡æŒ‡å®šçš„æ¬„ä½ã€‚\n",
      "\n",
      "  Args:\n",
      "    df: pandas DataFrame.\n",
      "    type_col: é¡å‹æ¬„ä½çš„åç¨± (å­—ä¸²).\n",
      "    lose_reason_col: å¤±æ•—åŸå› æ¬„ä½çš„åç¨± (å­—ä¸²).\n",
      "    getpoint_player_col: å¾—åˆ†çƒå“¡æ¬„ä½çš„åç¨± (å­—ä¸²).\n",
      "\n",
      "  Returns:\n",
      "    ä¸€å€‹æ–°çš„ DataFrameï¼Œåƒ…åŒ…å«æŒ‡å®šçš„æ¬„ä½ã€‚\n",
      "  \"\"\"\n",
      "  return df[[type_col, lose_reason_col, getpoint_player_col]]\n",
      "\n",
      "\n",
      "# è®€å– CSV æ•¸æ“šé›†\n",
      "try:\n",
      "    df = pd.read_csv(\"filtered_set1.csv\")\n",
      "except FileNotFoundError:\n",
      "    print(\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'filtered_set1.csv' æª”æ¡ˆã€‚è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\")\n",
      "    exit()\n",
      "except pd.errors.EmptyDataError:\n",
      "    print(\"éŒ¯èª¤ï¼š'filtered_set1.csv' æª”æ¡ˆç‚ºç©ºã€‚\")\n",
      "    exit()\n",
      "except pd.errors.ParserError:\n",
      "    print(\"éŒ¯èª¤ï¼šè§£æ 'filtered_set1.csv' æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚\")\n",
      "    exit()\n",
      "\n",
      "\n",
      "# æŒ‡å®šæ¬„ä½åç¨±\n",
      "type_col = \"type\"  # æ›¿æ›ç‚ºå¯¦éš›çš„æ¬„ä½åç¨±\n",
      "lose_reason_col = \"lose_reason\"  # æ›¿æ›ç‚ºå¯¦éš›çš„æ¬„ä½åç¨±\n",
      "getpoint_player_col = \"getpoint_player\" # æ›¿æ›ç‚ºå¯¦éš›çš„æ¬„ä½åç¨±\n",
      "\n",
      "\n",
      "# åŸ·è¡Œ select_column æ“ä½œ\n",
      "try:\n",
      "    modified_df = select_column(df.copy(), type_col, lose_reason_col, getpoint_player_col)\n",
      "except KeyError as e:\n",
      "    print(f\"éŒ¯èª¤ï¼šDataFrame ä¸­æ‰¾ä¸åˆ°æ¬„ä½ '{e.args[0]}'ã€‚è«‹æª¢æŸ¥æ¬„ä½åç¨±æ˜¯å¦æ­£ç¢ºã€‚\")\n",
      "    exit()\n",
      "\n",
      "\n",
      "# å°‡ä¿®æ”¹å¾Œçš„ DataFrame å­˜å…¥ 'tmp.csv'\n",
      "try:\n",
      "    modified_df.to_csv(\"tmp.csv\", index=False)\n",
      "    print(\"å·²æˆåŠŸå°‡ä¿®æ”¹å¾Œçš„ DataFrame å„²å­˜åˆ° 'tmp.csv'ã€‚\")\n",
      "except Exception as e:\n",
      "    print(f\"å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
      "```\n",
      "\n",
      "å·²æˆåŠŸå°‡ä¿®æ”¹å¾Œçš„ DataFrame å„²å­˜åˆ° 'tmp.csv'ã€‚\n",
      "\n",
      "è™•ç†çµæœï¼š\n",
      "     type lose_reason getpoint_player\n",
      "0     ç™¼é•·çƒ         NaN             NaN\n",
      "1      åˆ‡çƒ         NaN             NaN\n",
      "2      æŒ‘çƒ         NaN             NaN\n",
      "3      é•·çƒ         NaN             NaN\n",
      "4      æ®ºçƒ         NaN             NaN\n",
      "..    ...         ...             ...\n",
      "310  æœªçŸ¥çƒç¨®         NaN             NaN\n",
      "311    åˆ‡çƒ         NaN             NaN\n",
      "312    æŒ‘çƒ         NaN             NaN\n",
      "313    é•·çƒ         NaN             NaN\n",
      "314    é•·çƒ          å‡ºç•Œ               A\n",
      "\n",
      "[315 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "elements = ['select_column(type, lose_reason, getpoint_player)', 'value_counts(type)', 'value_counts(lose_reason)']\n",
    "\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "\n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{'text': response.text, 'logprobs': None}]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—\", 'logprobs': None}]\n",
    "\n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameOperator:\n",
    "    def __init__(self, api_key):\n",
    "        self.lm = setup_gemini_api(api_key)\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'ï¼Œæ’°å¯«å®Œæ•´python code.\n",
    "        åˆ‡å¿Œæ¯å€‹æ“ä½œåƒæ•¸éƒ½éœ€è¦ä½¿ç”¨\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self.lm.basic_request(prompt)\n",
    "\n",
    "    def safe_execute(self, code, df):\n",
    "        try:\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆä½œç‚ºæ¨¡æ“¬ df.csv è·¯å¾‘\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            # å»ºç«‹å®‰å…¨åŸ·è¡Œç’°å¢ƒ\n",
    "            exec_globals = {'pd': pd}\n",
    "            exec_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œç”Ÿæˆçš„ç¨‹å¼ç¢¼\n",
    "            exec(code, exec_globals, exec_locals)\n",
    "\n",
    "            # å¾ tmp.csv è®€å–è™•ç†å¾Œçš„çµæœ\n",
    "            result_df = pd.read_csv(\"tmp.csv\")\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "operator = DataFrameOperator(API_KEY)\n",
    "\n",
    "# ç²å–è³‡æ–™è³‡è¨Š\n",
    "df_info = df.info()\n",
    "df_path = \"filtered_set1.csv\"\n",
    "operation_def = elements[0]\n",
    "#print(operation)\n",
    "\n",
    "generated_code = operator.generate_code(\n",
    "    operation=operation_def,\n",
    "    df_info=df_info,\n",
    "    df_path=df_path\n",
    ")\n",
    "\n",
    "print(\"ç”Ÿæˆçš„ç¨‹å¼ç¢¼ï¼š\")\n",
    "print(generated_code)\n",
    "\n",
    "# åŸ·è¡Œæ“ä½œ\n",
    "processed_df = operator.safe_execute(generated_code, df)\n",
    "\n",
    "print(\"\\nè™•ç†çµæœï¼š\")\n",
    "print(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e320f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "5\n",
      "- Node(1)\n",
      "  - Node(2)\n",
      "    - Node(3)\n",
      "    - Node(4)\n",
      "      - Node(5)\n",
      "      - Node(6)\n",
      "        - Node(7)\n",
      "      - Node(8)\n",
      "      - Node(9)\n",
      "      - Node(10)\n",
      "        - Node(11)\n",
      "        - Node(12)\n",
      "        - Node(13)\n",
      "        - Node(14)\n",
      "        - Node(15)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.level = Level\n",
    "        self.text = text\n",
    "        self.table = table\n",
    "        self.\n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode({self.value})\"\n",
    "\n",
    "def build_random_tree(current_depth=1, max_depth=5, max_degree=5, value_counter=[0]):\n",
    "    value_counter[0] += 1\n",
    "    node = TreeNode(value_counter[0])\n",
    "\n",
    "    if current_depth >= max_depth or random.random() < 0.3:\n",
    "        return node  # è‘‰ç¯€é»\n",
    "\n",
    "    degree = random.randint(1, max_degree)\n",
    "    print(degree)\n",
    "    for _ in range(degree):\n",
    "        child = build_random_tree(current_depth + 1, max_depth, max_degree, value_counter)\n",
    "        node.children.append(child)\n",
    "\n",
    "    return node\n",
    "\n",
    "def print_tree(node, level=0):\n",
    "    print(\"  \" * level + f\"- Node({node.value})\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, level + 1)\n",
    "\n",
    "# å»ºç«‹ä¸¦å°å‡ºéš¨æ©Ÿæ¨¹\n",
    "random.seed(42)  # å¯é‡ç¾æ€§\n",
    "root = build_random_tree()\n",
    "print_tree(root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49301e",
   "metadata": {},
   "source": [
    "# STEP final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-of-Report for Data Analysis\n",
      "==================================================\n",
      "æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "æœ€å¤§æ·±åº¦: 3\n",
      "æœ€å¤§åˆ†æ”¯åº¦: 3\n",
      "è¼‰å…¥æ“ä½œæ± : ['write', 'select_row', 'select_column', 'group_by', 'aggregate', 'value_counts', 'crosstab', 'pivot_table', 'sort', 'calculate']\n",
      "\n",
      "é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 0, Operation: root(None)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['select_column(roundscore_A, roundscore_B)', 'value_counts(roundscore_A)', 'value_counts(roundscore_B)']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(roundscore_A, roundscore_B), çµæœå½¢ç‹€: (315, 2)\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_A), çµæœå½¢ç‹€: (315, 9)\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_B), çµæœå½¢ç‹€: (16, 2)\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 1, Operation: select_column(roundscore_A, roundscore_B)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['value_counts(roundscore_A)', 'value_counts(roundscore_B)']\n",
      "å·²æˆåŠŸå°‡ä¿®æ”¹å¾Œçš„ DataFrame å„²å­˜åˆ° 'tmp.csv'\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_A), çµæœå½¢ç‹€: (21, 2)\n",
      "æˆåŠŸå°‡ value_counts çµæœå„²å­˜åˆ° 'tmp.csv'\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_B), çµæœå½¢ç‹€: (16, 2)\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 1, Operation: value_counts(roundscore_A)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['value_counts(roundscore_B)', 'calculate(time)']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_B), çµæœå½¢ç‹€: (16, 2)\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: calculate(time), çµæœå½¢ç‹€: (315, 9)\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 1, Operation: value_counts(roundscore_B)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['aggregate(count, roundscore_B)', 'sort(count)', 'write()']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count, roundscore_B), çµæœå½¢ç‹€: (16, 2)\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count), çµæœå½¢ç‹€: (16, 2)\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: value_counts(roundscore_A)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['value_counts(roundscore_A)', 'write()']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_A), çµæœå½¢ç‹€: (21, 2)\n",
      "Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (2/3)...\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: value_counts(roundscore_B)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['value_counts(roundscore_B)', 'write()']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(roundscore_B), çµæœå½¢ç‹€: (16, 2)\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: value_counts(roundscore_B)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: calculate(time)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'value_counts(lose_reason)']\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(lose_reason), çµæœå½¢ç‹€: (4, 2)\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: aggregate(count, roundscore_B)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "\n",
      "è™•ç†ç¯€é» - Level: 2, Operation: sort(count)\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "ç”Ÿæˆæ“ä½œ: [\"'write'\"]\n",
      "å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: 'write', çµæœå½¢ç‹€: (16, 2)\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "å‰µå»º write ç¯€é»: write()\n",
      "Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n",
      "å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "\n",
      "ç”Ÿæˆæœ€çµ‚å ±å‘Š...\n",
      "\n",
      "==================================================\n",
      "TREE-OF-REPORT æœ€çµ‚å ±å‘Š\n",
      "==================================================\n",
      "## è³½äº‹æ•¸æ“šæ·±åº¦åˆ†æï¼šç²¾æº–æ´å¯Ÿï¼Œå‹è² é—œéµç›¡åœ¨æŒæ¡\n",
      "\n",
      "**èµ·ï¼šç·’è«– - æ•¸æ“šæ­ç¤ºçš„æ½›åœ¨å„ªå‹¢èˆ‡éš±æ†‚**\n",
      "\n",
      "æœ¬æ¬¡è³½äº‹æ•¸æ“šåˆ†ææ—¨åœ¨é€éé‡åŒ–é¸æ‰‹è¡¨ç¾ã€è§£æå¾—åˆ†æ¨¡å¼åŠå¤±èª¤åŸå› ï¼Œå¾è€ŒæŒ–æ˜æ½›åœ¨çš„æˆ°è¡“å„ªå‹¢èˆ‡æŠ€è¡“çŸ­æ¿ã€‚é€éå°Player Aå’ŒPlayer Bçš„å¾—åˆ†åˆ†å¸ƒã€æ“Šçƒç¨®é¡åå¥½ä»¥åŠå¤±åˆ†ç’°ç¯€çš„æ·±å…¥å‰–æï¼Œæˆ‘å€‘æœŸæœ›èƒ½ç‚ºå¾ŒçºŒçš„è¨“ç·´åŠæˆ°è¡“åˆ¶å®šæä¾›æ›´ç²¾æº–çš„æ•¸æ“šæ”¯æŒï¼Œé€²è€Œæå‡æ•´é«”ç«¶æŠ€æ°´å¹³ã€‚æœ¬æ¬¡åˆ†æä¸åƒ…é—œæ³¨è¡¨é¢ä¸Šçš„å¾—åˆ†ï¼Œæ›´è‘—é‡æ–¼å¾—åˆ†èƒŒå¾Œçš„åŸå› ï¼Œä¾‹å¦‚ä¸åŒæ“Šçƒç¨®é¡çš„æ•ˆç‡ã€å°æ‰‹å¤±èª¤çš„æ¯”é‡ç­‰ï¼ŒåŠ›æ±‚æ­ç¤ºæ›´æ·±å±¤æ¬¡çš„æ¯”è³½é‚è¼¯ã€‚\n",
      "\n",
      "**æ‰¿ï¼šPlayer Aèˆ‡Player Bå¾—åˆ†æ¨¡å¼å°æ¯” - åˆ†å¸ƒå·®ç•°èˆ‡ç©©å®šæ€§è€ƒé‡**\n",
      "\n",
      "å¾æ•¸æ“šä¾†çœ‹ï¼ŒPlayer Açš„å¾—åˆ†åˆ†ä½ˆå‘ˆç¾æ˜é¡¯çš„é›¢æ•£å‹ç‰¹å¾µï¼Œå…¶ä¸­13åˆ†é€™å€‹åˆ†æ•¸å‡ºç¾çš„é »ç‡æ¥µé«˜ï¼Œé”52æ¬¡ã€‚é€™æ„å‘³è‘—Player Aå¯èƒ½åœ¨ç‰¹å®šçš„æˆ°è¡“æˆ–æŠ€è¡“ç’°ç¯€ä¸Šè¡¨ç¾çªå‡ºï¼Œèƒ½ç©©å®šå–å¾—13åˆ†ã€‚ç„¶è€Œï¼Œå…¶ä»–åˆ†æ•¸å‡ºç¾çš„é »ç‡å‰‡è¼ƒä½ï¼Œä¾‹å¦‚ç”šè‡³æœ‰1åˆ†å’Œ2åˆ†çš„æ¥µä½åˆ†å‡ºç¾ã€‚é€™ç¨®æƒ…æ³æš—ç¤ºPlayer Aåœ¨ç©©å®šæ€§æ–¹é¢å¯èƒ½å­˜åœ¨å•é¡Œï¼Œæ¯”è³½ç‹€æ…‹å®¹æ˜“å—åˆ°æ³¢å‹•å½±éŸ¿ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†æé€ æˆé€™ç¨®æ³¢å‹•çš„åŸå› ï¼Œä¾‹å¦‚æ˜¯å¦èˆ‡å°æ‰‹æˆ°è¡“ã€è‡ªèº«é«”èƒ½ç‹€æ…‹ã€æˆ–ç‰¹å®šæŠ€è¡“ç’°ç¯€çš„å¤±èª¤æœ‰é—œã€‚\n",
      "\n",
      "Player Bçš„å¾—åˆ†åˆ†ä½ˆå‰‡ç›¸å°é›†ä¸­ï¼Œ6åˆ†ã€10åˆ†ã€9åˆ†æ˜¯å…¶å¾—åˆ†é«˜å³°ï¼Œåˆ†åˆ¥å‡ºç¾äº†59æ¬¡ã€42æ¬¡å’Œ40æ¬¡ã€‚6åˆ†çš„é »ç‡æœ€é«˜ï¼Œé¡¯ç¤ºPlayer Båœ¨å–å¾—6åˆ†æ–¹é¢å…·æœ‰ç›¸ç•¶çš„ç©©å®šæ€§ã€‚é€™ç¨®ç›¸å°é›†ä¸­çš„å¾—åˆ†åˆ†ä½ˆå¯èƒ½ä»£è¡¨Player Bçš„æˆ°è¡“åŸ·è¡Œæ›´ç‚ºç©©å¥ï¼Œæˆ–æ›´æ“…é•·åœ¨ç‰¹å®šçš„æ¯”åˆ†ç¯„åœå…§æ§åˆ¶ç¯€å¥ã€‚ä½†åŒæ™‚ä¹Ÿéœ€è¦è­¦æƒ•ï¼Œé€™ç¨®ç©©å®šæ€§æ˜¯å¦æ„å‘³è‘—ç¼ºä¹è®ŠåŒ–ï¼Œå®¹æ˜“è¢«å°æ‰‹æŒæ¡è¦å¾‹ã€‚\n",
      "\n",
      "**è½‰ï¼šæ“Šçƒç¨®é¡èˆ‡å¤±åˆ†åˆ†æ - æˆ°è¡“åå¥½èˆ‡å¼±é»æš´éœ²**\n",
      "\n",
      "åœ¨æ“Šçƒç¨®é¡æ–¹é¢ï¼Œé•·çƒï¼ˆ55æ¬¡ï¼‰çš„ä½¿ç”¨é »ç‡æœ€é«˜ï¼Œå…¶æ¬¡æ˜¯æ®ºçƒï¼ˆ36æ¬¡ï¼‰å’ŒæŒ‘çƒï¼ˆ35æ¬¡ï¼‰ã€‚é€™åæ˜ äº†æœ¬æ¬¡æ¯”è³½ä¸­ï¼Œé•·çƒåœ¨æˆ°è¡“é‹ç”¨ä¸­çš„é‡è¦æ€§ï¼Œå¯èƒ½æ˜¯ä½œç‚ºæ§åˆ¶ç¯€å¥ã€å°‹æ‰¾é€²æ”»æ©Ÿæœƒæˆ–é˜²å®ˆåæ“Šçš„æ‰‹æ®µã€‚æ®ºçƒå’ŒæŒ‘çƒçš„æ¬¡æ•¸ç›¸è¿‘ï¼Œèªªæ˜é¸æ‰‹åœ¨é€²æ”»å’Œé˜²å®ˆä¹‹é–“ç›¸å°å‡è¡¡ã€‚ç„¶è€Œï¼Œåƒ…åƒ…çµ±è¨ˆæ¬¡æ•¸ä¸¦ä¸èƒ½èªªæ˜æ•ˆç‡ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†æä¸åŒæ“Šçƒç¨®é¡çš„æˆåŠŸç‡ï¼Œä»¥åŠå®ƒå€‘èˆ‡å¾—åˆ†ä¹‹é–“çš„é—œè¯ï¼Œä¾‹å¦‚ï¼Œæ®ºçƒçš„æˆåŠŸç‡æ˜¯å¦é«˜æ–¼æŒ‘çƒï¼Ÿé•·çƒæ˜¯å¦æ›´å¤šç”¨æ–¼éæ¸¡é‚„æ˜¯ç›´æ¥å¾—åˆ†ï¼Ÿ\n",
      "\n",
      "å¤±åˆ†æ–¹é¢ï¼Œå°æ‰‹ç›´æ¥å¾—åˆ†12æ¬¡ï¼Œå› çƒå‡ºç•Œå¤±åˆ†12æ¬¡ã€‚é€™å…©é …æ•¸æ“šéƒ½éœ€è¦å¼•èµ·é‡è¦–ã€‚å°æ‰‹ç›´æ¥å¾—åˆ†å¯èƒ½æºæ–¼é˜²å®ˆæ¼æ´ã€åæ‡‰é²ç·©æˆ–åˆ¤æ–·å¤±èª¤ï¼Œéœ€è¦é‡å°æ€§åœ°åŠ å¼·é˜²å®ˆè¨“ç·´ï¼Œæå‡åæ‡‰é€Ÿåº¦å’Œåˆ¤æ–·èƒ½åŠ›ã€‚çƒå‡ºç•Œå¤±åˆ†å‰‡å¯èƒ½èˆ‡ç™¼åŠ›æ§åˆ¶ã€æ“Šçƒè§’åº¦æˆ–å ´åœ°é©æ‡‰æ€§æœ‰é—œï¼Œéœ€è¦åŠ å¼·å°çƒçš„æ§åˆ¶åŠ›ï¼Œä»¥åŠå°ä¸åŒå ´åœ°çš„é©æ‡‰èƒ½åŠ›ã€‚\n",
      "\n",
      "**åˆï¼šç¸½çµèˆ‡å»ºè­° - æŒçºŒå„ªåŒ–ï¼Œæå‡ç«¶æŠ€å¯¦åŠ›**\n",
      "\n",
      "ç¶œä¸Šæ‰€è¿°ï¼Œæœ¬æ¬¡è³½äº‹æ•¸æ“šåˆ†ææ­ç¤ºäº†Player Aå’ŒPlayer Båœ¨å¾—åˆ†æ¨¡å¼ã€æ“Šçƒç¨®é¡åŠå¤±åˆ†ç’°ç¯€ä¸Šçš„å·®ç•°ã€‚Player Açš„å¾—åˆ†åˆ†ä½ˆè¼ƒç‚ºåˆ†æ•£ï¼Œç©©å®šæ€§éœ€æå‡ï¼›Player Bå‰‡æ›´ç‚ºé›†ä¸­ï¼Œå¯èƒ½å­˜åœ¨æˆ°è¡“è®ŠåŒ–ä¸è¶³çš„éš±æ†‚ã€‚é•·çƒåœ¨æˆ°è¡“é‹ç”¨ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œè€Œå¤±åˆ†ä¸»è¦æºæ–¼å°æ‰‹ç›´æ¥å¾—åˆ†å’Œçƒå‡ºç•Œã€‚\n",
      "\n",
      "åŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œæˆ‘å€‘æå‡ºä»¥ä¸‹å»ºè­°ï¼š\n",
      "\n",
      "*   **é‡å°Player Aï¼š** åŠ å¼·ç©©å®šæ€§è¨“ç·´ï¼Œåˆ†æ13åˆ†é«˜é »æ¬¡å‡ºç¾çš„åŸå› ï¼Œä¸¦å°‡å…¶æˆåŠŸç¶“é©—è¤‡è£½åˆ°å…¶ä»–å¾—åˆ†ç’°ç¯€ï¼›åŒæ™‚ï¼Œåˆ†æä½åˆ†å‡ºç¾çš„åŸå› ï¼Œé‡å°æ€§åœ°é€²è¡Œæ”¹é€²ã€‚\n",
      "*   **é‡å°Player Bï¼š** åœ¨ä¿æŒç©©å®šæ€§çš„å‰æä¸‹ï¼Œå¢åŠ æˆ°è¡“è®ŠåŒ–ï¼Œé¿å…è¢«å°æ‰‹æŒæ¡è¦å¾‹ï¼›åŒæ™‚ï¼Œåˆ†æ6åˆ†é«˜é »æ¬¡å‡ºç¾çš„åŸå› ï¼Œä¸¦æ€è€ƒå¦‚ä½•åˆ©ç”¨å…¶å„ªå‹¢ä¾†å‰µé€ æ›´å¤šå¾—åˆ†æ©Ÿæœƒã€‚\n",
      "*   **é‡å°å…±åŒå•é¡Œï¼š** åŠ å¼·é˜²å®ˆè¨“ç·´ï¼Œæå‡åæ‡‰é€Ÿåº¦å’Œåˆ¤æ–·èƒ½åŠ›ï¼›å¼·åŒ–å°çƒçš„æ§åˆ¶åŠ›ï¼Œé™ä½çƒå‡ºç•Œå¤±åˆ†ç‡ï¼›åˆ†æä¸åŒæ“Šçƒç¨®é¡çš„æˆåŠŸç‡ï¼Œä¸¦æ“šæ­¤èª¿æ•´æˆ°è¡“ç­–ç•¥ã€‚\n",
      "\n",
      "ç¸½è€Œè¨€ä¹‹ï¼Œæ•¸æ“šåˆ†ææ˜¯æå‡ç«¶æŠ€å¯¦åŠ›çš„é‡è¦æ‰‹æ®µã€‚é€éæŒçºŒçš„æ•¸æ“šæ”¶é›†ã€åˆ†æå’Œåé¥‹ï¼Œæˆ‘å€‘å¯ä»¥æ›´å…¨é¢åœ°äº†è§£è‡ªèº«å„ªåŠ£å‹¢ï¼Œä¸¦æ“šæ­¤åˆ¶å®šæ›´ç²¾æº–çš„è¨“ç·´è¨ˆåŠƒå’Œæˆ°è¡“ç­–ç•¥ï¼Œå¾è€Œåœ¨æœªä¾†çš„æ¯”è³½ä¸­å–å¾—æ›´å¥½çš„æˆç¸¾ã€‚\n",
      "å ±å‘Šå·²å„²å­˜è‡³ tree_of_report.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "import copy\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„å‡½æ•¸ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        print(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # è¿”å›é»˜èªæ“ä½œé›†åˆ\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# ===== æ¨¹ç¯€é»é¡åˆ¥ =====\n",
    "class TreeNode:\n",
    "    \"\"\"æ¨¹ç¯€é»é¡åˆ¥ï¼ŒåŒ…å« children, level, text, table å±¬æ€§\"\"\"\n",
    "    def __init__(self, level: int = 0, text: str = \"\", table: pd.DataFrame = None, operation: str = None):\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.level: int = level\n",
    "        self.text: str = text\n",
    "        self.table: pd.DataFrame = table if table is not None else pd.DataFrame()\n",
    "        self.operation: str = operation\n",
    "        self.parent: Optional['TreeNode'] = None\n",
    "        self.operation_history: List[str] = []\n",
    "    \n",
    "    def add_child(self, child: 'TreeNode'):\n",
    "        \"\"\"æ·»åŠ å­ç¯€é»\"\"\"\n",
    "        child.parent = self\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºè‘‰ç¯€é»\"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„ ContentPlanner =====\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨Geminiç”Ÿæˆoperationså’Œarguments\n",
    "        \"\"\"\n",
    "        \n",
    "        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}.\n",
    "6. Only select Opertions from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path) if os.path.exists(outline_path) else \"Generate comprehensive data analysis\"}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                print(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                return self.parse_operations(response.text.strip())\n",
    "            else:\n",
    "                print(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def parse_operations(self, response_text):\n",
    "        \"\"\"è§£æ Gemini å›æ‡‰çš„æ“ä½œåˆ—è¡¨\"\"\"\n",
    "        try:\n",
    "            # å°‹æ‰¾æ–¹æ‹¬è™Ÿå…§çš„å…§å®¹\n",
    "            bracket_match = re.search(r'\\[(.*?)\\]', response_text, re.DOTALL)\n",
    "            if bracket_match:\n",
    "                operations_str = bracket_match.group(1)\n",
    "                # åˆ†å‰²æ“ä½œ\n",
    "                operations = []\n",
    "                # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²æ“ä½œ\n",
    "                op_pattern = r'([a-zA-Z_]+\\([^)]*\\))'\n",
    "                matches = re.findall(op_pattern, operations_str)\n",
    "                if matches:\n",
    "                    return matches\n",
    "                else:\n",
    "                    # å¦‚æœæ²’æœ‰åŒ¹é…ï¼Œå˜—è©¦ç°¡å–®åˆ†å‰²\n",
    "                    ops = [op.strip().strip(',') for op in operations_str.split(',')]\n",
    "                    return [op for op in ops if op and op != '']\n",
    "            else:\n",
    "                # å¦‚æœæ²’æœ‰æ–¹æ‹¬è™Ÿï¼ŒæŒ‰è¡Œåˆ†å‰²\n",
    "                lines = response_text.split('\\n')\n",
    "                operations = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.startswith('#') and not line.startswith('System'):\n",
    "                        operations.append(line)\n",
    "                return operations\n",
    "        except Exception as e:\n",
    "            print(f\"è§£ææ“ä½œå¤±æ•—: {e}\")\n",
    "            return []\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„ Gemini å’Œ DataFrameOperator =====\n",
    "class Gemini(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        genai.configure(api_key=api_key)\n",
    "        self._model_instance = genai.GenerativeModel(model_name)\n",
    "        super().__init__(model=model_name)\n",
    "\n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "        if isinstance(messages, list):\n",
    "            prompt_text = \"\".join([msg.get('content', '') for msg in messages])\n",
    "        else:\n",
    "            prompt_text = str(messages)\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt_text)\n",
    "            if not response.text:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            return [{'text': response.text, 'logprobs': None}]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—\", 'logprobs': None}]\n",
    "\n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self._model_instance.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key):\n",
    "    lm = Gemini(api_key=api_key)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "class DataFrameOperator:\n",
    "    def __init__(self, api_key):\n",
    "        self.lm = setup_gemini_api(api_key)\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path=\"input_tmp.csv\"):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'ï¼Œæ’°å¯«å®Œæ•´python code.\n",
    "        åˆ‡å¿Œæ¯å€‹æ“ä½œåƒæ•¸éƒ½éœ€è¦ä½¿ç”¨\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self.lm.basic_request(prompt)\n",
    "\n",
    "    def safe_execute(self, code, df):\n",
    "        try:\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆä½œç‚ºæ¨¡æ“¬ df.csv è·¯å¾‘\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            # å»ºç«‹å®‰å…¨åŸ·è¡Œç’°å¢ƒ\n",
    "            exec_globals = {'pd': pd}\n",
    "            exec_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œç”Ÿæˆçš„ç¨‹å¼ç¢¼\n",
    "            exec(code, exec_globals, exec_locals)\n",
    "\n",
    "            # å¾ tmp.csv è®€å–è™•ç†å¾Œçš„çµæœ\n",
    "            if os.path.exists(\"tmp.csv\"):\n",
    "                result_df = pd.read_csv(\"tmp.csv\")\n",
    "                return result_df\n",
    "            else:\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "# ===== æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "# ä¿®æ”¹ TextGenerator é¡åˆ¥ï¼šåˆå§‹åŒ–æ™‚æ¥æ”¶ table_description\n",
    "import time  # ç¢ºä¿å°å…¥ time æ¨¡çµ„\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=3, delay_seconds=30):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                print(f\"Gemini å›æ‡‰å¤±æ•—: {err}\")\n",
    "                if \"429\" in err:\n",
    "                    print(f\"å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… {delay_seconds} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                else:\n",
    "                    break\n",
    "        return \"âš ï¸ å¯«ä½œè«‹æ±‚å¤±æ•—ï¼šAPI é™åˆ¶æˆ–å…¶ä»–éŒ¯èª¤\"\n",
    "\n",
    "    def generate_text_for_write_operation(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        table_str = table.head(10).to_string() if len(table) > 10 else table.to_string()\n",
    "        WRITE_TOKENS = 50\n",
    "        TABLE_FORMAT = \"Pandas DataFrame as plain text\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "    System :\n",
    "    You are a content writer for the badminton game report .\n",
    "    Please write the Report based on the input Table .\n",
    "\n",
    "    # Requirements\n",
    "    1. Strictly adhere to the requirements .\n",
    "    2. The output must be in English .\n",
    "    3. The output must be based on the input data ; do not hallucinate .\n",
    "    4. The Table format is {TABLE_FORMAT}.\n",
    "    5. The Report can only describe the content included in the Tables and cannot describe anything not included in the Tables .\n",
    "    6. The Report must consist of only one paragraph .\n",
    "    7. The number of tokens in the Report must be within {WRITE_TOKENS}.\n",
    "\n",
    "    # Table Description\n",
    "    {self.table_description}\n",
    "\n",
    "    User :\n",
    "    # Test\n",
    "    ## Tables\n",
    "    {table_str}\n",
    "    ## Report\n",
    "    \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def merge_child_texts(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        GENERATING_TOKENS = 100\n",
    "        reports_str = \"\\n\".join([f\"- {txt}\" for txt in child_texts])\n",
    "        prompt = f\"\"\"\n",
    "    System :\n",
    "    You are a content generator for the badminton game report .\n",
    "    Please merge and rewrite a New Report based on the input Reports .\n",
    "\n",
    "    # Requirements\n",
    "    1. Strictly adhere to the requirements .\n",
    "    2. The output must be in English .\n",
    "    3. The output must be based on the input data ; do not hallucinate .\n",
    "    4. The New Report must include all the content from the input Reports ; do not omit any information .\n",
    "    5. The New Report must follow the order of the input Reports .\n",
    "    6. The number of tokens in the New Report must be within {GENERATING_TOKENS}.\n",
    "\n",
    "    User :\n",
    "    # Test\n",
    "    ## Reports\n",
    "    {reports_str}\n",
    "    ## New Report\n",
    "    \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "# ä¿®æ”¹ TreeOfReport é¡åˆ¥ä¸­ TextGenerator çš„åˆå§‹åŒ–é‚è¼¯\n",
    "class TreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # åˆå§‹åŒ–çµ„ä»¶ï¼Œæ³¨æ„ TextGenerator å‚³å…¥ table_description\n",
    "        self.content_planner = ContentPlanner(api_key)\n",
    "        self.df_operator = DataFrameOperator(api_key)\n",
    "        self.text_generator = TextGenerator(api_key, table_description=self.table_description)\n",
    "\n",
    "    def load_configurations(self):\n",
    "        self.table_description = read_text_file(\"filtered_data_description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å„ç¨®æ¬„ä½ç”¨æ–¼åˆ†æ\"\n",
    "\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        if isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['name'] for op in self.operation_description]\n",
    "        else:\n",
    "            self.operation_pool = list(self.operation_description.keys())\n",
    "\n",
    "        print(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "    \n",
    "    def build_tree(self, root_table: pd.DataFrame) -> TreeNode:\n",
    "        \"\"\"å»ºæ§‹å ±å‘Šæ¨¹\"\"\"\n",
    "        root = TreeNode(level=0, text=\"è³‡æ–™åˆ†æå ±å‘Š\", table=root_table, operation=\"root(None)\")\n",
    "        root.operation_history = ['root(None)']\n",
    "        queue = [root]\n",
    "\n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "\n",
    "            if current_node.operation.lower().startswith('write'):\n",
    "                continue\n",
    "\n",
    "            if current_node.level >= self.max_depth:\n",
    "                write_node = self.create_child_node(current_node, 'write()')\n",
    "                if write_node:\n",
    "                    current_node.add_child(write_node)\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nè™•ç†ç¯€é» - Level: {current_node.level}, Operation: {current_node.operation}\")\n",
    "\n",
    "            tables_str = current_node.table.to_string()\n",
    "            operations = self.content_planner.generate_operations(\n",
    "                tables=tables_str,\n",
    "                table_description=self.table_description,\n",
    "                operation_description=self.operation_description,\n",
    "                operation_history=current_node.operation_history,\n",
    "                operation_pool=self.operation_pool,\n",
    "                max_depth=self.max_depth,\n",
    "                max_degree=self.max_degree\n",
    "            )\n",
    "\n",
    "            print(f\"ç”Ÿæˆæ“ä½œ: {operations}\")\n",
    "\n",
    "            for operation in operations[:self.max_degree]:\n",
    "                if operation.strip():\n",
    "                    child_node = self.create_child_node(current_node, operation)\n",
    "                    if child_node:\n",
    "                        current_node.add_child(child_node)\n",
    "                        queue.append(child_node)\n",
    "\n",
    "        self.generate_all_texts(root)\n",
    "        return root\n",
    "    \n",
    "    def create_child_node(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"å‰µå»ºå­ç¯€é»\"\"\"\n",
    "        try:\n",
    "            # å»ºç«‹æ–°çš„æ“ä½œæ­·å²\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚º write æ“ä½œ\n",
    "            if operation.lower().startswith('write'):\n",
    "                # Write æ“ä½œï¼šç”Ÿæˆæ–‡æœ¬ï¼Œè¡¨æ ¼ä¿æŒä¸è®Š\n",
    "                text = self.text_generator.generate_text_for_write_operation(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                print(f\"å‰µå»º write ç¯€é»: {operation}\")\n",
    "                return child\n",
    "            else:\n",
    "                # å…¶ä»–æ“ä½œï¼šåŸ·è¡Œæ•¸æ“šæ“ä½œ\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    print(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    print(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_texts(self, node: TreeNode):\n",
    "        \"\"\"éæ­¸ç”Ÿæˆæ‰€æœ‰ç¯€é»çš„æ–‡æœ¬\"\"\"\n",
    "        # å…ˆè™•ç†å­ç¯€é»\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts(child)\n",
    "        \n",
    "        # å¦‚æœæ˜¯è‘‰ç¯€é»ä¸”æ²’æœ‰æ–‡æœ¬ï¼ˆé write æ“ä½œï¼‰\n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_text_for_write_operation(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "        # å¦‚æœæœ‰å­ç¯€é»ï¼Œåˆä½µå­ç¯€é»çš„æ–‡æœ¬\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        print(f'ç¯€é»è³‡è¨Š: {node.text}')\n",
    "        \n",
    "    def generate_report(self, node: TreeNode, level: int = 0) -> str:\n",
    "            if node.level == 0:\n",
    "                prompt = f\"\"\"\n",
    "                æ ¹æ“šä»¥ä¸‹åˆ†æç¸½çµï¼Œè«‹æ’°å¯«ä¸€ç¯‡è³½äº‹æ•¸æ“šåˆ†æå ±å°ï¼ŒåŒ…å«ï¼šèµ·ã€æ‰¿ã€è½‰ã€åˆï¼Œæä¾›å…¨é¢æ·±å…¥çš„åˆ†æã€‚\n",
    "                è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ï¼Œè³‡è¨Šæº–ç¢ºã€‚\n",
    "\n",
    "                åˆ†æç¸½çµ:\n",
    "                {node.text}\n",
    "                \"\"\"\n",
    "                final_text = self.text_generator._retry_generate(prompt)\n",
    "                with open(\"tree_of_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(final_text)\n",
    "                return final_text\n",
    "            else:\n",
    "                print(f'generate report from not root')\n",
    "                indent = \"  \" * level\n",
    "                report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "                if node.text:\n",
    "                    report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "                if node.table is not None and not node.table.empty and level < 2:\n",
    "                    report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "                    if len(node.table) <= 10:\n",
    "                        report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "                    else:\n",
    "                        report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "                for child in node.children:\n",
    "                    report += self.generate_report(child, level + 1)\n",
    "\n",
    "                return report\n",
    "\n",
    "\n",
    "\n",
    "# ===== ä¸»ç¨‹åº =====\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•¸ - åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çµæ§‹\"\"\"\n",
    "    \n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"è«‹è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "        return\n",
    "    \n",
    "    print(\"Tree-of-Report for Data Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "    try:\n",
    "        TABLES = pd.read_csv('filtered_set1.csv')\n",
    "        print(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"æ‰¾ä¸åˆ° filtered_set1.csvï¼Œä½¿ç”¨ç¤ºä¾‹æ•¸æ“š\")\n",
    "        # å‰µå»ºç¤ºä¾‹æ•¸æ“š\n",
    "        TABLES = pd.DataFrame({\n",
    "            'type': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B'],\n",
    "            'lose_reason': ['net', 'out', 'net', 'long', 'net', 'out', 'long', 'net'],\n",
    "            'getpoint_player': ['Player1', 'Player2', 'Player1', 'Player2', 'Player1', 'Player2', 'Player1', 'Player2'],\n",
    "            'score': [1, 2, 1, 3, 2, 1, 4, 2]\n",
    "        })\n",
    "    \n",
    "    # è¨­ç½®åƒæ•¸\n",
    "    MAX_DEPTH = 3  # é™ä½æ·±åº¦ä»¥ä¾¿æ¸¬è©¦\n",
    "    MAX_DEGREE = 3  # é™ä½åˆ†æ”¯åº¦ä»¥ä¾¿æ¸¬è©¦\n",
    "    \n",
    "    print(f\"æœ€å¤§æ·±åº¦: {MAX_DEPTH}\")\n",
    "    print(f\"æœ€å¤§åˆ†æ”¯åº¦: {MAX_DEGREE}\")\n",
    "    \n",
    "    # åˆå§‹åŒ– Tree-of-Report\n",
    "    tree_report = TreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "    \n",
    "    # å»ºæ§‹å ±å‘Šæ¨¹\n",
    "    print(\"\\né–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\")\n",
    "    root = tree_report.build_tree(TABLES)\n",
    "    \n",
    "    # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "    print(\"\\nç”Ÿæˆæœ€çµ‚å ±å‘Š...\")\n",
    "    final_report = tree_report.generate_report(root)\n",
    "    \n",
    "    # è¼¸å‡ºå ±å‘Š\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "    print(\"=\"*50)\n",
    "    print(final_report)\n",
    "    \n",
    "    # å„²å­˜å ±å‘Š\n",
    "    with open('tree_of_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Tree-of-Report æ•¸æ“šåˆ†æå ±å‘Š\\n\\n\")\n",
    "        f.write(final_report)\n",
    "    \n",
    "    print(\"å ±å‘Šå·²å„²å­˜è‡³ tree_of_report.md\")\n",
    "    \n",
    "    # æ¸…ç†æš«å­˜æª”æ¡ˆ\n",
    "    for temp_file in ['input_tmp.csv', 'tmp.csv']:\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e90ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:11,808 - INFO - Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\n",
      "2025-06-06 11:24:11,809 - INFO - ==================================================\n",
      "2025-06-06 11:24:11,809 - INFO - æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "2025-06-06 11:24:11,812 - INFO - æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "2025-06-06 11:24:11,812 - INFO - æœ€å¤§æ·±åº¦: 3\n",
      "2025-06-06 11:24:11,812 - INFO - æœ€å¤§åˆ†æ”¯åº¦: 4\n",
      "2025-06-06 11:24:11,814 - INFO - è¼‰å…¥æ“ä½œæ± : ['write', 'select_row', 'select_column', 'group_by', 'aggregate', 'value_counts', 'crosstab', 'pivot_table', 'sort', 'calculate']\n",
      "2025-06-06 11:24:11,815 - INFO - é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\n",
      "2025-06-06 11:24:11,815 - INFO - è™•ç†ç¯€é» - Level: 0, Operation: root(None)\n",
      "2025-06-06 11:24:11,822 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:13,400 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-06-06 11:24:13,400 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(player,type,lose_reason,getpoint_player)', 'value_counts(type)', 'value_counts(lose_reason)', 'value_counts(getpoint_player)']\n",
      "2025-06-06 11:24:16,452 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 4)\n",
      "2025-06-06 11:24:16,453 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(player,type,lose_reason,getpoint_player), çµæœå½¢ç‹€: (315, 4)\n",
      "2025-06-06 11:24:16,454 - INFO - æ·»åŠ å­ç¯€é»: 68962585 to a46eb399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameå·²æˆåŠŸä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:18,701 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:18,702 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:18,703 - INFO - æ·»åŠ å­ç¯€é»: 6766c7a8 to a46eb399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type') æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:20,505 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:20,506 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(lose_reason), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:20,507 - INFO - æ·»åŠ å­ç¯€é»: 6b604cfd to a46eb399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts æ“ä½œå·²å®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:22,720 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-06-06 11:24:22,720 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(getpoint_player), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-06-06 11:24:22,720 - INFO - æ·»åŠ å­ç¯€é»: ad3b8632 to a46eb399\n",
      "2025-06-06 11:24:22,721 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: select_column(player,type,lose_reason,getpoint_player)\n",
      "2025-06-06 11:24:22,724 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts æ“ä½œå·²æˆåŠŸåŸ·è¡Œä¸¦ä¿å­˜åˆ° 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:23,745 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-06-06 11:24:23,745 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'value_counts(lose_reason)']\n",
      "2025-06-06 11:24:25,559 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:25,561 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:25,561 - INFO - æ·»åŠ å­ç¯€é»: 1a48c3b5 to 68962585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type') æ“ä½œå·²å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:27,829 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:27,830 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(lose_reason), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:27,830 - INFO - æ·»åŠ å­ç¯€é»: fe370ae8 to 68962585\n",
      "2025-06-06 11:24:27,831 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(type)\n",
      "2025-06-06 11:24:27,832 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('lose_reason') æ“ä½œæˆåŠŸï¼Œç»“æœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:28,569 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-06-06 11:24:28,572 - INFO - ç”Ÿæˆæ“ä½œ: ['sort(count)']\n",
      "2025-06-06 11:24:30,259 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:30,261 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-06-06 11:24:30,261 - INFO - æ·»åŠ å­ç¯€é»: d7445d2b to 6766c7a8\n",
      "2025-06-06 11:24:30,261 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(lose_reason)\n",
      "2025-06-06 11:24:30,263 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameå·²æˆåŠŸæ’åºä¸¦å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:31,118 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-06-06 11:24:31,118 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(lose_reason, count)', 'sort(count, ascending=False)', 'write()']\n",
      "2025-06-06 11:24:33,906 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:33,906 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(lose_reason, count), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:33,908 - INFO - æ·»åŠ å­ç¯€é»: 496577ff to 6b604cfd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 'input_tmp.csv' and saved the result to 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:35,569 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:35,569 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count, ascending=False), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-06-06 11:24:35,570 - INFO - æ·»åŠ å­ç¯€é»: 50728b8d to 6b604cfd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVæª”æ¡ˆå·²æˆåŠŸè®€å–ã€æ’åºä¸¦å„²å­˜ç‚º 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:24:36,444 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-06-06 11:24:36,445 - INFO - æ·»åŠ å­ç¯€é»: cdb60d60 to 6b604cfd\n",
      "2025-06-06 11:24:36,446 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(getpoint_player)\n",
      "2025-06-06 11:24:36,447 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:37,202 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-06-06 11:24:37,204 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-06-06 11:24:38,277 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-06-06 11:24:38,277 - INFO - æ·»åŠ å­ç¯€é»: 7711d075 to ad3b8632\n",
      "2025-06-06 11:24:38,278 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-06-06 11:24:38,279 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:38,625 - ERROR - Gemini APIè«‹æ±‚å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:38,625 - INFO - ç”Ÿæˆæ“ä½œ: []\n",
      "2025-06-06 11:24:38,625 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(lose_reason)\n",
      "2025-06-06 11:24:38,627 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:38,896 - ERROR - Gemini APIè«‹æ±‚å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:38,897 - INFO - ç”Ÿæˆæ“ä½œ: []\n",
      "2025-06-06 11:24:38,897 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: sort(count)\n",
      "2025-06-06 11:24:38,899 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:39,156 - ERROR - Gemini APIè«‹æ±‚å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:39,156 - INFO - ç”Ÿæˆæ“ä½œ: []\n",
      "2025-06-06 11:24:39,157 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: select_column(lose_reason, count)\n",
      "2025-06-06 11:24:39,158 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:39,669 - ERROR - Gemini APIè«‹æ±‚å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:39,669 - INFO - ç”Ÿæˆæ“ä½œ: []\n",
      "2025-06-06 11:24:39,670 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: sort(count, ascending=False)\n",
      "2025-06-06 11:24:39,672 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-06-06 11:24:39,914 - ERROR - Gemini APIè«‹æ±‚å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:39,914 - INFO - ç”Ÿæˆæ“ä½œ: []\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\217919585.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-06-06 11:24:40,188 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "2025-06-06 11:24:40,189 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "2025-06-06 11:25:11,260 - INFO - ç¯€é» 1a48c3b5 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: å ´ä¸Šå±€å‹¢è† è‘—ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ã€‚é•·çƒä½¿ç”¨æ¬¡æ•¸æœ€å¤šï¼Œé«˜é”55æ¬¡ï¼Œä½†æœªçŸ¥çƒå“¡å¾—åˆ†æ•ˆç‡é©šäººã€‚æ®ºçƒç·Šéš¨å…¶å¾Œï¼Œæœ‰36æ¬¡ï¼Œå¯è¦‹é€²æ”»ç«¯ç«åŠ›åè¶³ï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:12,487 - INFO - ç¯€é» fe370ae8 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­ï¼Œå¯è¦‹å°æ‰‹è½åœ°å¾—åˆ†èˆ‡æˆ‘æ–¹å‡ºç•Œæ˜¯ä¸»è¦å¤±åˆ†å› ç´ ï¼Œå„ä½”12åˆ†ï¼Œæ›ç¶²å¤±èª¤ä¹Ÿä¸å®¹å¿½è¦–ï¼Œä¸Ÿå¤±10åˆ†ã€‚æ¸›å°‘ç„¡è¬‚å¤±èª¤ï¼Œæ˜¯æ¥ä¸‹ä¾†éœ€è¦é‡é»èª¿æ•´çš„æ–¹å‘ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:13,750 - INFO - ç¯€é» 68962585 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\217919585.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:     player  type lose_reason getpoint_player\n",
      "0        B   ç™¼é•·çƒ         NaN             NaN\n",
      "1        A    åˆ‡çƒ         NaN             NaN\n",
      "2        B    æŒ‘çƒ         NaN             NaN\n",
      "3        A    é•·çƒ         NaN             NaN\n",
      "4        B    æ®ºçƒ         NaN             NaN\n",
      "..     ...   ...         ...             ...\n",
      "310      B  æœªçŸ¥çƒç¨®         NaN             NaN\n",
      "311      A    åˆ‡çƒ         NaN             NaN\n",
      "312      B    æŒ‘çƒ         NaN             NaN\n",
      "313      A    é•·çƒ         NaN             NaN\n",
      "314      B    é•·çƒ          å‡ºç•Œ               A\n",
      "\n",
      "[315 rows x 4 columns]\n",
      "ç¯€é»æ–‡æœ¬: çƒå ´å±€å‹¢è† è‘—ï¼Œé›™æ–¹äº’æœ‰æ”»é˜²ã€‚é•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œé”åˆ°55æ¬¡ï¼ŒæœªçŸ¥çƒå“¡å¾—åˆ†æ•ˆç‡é©šäººã€‚æ®ºçƒæ¬¡æ•¸ç·Šéš¨å…¶å¾Œï¼Œå…±36æ¬¡ï¼Œé€²æ”»ç«åŠ›å¼·å‹ã€‚æ¯”è³½ä¸­ï¼Œå°æ‰‹è½åœ°å¾—åˆ†èˆ‡æˆ‘æ–¹å‡ºç•Œç‚ºä¸»è¦å¤±åˆ†å› ç´ ï¼Œå„ä½”12åˆ†ï¼Œæ›ç¶²å¤±èª¤ä¹Ÿé€ æˆ10åˆ†ä¸Ÿå¤±ã€‚æ¸›å°‘ä¸å¿…è¦çš„å¤±èª¤å°‡æ˜¯æœªä¾†èª¿æ•´çš„é‡é»ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:14,787 - INFO - ç¯€é» d7445d2b æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:      type  count\n",
      "0      å¹³çƒ      2\n",
      "1    é˜²å®ˆå›æŒ‘      2\n",
      "2      é»æ‰£      4\n",
      "3    é˜²å®ˆå›æŠ½      5\n",
      "4      æ’²çƒ      5\n",
      "5    éåº¦åˆ‡çƒ      6\n",
      "6   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "7     ç™¼é•·çƒ     10\n",
      "8     ç™¼çŸ­çƒ     10\n",
      "9      å‹¾çƒ     12\n",
      "10   æœªçŸ¥çƒç¨®     16\n",
      "11    æ“‹å°çƒ     20\n",
      "12    æ”¾å°çƒ     28\n",
      "13     åˆ‡çƒ     31\n",
      "14     æ¨çƒ     31\n",
      "15     æŒ‘çƒ     35\n",
      "16     æ®ºçƒ     36\n",
      "17     é•·çƒ     55\n",
      "node.table:      type  count\n",
      "0      å¹³çƒ      2\n",
      "1    é˜²å®ˆå›æŒ‘      2\n",
      "2      é»æ‰£      4\n",
      "3    é˜²å®ˆå›æŠ½      5\n",
      "4      æ’²çƒ      5\n",
      "5    éåº¦åˆ‡çƒ      6\n",
      "6   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "7     ç™¼é•·çƒ     10\n",
      "8     ç™¼çŸ­çƒ     10\n",
      "9      å‹¾çƒ     12\n",
      "10   æœªçŸ¥çƒç¨®     16\n",
      "11    æ“‹å°çƒ     20\n",
      "12    æ”¾å°çƒ     28\n",
      "13     åˆ‡çƒ     31\n",
      "14     æ¨çƒ     31\n",
      "15     æŒ‘çƒ     35\n",
      "16     æ®ºçƒ     36\n",
      "17     é•·çƒ     55\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹åœ¨é•·çƒçš„ä½¿ç”¨ä¸Šéå¸¸é »ç¹ï¼Œé«˜é”55æ¬¡ï¼ŒæŒ‘çƒçš„æ¬¡æ•¸ä¹Ÿä¸å°‘ï¼Œæœ‰35æ¬¡ã€‚å¦å¤–ï¼Œåˆ‡çƒå’Œæ¨çƒçš„æ¬¡æ•¸ä¹Ÿæ——é¼“ç›¸ç•¶ï¼Œåˆ†åˆ¥ç‚º31æ¬¡ï¼Œå¯è¦‹é€™å…©ç¨®æŠ€è¡“æ˜¯é¸æ‰‹å€‘å¸¸ç”¨çš„å¾—åˆ†æ‰‹æ®µã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:15,625 - INFO - ç¯€é» 6766c7a8 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹é »ç¹ä½¿ç”¨é•·çƒï¼Œæ¬¡æ•¸é«˜é”55æ¬¡ï¼ŒæŒ‘çƒä¹Ÿæœ‰35æ¬¡ã€‚åˆ‡çƒå’Œæ¨çƒçš„ä½¿ç”¨æ¬¡æ•¸ç›¸ç•¶ï¼Œåˆ†åˆ¥ç‚º31æ¬¡ï¼Œé¡¯ç¤ºé€™å…©ç¨®æŠ€è¡“æ˜¯é¸æ‰‹å€‘å¸¸ç”¨çš„å¾—åˆ†æ‰‹æ®µã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:16,534 - INFO - ç¯€é» 496577ff æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­ï¼Œå¯è¦‹å°æ‰‹è½åœ°å¾—åˆ†èˆ‡å‡ºç•Œæ˜¯ä¸»è¦å¤±åˆ†é»ï¼Œå„æœ‰12æ¬¡ï¼Œéœ€è¦é‡é»æé˜²ã€‚æ›ç¶²å¤±èª¤ä¹Ÿæœ‰10æ¬¡ï¼Œä¸å¯å¿½è¦–ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:17,737 - INFO - ç¯€é» 50728b8d æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-06-06 11:25:17,739 - INFO - ç¯€é» cdb60d60 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹åœ¨å ´ä¸Šçˆ­å¥ªæ¿€çƒˆï¼Œå¯è¦‹ã€Œå°æ‰‹è½åœ°è‡´å‹ã€èˆ‡ã€Œå‡ºç•Œã€æ˜¯ä¸»è¦å¤±åˆ†åŸå› ï¼Œå„æœ‰12æ¬¡ä¹‹å¤šï¼Œè€Œã€Œæ›ç¶²ã€å¤±èª¤ä¹Ÿæœ‰10æ¬¡ï¼Œé¸æ‰‹éœ€å¤šåŠ æ³¨æ„ã€‚\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: å ´ä¸Šå±€å‹¢è† è‘—ï¼Œé›™æ–¹äº’æœ‰æ”»é˜²ã€‚å¯è¦‹ã€Œå°æ‰‹è½åœ°è‡´å‹ã€èˆ‡ã€Œå‡ºç•Œã€ç‚ºä¸»è¦å¤±åˆ†å› ç´ ï¼Œå„ä½çƒå“¡éœ€è¦å¤šåŠ ç•™æ„ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:18,620 - INFO - ç¯€é» 6b604cfd æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-06-06 11:25:18,622 - INFO - ç¯€é» 7711d075 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹çˆ­å¥ªæ¿€çƒˆï¼Œå±€å‹¢è† è‘—ã€‚ä¸»è¦å¤±åˆ†é»ç‚ºå°æ‰‹è½åœ°å¾—åˆ†èˆ‡å‡ºç•Œï¼Œå„æœ‰12æ¬¡ï¼Œéœ€é‡é»æé˜²ã€‚æ›ç¶²å¤±èª¤äº¦æœ‰10æ¬¡ï¼Œä¸å¯å¿½è¦–ï¼Œçƒå“¡å€‘éœ€å¤šåŠ ç•™æ„ã€‚\n",
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: Aé¸æ‰‹é€²æ”»ç«åŠ›å…¨é–‹ï¼Œæ‹¿ä¸‹å…¨å ´æœ€é«˜çš„21åˆ†ï¼Bé¸æ‰‹ä¹Ÿä¸ç”˜ç¤ºå¼±ï¼ŒåŠªåŠ›è¿½åˆ†ï¼Œå–å¾—15åˆ†ã€‚é›–ç„¶æ•¸æ“šæœªèƒ½é¡¯ç¤ºå…·é«”çš„å¤±åˆ†åŸå› ï¼Œä½†Aé¸æ‰‹çš„å¾—åˆ†èƒ½åŠ›ç„¡ç–‘æ˜¯æœ¬å ´æ¯”è³½çš„ä¸€å¤§äº®é»ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:19,499 - INFO - ç¯€é» ad3b8632 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: Aé¸æ‰‹é€²æ”»ç«åŠ›å…¨é–‹ï¼Œä»¥å…¨å ´æœ€é«˜çš„21åˆ†é ˜å…ˆï¼Bé¸æ‰‹å¥®åŠ›è¿½è¶•ï¼Œç²å¾—15åˆ†ã€‚é›–ç„¶æœªæ˜ç¢ºæŒ‡å‡ºå¤±åˆ†åŸå› ï¼Œä½†Aé¸æ‰‹çš„å¾—åˆ†èƒ½åŠ›æ˜¯æœ¬æ¬¡æ¯”è³½çš„äº®é»ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:20,804 - INFO - ç¯€é» a46eb399 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-06-06 11:25:20,807 - INFO - ç”Ÿæˆæœ€çµ‚å ±å‘Š...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      Unnamed: 0  rally      time  roundscore_A  roundscore_B player  type  \\\n",
      "0             0      1  00:05:47             1             0      B   ç™¼é•·çƒ   \n",
      "1             1      1  00:05:49             1             0      A    åˆ‡çƒ   \n",
      "2             2      1  00:05:50             1             0      B    æŒ‘çƒ   \n",
      "3             3      1  00:05:51             1             0      A    é•·çƒ   \n",
      "4             4      1  00:05:52             1             0      B    æ®ºçƒ   \n",
      "..          ...    ...       ...           ...           ...    ...   ...   \n",
      "310         310     36  00:24:44            21            15      B  æœªçŸ¥çƒç¨®   \n",
      "311         311     36  00:24:58            21            15      A    åˆ‡çƒ   \n",
      "312         312     36  00:25:00            21            15      B    æŒ‘çƒ   \n",
      "313         313     36  00:25:01            21            15      A    é•·çƒ   \n",
      "314         314     36  00:25:02            21            15      B    é•·çƒ   \n",
      "\n",
      "    lose_reason getpoint_player  \n",
      "0           NaN             NaN  \n",
      "1           NaN             NaN  \n",
      "2           NaN             NaN  \n",
      "3           NaN             NaN  \n",
      "4           NaN             NaN  \n",
      "..          ...             ...  \n",
      "310         NaN             NaN  \n",
      "311         NaN             NaN  \n",
      "312         NaN             NaN  \n",
      "313         NaN             NaN  \n",
      "314          å‡ºç•Œ               A  \n",
      "\n",
      "[315 rows x 9 columns]\n",
      "ç¯€é»æ–‡æœ¬: è³‡æ–™åˆ†æå ±å‘Š\n",
      "\n",
      "æœ¬å ´çƒè³½é›™æ–¹å±€å‹¢è† è‘—ï¼Œäº’æœ‰æ”»é˜²ã€‚é•·çƒä½¿ç”¨æœ€é »ç¹ï¼Œé«˜é”55æ¬¡ï¼ŒæŒ‘çƒ35æ¬¡ã€‚åˆ‡çƒå’Œæ¨çƒæ¬¡æ•¸ç›¸è¿‘ï¼Œçš†ç‚º31æ¬¡ã€‚æœªçŸ¥çƒå“¡å¾—åˆ†æ•ˆç‡é©šäººï¼Œæ®ºçƒæ¬¡æ•¸ç‚º36æ¬¡ï¼Œé€²æ”»ç«åŠ›å¼·å‹ã€‚ä¸»è¦å¤±åˆ†å› ç´ ç‚ºå°æ‰‹è½åœ°å¾—åˆ†èˆ‡æˆ‘æ–¹å‡ºç•Œï¼Œå„ä½”12åˆ†ï¼Œæ›ç¶²å¤±èª¤äº¦æœ‰10åˆ†ï¼Œéœ€æ¸›å°‘ä¸å¿…è¦å¤±èª¤ã€‚Aé¸æ‰‹é€²æ”»ç«åŠ›å…¨é–‹ï¼Œä»¥21åˆ†é ˜å…ˆï¼ŒBé¸æ‰‹ç²å¾—15åˆ†ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:25:24,558 - INFO - æ¨¹çµæ§‹å·²å°å‡ºè‡³: tree_structure.json\n",
      "2025-06-06 11:25:24,559 - INFO - \n",
      "==================================================\n",
      "2025-06-06 11:25:24,560 - INFO - TREE-OF-REPORT æœ€çµ‚å ±å‘Š\n",
      "2025-06-06 11:25:24,561 - INFO - ==================================================\n",
      "2025-06-06 11:25:24,563 - INFO - å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: 72.75 ç§’\n",
      "2025-06-06 11:25:24,563 - INFO - ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "2025-06-06 11:25:24,565 - INFO - - tree_of_report.md: æœ€çµ‚å ±å‘Š\n",
      "2025-06-06 11:25:24,566 - INFO - - tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\n",
      "2025-06-06 11:25:24,567 - INFO - - tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\n",
      "2025-06-06 11:25:24,568 - INFO - - execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\n",
      "2025-06-06 11:25:24,569 - INFO - - tree_visualization.html: å¯è¦–åŒ–é é¢\n",
      "2025-06-06 11:25:24,570 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: input_tmp.csv\n",
      "2025-06-06 11:25:24,571 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: tmp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish generate report\n",
      "## çŒ›æ”»å¥æ•ˆï¼ç¾½çƒè³½äº‹æˆ°æ³è† è‘—ï¼ŒAé¸æ‰‹ç«åŠ›å…¨é–‹éšªå‹\n",
      "\n",
      "ç¾½çƒè³½äº‹ç¾å ´æ°£æ°›ç·Šå¼µï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œæ¯”åˆ†å§‹çµ‚ç„¡æ³•æ‹‰é–‹ã€‚æ•´å ´æ¯”è³½å¯è¬‚é«˜ä¾†é«˜å»ï¼Œé•·çƒæˆ°è¡“é »ç¹ä½¿ç”¨ï¼Œé«˜é”55æ¬¡ï¼Œå±•ç¾é¸æ‰‹å€‘å°å ´åœ°æ·±åº¦çš„é«˜åº¦æŒæ¡ã€‚æŒ‘çƒçš„é‹ç”¨ä¹Ÿç›¸ç•¶é—œéµï¼Œå…±è¨ˆ35æ¬¡ï¼Œè©¦åœ–æ“¾äº‚å°æ‰‹ç¯€å¥ã€‚åˆ‡çƒèˆ‡æ¨çƒå‰‡å¦‚å…©æŠŠåˆ©åˆƒï¼Œå„æœ‰31æ¬¡çš„ç²¾æº–æ–½æ”¾ï¼Œè€ƒé©—è‘—é¸æ‰‹çš„ç´°è†©æ‰‹æ„Ÿã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œåœ¨é€™å ´æˆ°è¡“äº¤é‹’ä¸­ï¼ŒAé¸æ‰‹æ†‘è—‰å…¶é©šäººçš„é€²æ”»ç«åŠ›è„«ç©è€Œå‡ºã€‚ä»–çŒ¶å¦‚ä¸€é ­çŒ›ç¸ï¼Œæ®ºçƒæ¬¡æ•¸é«˜é”36æ¬¡ï¼Œè®“å°æ‰‹é›£ä»¥æ‹›æ¶ã€‚åè§€Bé¸æ‰‹ï¼Œé›–å¥®åŠ›æŠµæŠ—ï¼Œå»é›£ä»¥æŠµæ“‹Aé¸æ‰‹çš„å¼·å‹¢é€²æ”»ã€‚\n",
      "\n",
      "å„˜ç®¡å¦‚æ­¤ï¼Œæ¯”è³½ä¸¦éæ¯«ç„¡ç ´ç¶»ã€‚é›™æ–¹é¸æ‰‹åœ¨æ¯”è³½ä¸­éƒ½å‡ºç¾äº†å› å°æ‰‹è½åœ°å¾—åˆ†ã€è‡ªèº«å‡ºç•Œä»¥åŠæ›ç¶²ç­‰å¤±èª¤ï¼Œå…¶ä¸­è½åœ°å¾—åˆ†èˆ‡å‡ºç•Œå„ä½”12åˆ†ï¼Œæ›ç¶²å¤±èª¤äº¦æœ‰10åˆ†ï¼Œçªé¡¯äº†æ¸›å°‘éå—è¿«æ€§å¤±èª¤çš„é‡è¦æ€§ã€‚\n",
      "\n",
      "æœ€çµ‚ï¼ŒAé¸æ‰‹æ†‘è—‰è‘—æ›´å‹ä¸€ç±Œçš„é€²æ”»èƒ½åŠ›ï¼Œä»¥21åˆ†çš„ä½³ç¸¾åŠ›å£“Bé¸æ‰‹çš„15åˆ†ï¼Œéšªå‹å°æ‰‹ã€‚é€™å ´æ¯”è³½ä¸åƒ…å±•ç¾äº†é¸æ‰‹å€‘ç²¾æ¹›çš„çƒæŠ€ï¼Œä¹Ÿæé†’è‘—æˆ‘å€‘ï¼Œåœ¨è¿½æ±‚é€²æ”»çš„åŒæ™‚ï¼Œç©©ç´®ç©©æ‰“ï¼Œæ¸›å°‘ä¸å¿…è¦çš„å¤±èª¤ï¼Œæ‰èƒ½åœ¨æ¿€çƒˆçš„ç«¶çˆ­ä¸­è„«ç©è€Œå‡ºã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import builtins\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„å‡½æ•¸ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # è¿”å›é»˜èªæ“ä½œé›†åˆ\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# ===== æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ =====\n",
    "class TreeNode:\n",
    "    \"\"\"æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ï¼Œå¢åŠ èªæ„é©—è­‰å’Œè¿½è¹¤åŠŸèƒ½\"\"\"\n",
    "    def __init__(self, level: int = 0, text: str = \"\", table: pd.DataFrame = None, operation: str = None):\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.level: int = level\n",
    "        self.text: str = text\n",
    "        self.table: pd.DataFrame = table if table is not None else pd.DataFrame()\n",
    "        self.operation: str = operation\n",
    "        self.parent: Optional['TreeNode'] = None\n",
    "        self.operation_history: List[str] = []\n",
    "        \n",
    "        # æ–°å¢å±¬æ€§ç”¨æ–¼æ”¹é€²åŠŸèƒ½\n",
    "        self.node_id: str = self._generate_node_id()\n",
    "        self.created_at: datetime = datetime.now()\n",
    "        self.validation_errors: List[str] = []\n",
    "        self.table_hash: str = self._calculate_table_hash()\n",
    "        self.semantic_score: float = 0.0\n",
    "        \n",
    "    def _generate_node_id(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆå”¯ä¸€ç¯€é»ID\"\"\"\n",
    "        content = f\"{self.level}_{self.operation}_{datetime.now().isoformat()}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        \n",
    "    def _calculate_table_hash(self) -> str:\n",
    "        \"\"\"è¨ˆç®—è¡¨æ ¼å…§å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨æ–¼æª¢æ¸¬é‡è¤‡\"\"\"\n",
    "        if self.table.empty:\n",
    "            return \"\"\n",
    "        try:\n",
    "            return hashlib.md5(str(self.table.values.tobytes()).encode()).hexdigest()[:8]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def add_child(self, child: 'TreeNode'):\n",
    "        \"\"\"æ·»åŠ å­ç¯€é»ä¸¦é€²è¡Œé©—è­‰\"\"\"\n",
    "        if self._validate_child(child):\n",
    "            child.parent = self\n",
    "            self.children.append(child)\n",
    "            logger.info(f\"æ·»åŠ å­ç¯€é»: {child.node_id} to {self.node_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"å­ç¯€é»é©—è­‰å¤±æ•—: {child.validation_errors}\")\n",
    "    \n",
    "    def _validate_child(self, child: 'TreeNode') -> bool:\n",
    "        \"\"\"é©—è­‰å­ç¯€é»çš„åˆç†æ€§\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„è¡¨æ ¼ç‹€æ…‹\n",
    "        if child.table_hash and child.table_hash == self.table_hash:\n",
    "            if not child.operation.lower().startswith('write'):\n",
    "                errors.append(\"è¡¨æ ¼å…§å®¹æœªç™¼ç”Ÿè®ŠåŒ–ä½†éå¯«ä½œæ“ä½œ\")\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œæ˜¯å¦é‚è¼¯åˆç†\n",
    "        if self._is_redundant_operation(child.operation):\n",
    "            errors.append(f\"æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: {child.operation}\")\n",
    "        \n",
    "        child.validation_errors = errors\n",
    "        return len(errors) == 0\n",
    "    \n",
    "    def _is_redundant_operation(self, operation: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æ“ä½œæ˜¯å¦å†—é¤˜\"\"\"\n",
    "        if len(self.operation_history) < 2:\n",
    "            return False\n",
    "            \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ“ä½œåœ¨è¿‘æœŸæ­·å²ä¸­\n",
    "        recent_ops = self.operation_history[-3:]  # æª¢æŸ¥æœ€è¿‘3å€‹æ“ä½œ\n",
    "        op_name = operation.split('(')[0].lower()\n",
    "        \n",
    "        for hist_op in recent_ops:\n",
    "            if hist_op.split('(')[0].lower() == op_name:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºè‘‰ç¯€é»\"\"\"\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"å°‡ç¯€é»è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self.node_id,\n",
    "            \"level\": self.level,\n",
    "            \"operation\": self.operation,\n",
    "            \"text_preview\": self.text[:100] + \"...\" if len(self.text) > 100 else self.text,\n",
    "            \"table_shape\": list(self.table.shape) if not self.table.empty else [0, 0],\n",
    "            \"table_columns\": list(self.table.columns) if not self.table.empty else [],\n",
    "            \"children_count\": len(self.children),\n",
    "            \"validation_errors\": self.validation_errors,\n",
    "            \"semantic_score\": self.semantic_score,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"table_hash\": self.table_hash\n",
    "        }\n",
    "\n",
    "# ===== æ”¹é€²çš„æ“ä½œè§£æå™¨ =====\n",
    "class OperationParser:\n",
    "    \"\"\"å°ˆé–€è² è²¬è§£æå’Œé©—è­‰æ“ä½œçš„é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.valid_operations = {\n",
    "            'select_column', 'select_row',  'sort', 'calculate',\n",
    "            'group_by', 'value_counts', 'aggregate', 'crosstab','pivot_table', 'write'\n",
    "        }\n",
    "        \n",
    "    def parse_operations(self, response_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"æ”¹é€²çš„æ“ä½œè§£æï¼Œè¿”å›çµæ§‹åŒ–çµæœ\"\"\"\n",
    "        try:\n",
    "            parsed_operations = []\n",
    "            \n",
    "            # å¤šç¨®è§£æç­–ç•¥\n",
    "            operations = self._extract_operations_multiple_strategies(response_text)\n",
    "            \n",
    "            for op_str in operations:\n",
    "                parsed_op = self._parse_single_operation(op_str)\n",
    "                if parsed_op and self._validate_operation(parsed_op):\n",
    "                    parsed_operations.append(parsed_op)\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: {op_str}\")\n",
    "            \n",
    "            return parsed_operations[:5]  # é™åˆ¶æœ€å¤š5å€‹æ“ä½œ\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œå¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_operations_multiple_strategies(self, text: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨å¤šç¨®ç­–ç•¥æå–æ“ä½œ\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç­–ç•¥1: å°‹æ‰¾æ–¹æ‹¬è™Ÿå…§å®¹\n",
    "        bracket_match = re.search(r'\\[(.*?)\\]', text, re.DOTALL)\n",
    "        if bracket_match:\n",
    "            content = bracket_match.group(1)\n",
    "            # ä½¿ç”¨æ­£å‰‡æå–å‡½æ•¸èª¿ç”¨æ ¼å¼\n",
    "            pattern = r'([a-zA-Z_]+\\([^)]*\\))'\n",
    "            ops = re.findall(pattern, content)\n",
    "            operations.extend(ops)\n",
    "        \n",
    "        # ç­–ç•¥2: é€è¡Œè§£æ\n",
    "        if not operations:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '(' in line and ')' in line:\n",
    "                    operations.append(line)\n",
    "        \n",
    "        # ç­–ç•¥3: é€—è™Ÿåˆ†å‰²\n",
    "        if not operations:\n",
    "            parts = text.replace('[', '').replace(']', '').split(',')\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part and '(' in part:\n",
    "                    operations.append(part)\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def _parse_single_operation(self, op_str: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"è§£æå–®å€‹æ“ä½œå­—ç¬¦ä¸²\"\"\"\n",
    "        try:\n",
    "            # ç§»é™¤å¤šé¤˜çš„å­—ç¬¦\n",
    "            op_str = op_str.strip().rstrip(',').strip()\n",
    "            \n",
    "            # æå–æ“ä½œåç¨±å’Œåƒæ•¸\n",
    "            if '(' not in op_str:\n",
    "                return {\"name\": op_str, \"args\": [], \"raw\": op_str}\n",
    "            \n",
    "            name_part = op_str.split('(')[0].strip()\n",
    "            args_part = op_str[op_str.find('(')+1:op_str.rfind(')')].strip()\n",
    "            \n",
    "            # è§£æåƒæ•¸\n",
    "            args = []\n",
    "            if args_part:\n",
    "                # ç°¡å–®çš„åƒæ•¸åˆ†å‰²ï¼ˆå¯ä»¥é€²ä¸€æ­¥æ”¹é€²ï¼‰\n",
    "                for arg in args_part.split(','):\n",
    "                    arg = arg.strip().strip('\\'\"')\n",
    "                    if arg:\n",
    "                        args.append(arg)\n",
    "            \n",
    "            return {\n",
    "                \"name\": name_part.lower(),\n",
    "                \"args\": args,\n",
    "                \"raw\": op_str\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œ '{op_str}' å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_operation(self, operation: Dict[str, Any]) -> bool:\n",
    "        \"\"\"é©—è­‰æ“ä½œçš„æœ‰æ•ˆæ€§\"\"\"\n",
    "        name = operation.get(\"name\", \"\").lower()\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "        if name not in self.valid_operations:\n",
    "            logger.warning(f\"æœªçŸ¥æ“ä½œ: {name}\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥ç‰¹å®šæ“ä½œçš„åƒæ•¸\n",
    "        args = operation.get(\"args\", [])\n",
    "        \n",
    "        if name in ['select_column', 'sort_values'] and not args:\n",
    "            logger.warning(f\"{name} æ“ä½œéœ€è¦åƒæ•¸\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ===== æ”¹é€²çš„å…§å®¹è¦åŠƒå™¨ =====\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.parser = OperationParser()\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        æ”¹é€²çš„æ“ä½œç”Ÿæˆï¼ŒåŠ å…¥é‡è¤‡æª¢æ¸¬å’Œèªæ„é©—è­‰\n",
    "        \"\"\"\n",
    "        \n",
    "        # æª¢æ¸¬è¿‘æœŸæ“ä½œï¼Œé¿å…é‡è¤‡\n",
    "        recent_operations = self._extract_recent_operations(operation_history)\n",
    "        \n",
    "        # æ§‹å»ºæ”¹é€²çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}  and more than zero.\n",
    "6. Only select Opertions from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "13. AVOID repeating recent operations: {recent_operations}\n",
    "14. Prioritize operations that will meaningfully transform the data.\n",
    "15. Avoid give the arguments that not match by the operation.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path) if os.path.exists(outline_path) else \"Generate comprehensive data analysis\"}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                logger.info(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                parsed_ops = self.parser.parse_operations(response.text.strip())\n",
    "                return [op[\"raw\"] for op in parsed_ops]  # è¿”å›åŸå§‹å­—ç¬¦ä¸²æ ¼å¼\n",
    "            else:\n",
    "                logger.warning(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_recent_operations(self, operation_history: List[str]) -> List[str]:\n",
    "        \"\"\"æå–æœ€è¿‘çš„æ“ä½œåç¨±\"\"\"\n",
    "        recent = []\n",
    "        for op in operation_history[-3:]:  # æœ€è¿‘3å€‹æ“ä½œ\n",
    "            if '(' in op:\n",
    "                name = op.split('(')[0].strip()\n",
    "                recent.append(name)\n",
    "        return recent\n",
    "\n",
    "# ===== å®‰å…¨çš„DataFrameæ“ä½œå™¨ =====\n",
    "class SafeDataFrameOperator:\n",
    "    \"\"\"å®‰å…¨çš„DataFrameæ“ä½œå™¨ï¼Œä½¿ç”¨ASTé©—è­‰è€Œéç›´æ¥exec\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.allowed_modules = {'pandas', 'numpy', 're'}\n",
    "        self.allowed_functions = {\n",
    "            'pd.read_csv', 'pd.DataFrame', 'df.head', 'df.tail', 'df.sort_values',\n",
    "            'df.groupby', 'df.filter', 'df.select', 'df.drop', 'df.fillna',\n",
    "            'df.to_csv', 'df.value_counts', 'df.describe', 'df.info'\n",
    "        }\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path=\"input_tmp.csv\"):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        1. è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'\n",
    "        2. åªä½¿ç”¨pandasåŸºæœ¬æ“ä½œï¼Œé¿å…è¤‡é›œçš„è‡ªå®šç¾©å‡½æ•¸\n",
    "        3. ç¢ºä¿ä»£ç¢¼å®‰å…¨ï¼Œä¸åŒ…å«æ–‡ä»¶ç³»çµ±æ“ä½œï¼ˆé™¤äº†æŒ‡å®šçš„CSVè®€å¯«ï¼‰\n",
    "        4. æ’°å¯«å®Œæ•´python codeï¼ŒåŒ…å«éŒ¯èª¤è™•ç†\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=2):\n",
    "        \"\"\"å¸¶é‡è©¦çš„ç”Ÿæˆè«‹æ±‚\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    import time\n",
    "                    time.sleep(1)\n",
    "        return \"\"\n",
    "\n",
    "    def safe_execute(self, code: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"å®‰å…¨åŸ·è¡Œç”Ÿæˆçš„ä»£ç¢¼\"\"\"\n",
    "        try:\n",
    "            # æå–ä»£ç¢¼å¡Š\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            #print(f'python code: {code_block}')\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # ASTå®‰å…¨é©—è­‰\n",
    "            if not self._validate_code_safety(code):\n",
    "                logger.error(\"ä»£ç¢¼å®‰å…¨é©—è­‰å¤±æ•—\")\n",
    "                return df\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆ\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            allowed_builtin_names = [\n",
    "                'int', 'float', 'str', 'bool', 'list', 'dict', 'set', 'tuple',\n",
    "                'len', 'range', 'enumerate', 'zip', 'min', 'max', 'sum', 'abs',\n",
    "                'print',\n",
    "                'Exception', 'TypeError', 'ValueError', 'KeyError', 'IndexError',\n",
    "                'FileNotFoundError', 'ZeroDivisionError', 'AttributeError', 'ImportError',\n",
    "                '__import__'\n",
    "            ]\n",
    "\n",
    "            safe_globals = {\n",
    "                'pd': pd,\n",
    "                '__name__': '__main__',\n",
    "                '__builtins__': {name: getattr(builtins, name) for name in allowed_builtin_names}\n",
    "            }\n",
    "\n",
    "            safe_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œä»£ç¢¼\n",
    "            exec(code, safe_globals, safe_locals)\n",
    "\n",
    "            # è®€å–çµæœ\n",
    "            if os.path.exists(\"tmp.csv\"):\n",
    "                result_df = pd.read_csv(\"tmp.csv\")\n",
    "                logger.info(f\"æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                return result_df\n",
    "            else:\n",
    "                logger.warning(\"æœªç”Ÿæˆçµæœæ–‡ä»¶ï¼Œè¿”å›åŸå§‹DataFrame\")\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            print(\"éŒ¯èª¤ä»£ç¢¼å¦‚ä¸‹ï¼š\\n\" + \"-\" * 30)\n",
    "            print(code)  # âœ… è¼¸å‡ºé€ æˆéŒ¯èª¤çš„ç¨‹å¼ç¢¼\n",
    "            print(\"-\" * 30)\n",
    "            logger.error(error_msg)\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_code_safety(self, code: str) -> bool:\n",
    "        \"\"\"ä½¿ç”¨ASTé©—è­‰ä»£ç¢¼å®‰å…¨æ€§\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # æª¢æŸ¥å±éšªçš„å‡½æ•¸èª¿ç”¨\n",
    "                if isinstance(node, ast.Call):\n",
    "                    if isinstance(node.func, ast.Name):\n",
    "                        func_name = node.func.id\n",
    "                        if func_name in ['exec', 'eval', 'compile', '__import__', 'open']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°å±éšªå‡½æ•¸: {func_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥æ–‡ä»¶æ“ä½œï¼ˆé™¤äº†å…è¨±çš„CSVæ“ä½œï¼‰\n",
    "                if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
    "                    if hasattr(node.func, 'attr'):\n",
    "                        attr_name = node.func.attr\n",
    "                        if attr_name in ['system', 'popen', 'subprocess']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ç³»çµ±èª¿ç”¨: {attr_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥å°å…¥èªå¥\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for alias in node.names:\n",
    "                        if alias.name not in self.allowed_modules:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ä¸å…è¨±çš„æ¨¡çµ„å°å…¥: {alias.name}\")\n",
    "                            return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ASTé©—è­‰å¤±æ•—: {e}\")\n",
    "            return False\n",
    "\n",
    "# ===== æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "import time\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "\n",
    "    def extract_highlights_from_table(self, table: pd.DataFrame) -> str:\n",
    "        try:\n",
    "            if 'lose_reason' in table.columns:\n",
    "                top_reason = table['lose_reason'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_reason = \"ç„¡è³‡æ–™\"\n",
    "            if 'getpoint_player' in table.columns:\n",
    "                top_player = table['getpoint_player'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_player = \"æœªçŸ¥çƒå“¡\"\n",
    "            return f\"æœ€å¤šå¤±åˆ†åŸå› ç‚ºã€Œ{top_reason}ã€ï¼Œå¾—åˆ†æœ€å¤šçš„æ˜¯ {top_player}ã€‚\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_table_features(self, table: pd.DataFrame) -> str:\n",
    "        summary = []\n",
    "        for col in table.columns:\n",
    "            dtype = str(table[col].dtype)\n",
    "            line = f\"æ¬„ä½ã€Œ{col}ã€é¡å‹ï¼š{dtype}\"\n",
    "\n",
    "            # é¡¯ç¤ºå¸¸è¦‹å€¼åƒ…é™é¡åˆ¥å‹æ¬„ä½\n",
    "            if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
    "                top_values = table[col].value_counts().head(3).to_dict()\n",
    "                line += f\"ï¼Œå¸¸è¦‹å€¼ï¼š{list(top_values.keys())}\"\n",
    "            summary.append(line)\n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=3, delay_seconds=30):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                logger.error(f\"Gemini å›æ‡‰å¤±æ•—: {err}\")\n",
    "                if \"429\" in err:\n",
    "                    logger.info(f\"å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… {delay_seconds} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                else:\n",
    "                    break\n",
    "        return \"âš ï¸ å¯«ä½œè«‹æ±‚å¤±æ•—ï¼šAPI é™åˆ¶æˆ–å…¶ä»–éŒ¯èª¤\"\n",
    "\n",
    "    def generate_text_for_write_operation(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        table_str = table.to_string()\n",
    "        WRITE_TOKENS = 50\n",
    "        TABLE_FORMAT = \"Pandas DataFrame as plain text\"\n",
    "        highlight_summary = self.extract_highlights_from_table(table)\n",
    "        table_feature_summary = self.extract_table_features(table)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a professional content writer for the badminton game report .\n",
    "Please write the Report based on the input Table, just pick one or two lightspots.\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The Table format is {TABLE_FORMAT}.\n",
    "5. The Report can only describe the content included in the Tables and cannot describe anything not included in the Tables .\n",
    "6. The Report must consist of only one paragraph .\n",
    "7. The number of tokens in the Report must be within {WRITE_TOKENS}.\n",
    "8. è«‹å°ˆæ³¨æè¿°å¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼ã€é—œéµæ¬„ä½è¶¨å‹¢æˆ–çƒå“¡äº®é»ã€‚\n",
    "9. è«‹æ¨¡ä»¿æ¯”è³½è½‰æ’­å“¡æˆ–æ•™ç·´çš„èªæ°£æè¿°ï¼Œå¥å¼è‡ªç„¶ã€æœ‰ç¯€å¥æ„Ÿã€‚\n",
    "10. è«‹ç‰¹åˆ¥è§€å¯Ÿçƒç¨®ä¹‹é–“çš„é€£çºŒè½‰æ›ï¼Œä¾‹å¦‚ æ”¾å°çƒ æ¥ æ®ºçƒ ç­‰ï¼Œæ‰¾å‡ºå…¶ä¸­æœ‰æ•ˆå¾—åˆ†æˆ–ä¸å°‹å¸¸çš„çµ„åˆä¸¦æè¿°ã€‚\n",
    "\n",
    "# Highlights Summary\n",
    "{highlight_summary}\n",
    "\n",
    "# Table Features\n",
    "{table_feature_summary}\n",
    "\n",
    "# Table Description\n",
    "{self.table_description}\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Tables\n",
    "{table_str}\n",
    "## Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def merge_child_texts(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        GENERATING_TOKENS = 100\n",
    "        reports_str = \"\\n\".join([f\"- {txt}\" for txt in child_texts])\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a content generator for the badminton game report .\n",
    "Please merge and rewrite a New Report based on the input Reports .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The New Report must include all the content from the input Reports ; do not omit any information .\n",
    "5. The New Report must follow the order of the input Reports .\n",
    "6. The number of tokens in the New Report must be within {GENERATING_TOKENS}.\n",
    "7. è«‹ä¾åºæ•´åˆæ¯æ®µå…§å®¹ï¼Œå½¢æˆçµæ§‹æ¸…æ™°çš„æ®µè½ï¼ŒåŒ…æ‹¬äº®é»ã€å¤±èª¤æ¨¡å¼èˆ‡çƒå“¡è²¢ç»ã€‚\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Reports\n",
    "{reports_str}\n",
    "## New Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "# ===== OperationParser._validate_operation å¼·åŒ–åƒæ•¸é©—è­‰ï¼ˆè£œå…¥ df æ¬„ä½æ¯”å°ï¼‰ =====\n",
    "def validate_operation_with_columns(operation: Dict[str, Any], df_columns: List[str]) -> bool:\n",
    "    name = operation.get(\"name\", \"\").lower()\n",
    "    args = operation.get(\"args\", [])\n",
    "\n",
    "    # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "    if name not in {\n",
    "        'select_column', 'select_row', 'sort', 'calculate',\n",
    "        'group_by', 'value_counts', 'aggregate', 'crosstab', 'pivot_table', 'write'\n",
    "    }:\n",
    "        return False\n",
    "\n",
    "    # åƒ…é‡å°éœ€åƒæ•¸æ“ä½œæª¢æŸ¥æ¬„ä½\n",
    "    if name in ['select_column', 'sort', 'group_by']:\n",
    "        for arg in args:\n",
    "            if arg not in df_columns:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# ===== æ”¹é€²çš„TreeOfReporté¡åˆ¥ =====\n",
    "class TreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # åˆå§‹åŒ–æ”¹é€²çš„çµ„ä»¶\n",
    "        self.content_planner = ContentPlanner(api_key)\n",
    "        self.df_operator = SafeDataFrameOperator(api_key)  # ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬\n",
    "        self.text_generator = TextGenerator(api_key, table_description=self.table_description)\n",
    "        \n",
    "        # æ–°å¢è¿½è¹¤åŠŸèƒ½\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        self.node_registry: Dict[str, TreeNode] = {}\n",
    "\n",
    "    def load_configurations(self):\n",
    "        self.table_description = read_text_file(\"filtered_data _description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å„ç¨®æ¬„ä½ç”¨æ–¼åˆ†æ\"\n",
    "\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        if isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['name'] for op in self.operation_description]\n",
    "        else:\n",
    "            self.operation_pool = list(self.operation_description.keys())\n",
    "\n",
    "        logger.info(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "    def build_tree(self, root_table: pd.DataFrame) -> TreeNode:\n",
    "        \"\"\"æ”¹é€²çš„æ¨¹æ§‹å»ºï¼ŒåŠ å…¥å®Œæ•´çš„è¿½è¹¤å’Œé©—è­‰\"\"\"\n",
    "        root = TreeNode(level=0, text=\"è³‡æ–™åˆ†æå ±å‘Š\", table=root_table, operation=\"root(None)\")\n",
    "        root.operation_history = ['root(None)']\n",
    "        self.node_registry[root.node_id] = root\n",
    "        \n",
    "        queue = [root]\n",
    "        \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            \n",
    "            # è¨˜éŒ„è™•ç†æ—¥èªŒ\n",
    "            self._log_node_processing(current_node)\n",
    "\n",
    "            if current_node.operation.lower().startswith('write'):\n",
    "                continue\n",
    "\n",
    "            if current_node.level >= self.max_depth:\n",
    "                write_node = self.create_child_node(current_node, 'write()')\n",
    "                if write_node:\n",
    "                    current_node.add_child(write_node)\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"è™•ç†ç¯€é» - Level: {current_node.level}, Operation: {current_node.operation}\")\n",
    "\n",
    "            tables_str = current_node.table.to_string()\n",
    "            operations = self.content_planner.generate_operations(\n",
    "                tables=tables_str,\n",
    "                table_description=self.table_description,\n",
    "                operation_description=self.operation_description,\n",
    "                operation_history=current_node.operation_history,\n",
    "                operation_pool=self.operation_pool,\n",
    "                max_depth=self.max_depth,\n",
    "                max_degree=self.max_degree\n",
    "            )\n",
    "\n",
    "            logger.info(f\"ç”Ÿæˆæ“ä½œ: {operations}\")\n",
    "\n",
    "            for operation in operations[:self.max_degree]:\n",
    "                if operation.strip():\n",
    "                    child_node = self.create_child_node(current_node, operation)\n",
    "                    if child_node:\n",
    "                        current_node.add_child(child_node)\n",
    "                        queue.append(child_node)\n",
    "\n",
    "        self.generate_all_texts(root)\n",
    "        return root\n",
    "    \n",
    "    def _log_node_processing(self, node: TreeNode):\n",
    "        \"\"\"è¨˜éŒ„ç¯€é»è™•ç†æ—¥èªŒ\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"node_id\": node.node_id,\n",
    "            \"level\": node.level,\n",
    "            \"operation\": node.operation,\n",
    "            \"table_shape\": list(node.table.shape) if not node.table.empty else [0, 0],\n",
    "            \"validation_errors\": node.validation_errors\n",
    "        }\n",
    "        self.execution_log.append(log_entry)\n",
    "    \n",
    "    def create_child_node(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"æ”¹é€²çš„å­ç¯€é»å‰µå»ºï¼ŒåŠ å…¥å®Œæ•´é©—è­‰\"\"\"\n",
    "        try:\n",
    "            # å»ºç«‹æ–°çš„æ“ä½œæ­·å²\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚º write æ“ä½œ\n",
    "            if operation.lower().startswith('write'):\n",
    "                text = self.text_generator.generate_text_for_write_operation(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                self.node_registry[child.node_id] = child\n",
    "                logger.info(f\"å‰µå»º write ç¯€é»: {operation}\")\n",
    "                return child\n",
    "            else:\n",
    "                # å…¶ä»–æ“ä½œï¼šåŸ·è¡Œæ•¸æ“šæ“ä½œ\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    self.node_registry[child.node_id] = child\n",
    "                    logger.info(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_texts(self, node: TreeNode):\n",
    "        \"\"\"éæ­¸ç”Ÿæˆæ‰€æœ‰ç¯€é»çš„æ–‡æœ¬\"\"\"\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts(child)\n",
    "        \n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_text_for_write_operation(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "            print(f'node table: {node.table}')\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        logger.info(f'ç¯€é» {node.node_id} æ–‡æœ¬ç”Ÿæˆå®Œæˆ')\n",
    "        print(f'node.table: {node.table}')\n",
    "        print(f'ç¯€é»æ–‡æœ¬: {node.text}')\n",
    "        \n",
    "    def export_tree_structure(self, root: TreeNode, output_path: str = \"tree_structure.json\"):\n",
    "        \"\"\"å°å‡ºæ¨¹çµæ§‹ç‚ºJSONæ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–å’Œåˆ†æ\"\"\"\n",
    "        def node_to_dict(node: TreeNode) -> Dict[str, Any]:\n",
    "            result = node.to_dict()\n",
    "            result[\"children\"] = [node_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_time\": datetime.now().isoformat(),\n",
    "                \"total_nodes\": len(self.node_registry),\n",
    "                \"max_depth\": self.max_depth,\n",
    "                \"max_degree\": self.max_degree\n",
    "            },\n",
    "            \"execution_log\": self.execution_log,\n",
    "            \"tree\": node_to_dict(root)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(tree_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"æ¨¹çµæ§‹å·²å°å‡ºè‡³: {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å°å‡ºæ¨¹çµæ§‹å¤±æ•—: {e}\")\n",
    "    \n",
    "    def generate_execution_report(self) -> str:\n",
    "        \"\"\"ç”ŸæˆåŸ·è¡Œéç¨‹å ±å‘Š\"\"\"\n",
    "        total_nodes = len(self.node_registry)\n",
    "        error_nodes = sum(1 for node in self.node_registry.values() if node.validation_errors)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Tree-of-Report åŸ·è¡Œå ±å‘Š\n",
    "\n",
    "## çµ±è¨ˆä¿¡æ¯\n",
    "- ç¸½ç¯€é»æ•¸: {total_nodes}\n",
    "- éŒ¯èª¤ç¯€é»æ•¸: {error_nodes}\n",
    "- æ¨¹æœ€å¤§æ·±åº¦: {self.max_depth}\n",
    "- æœ€å¤§åˆ†æ”¯åº¦: {self.max_degree}\n",
    "\n",
    "## ç¯€é»åˆ†å¸ƒ\n",
    "\"\"\"\n",
    "        \n",
    "        # æŒ‰å±¤ç´šçµ±è¨ˆç¯€é»\n",
    "        level_counts = {}\n",
    "        for node in self.node_registry.values():\n",
    "            level = node.level\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "        \n",
    "        for level, count in sorted(level_counts.items()):\n",
    "            report += f\"- Level {level}: {count} å€‹ç¯€é»\\n\"\n",
    "        \n",
    "        # éŒ¯èª¤æ‘˜è¦\n",
    "        if error_nodes > 0:\n",
    "            report += \"\\n## é©—è­‰éŒ¯èª¤æ‘˜è¦\\n\"\n",
    "            for node in self.node_registry.values():\n",
    "                if node.validation_errors:\n",
    "                    report += f\"- ç¯€é» {node.node_id} ({node.operation}): {'; '.join(node.validation_errors)}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def generate_report(self, node: TreeNode, level: int = 0) -> str:\n",
    "        \"\"\"æ”¹é€²çš„å ±å‘Šç”Ÿæˆ\"\"\"\n",
    "        if node.level == 0:\n",
    "            prompt = f\"\"\"\n",
    "            ä½ æ˜¯ä¸€ä½æ–°èè¨˜è€…ï¼Œæ ¹æ“šä»¥ä¸‹åˆ†æç¸½çµï¼Œè«‹æ’°å¯«ä¸€ç¯‡è³½äº‹æ–°èå ±å°ï¼Œæä¾›å…¨é¢æ·±å…¥çš„åˆ†æï¼Œçµ±æ•´æˆæ–°èå ±å°ï¼Œæ–‡è¾­ä¸­éå¤šç›´æ¥ä½¿ç”¨æ¬„ä½åç¨±èˆ‡ç›´æ¥æ¬¡æ•¸çµ±è¨ˆï¼Œç”¨player_Aèˆ‡player_Bè¡¨ç¤ºå…©çƒå“¡ï¼Œç”¨ç”Ÿå‹•çš„æ–‡å¥æè¿°ï¼Œå‹¿å‡ºç¾ç´¯è´…çš„å¥å­ï¼Œè«‹å¾åˆ†æç¸½çµä¸­æå–è½‰æ›ï¼Œç¦æ­¢å‡ºç¾å¹»è¦ºã€‚\n",
    "            è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ï¼Œè³‡è¨Šæº–ç¢ºã€‚\n",
    "\n",
    "            åˆ†æç¸½çµ:\n",
    "            {node.text}\n",
    "            \"\"\"\n",
    "            final_text = self.text_generator._retry_generate(prompt)\n",
    "            \n",
    "            # ä¿å­˜å¤šç¨®æ ¼å¼çš„å ±å‘Š\n",
    "            with open(\"tree_of_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            \n",
    "            # å°å‡ºæ¨¹çµæ§‹\n",
    "            self.export_tree_structure(node)\n",
    "            \n",
    "            # ç”ŸæˆåŸ·è¡Œå ±å‘Š\n",
    "            exec_report = self.generate_execution_report()\n",
    "            with open(\"execution_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(exec_report)\n",
    "                print(\"finish generate report\")\n",
    "            \n",
    "            return final_text\n",
    "        else:\n",
    "            logger.info(f'generate report from not root')\n",
    "            indent = \"  \" * level\n",
    "            report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "            if node.text:\n",
    "                report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "            if node.table is not None and not node.table.empty and level < 2:\n",
    "                report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "                if len(node.table) <= 10:\n",
    "                    report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "                else:\n",
    "                    report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "            for child in node.children:\n",
    "                report += self.generate_report(child, level + 1)\n",
    "\n",
    "            return report\n",
    "\n",
    "\n",
    "# ===== ä¸»ç¨‹åº =====\n",
    "def main():\n",
    "    \"\"\"æ”¹é€²çš„ä¸»å‡½æ•¸\"\"\"\n",
    "    \n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        logger.error(\"è«‹è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    logger.info(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "    try:\n",
    "        TABLES = pd.read_csv('filtered_set1.csv')\n",
    "        logger.info(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(\"æ‰¾ä¸åˆ° filtered_set1.csvï¼Œä½¿ç”¨ç¤ºä¾‹æ•¸æ“š\")\n",
    "        # å‰µå»ºç¤ºä¾‹æ•¸æ“š\n",
    "        TABLES = pd.DataFrame({\n",
    "            'type': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B'],\n",
    "            'lose_reason': ['net', 'out', 'net', 'long', 'net', 'out', 'long', 'net'],\n",
    "            'getpoint_player': ['Player1', 'Player2', 'Player1', 'Player2', 'Player1', 'Player2', 'Player1', 'Player2'],\n",
    "            'score': [1, 2, 1, 3, 2, 1, 4, 2]\n",
    "        })\n",
    "    \n",
    "    # è¨­ç½®åƒæ•¸\n",
    "    MAX_DEPTH = 3\n",
    "    MAX_DEGREE = 4\n",
    "    \n",
    "    logger.info(f\"æœ€å¤§æ·±åº¦: {MAX_DEPTH}\")\n",
    "    logger.info(f\"æœ€å¤§åˆ†æ”¯åº¦: {MAX_DEGREE}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ”¹é€²çš„ Tree-of-Report\n",
    "    tree_report = TreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "    \n",
    "    # å»ºæ§‹å ±å‘Šæ¨¹\n",
    "    logger.info(\"é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        root = tree_report.build_tree(TABLES)\n",
    "        \n",
    "        # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "        logger.info(\"ç”Ÿæˆæœ€çµ‚å ±å‘Š...\")\n",
    "        final_report = tree_report.generate_report(root)\n",
    "        \n",
    "        # è¼¸å‡ºå ±å‘Š\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(\"TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"=\"*50)\n",
    "        print(final_report)\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        with open('tree_of_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Tree-of-Report æ•¸æ“šåˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ)\\n\\n\")\n",
    "            f.write(final_report)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        logger.info(f\"å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’\")\n",
    "        logger.info(\"ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        logger.info(\"- tree_of_report.md: æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"- tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\")\n",
    "        logger.info(\"- tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\")\n",
    "        logger.info(\"- execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\")\n",
    "        logger.info(\"- tree_visualization.html: å¯è¦–åŒ–é é¢\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # æ¸…ç†æš«å­˜æª”æ¡ˆ\n",
    "        for temp_file in ['input_tmp.csv', 'tmp.csv']:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                    logger.info(f\"æ¸…ç†æš«å­˜æª”æ¡ˆ: {temp_file}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:17:50,505 - INFO - Enhanced Tree-of-Report for Sports News Generation\n",
      "2025-06-06 12:17:50,507 - INFO - ============================================================\n",
      "2025-06-06 12:17:50,507 - INFO - æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "2025-06-06 12:17:50,509 - INFO - æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "2025-06-06 12:17:50,510 - INFO - è¼‰å…¥æ“ä½œæ± : ['write', 'select_row', 'select_column', 'group_by', 'aggregate', 'value_counts', 'crosstab', 'pivot_table', 'sort', 'calculate']\n",
      "2025-06-06 12:17:50,510 - INFO - é–‹å§‹å»ºæ§‹å¢å¼·ç‰ˆå ±å‘Šæ¨¹...\n",
      "2025-06-06 12:17:50,520 - ERROR - å‰µå»ºå­ç¯€é»å¤±æ•—: Object of type int64 is not JSON serializable\n",
      "2025-06-06 12:17:52,337 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 9)\n",
      "2025-06-06 12:17:52,338 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_row, çµæœå½¢ç‹€: (315, 9)\n",
      "2025-06-06 12:17:52,345 - ERROR - å‰µå»ºå­ç¯€é»å¤±æ•—: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully read, processed, and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:17:54,946 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (258, 9)\n",
      "2025-06-06 12:17:54,946 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_row, çµæœå½¢ç‹€: (258, 9)\n",
      "2025-06-06 12:17:54,954 - ERROR - å‰µå»ºå­ç¯€é»å¤±æ•—: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame å·²æˆåŠŸå„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:17:56,504 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (258, 9)\n",
      "2025-06-06 12:17:56,505 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_row, çµæœå½¢ç‹€: (258, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameæ“ä½œæˆåŠŸä¸¦å·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:17:58,560 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (258, 8)\n",
      "2025-06-06 12:17:58,561 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column, çµæœå½¢ç‹€: (258, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameè™•ç†å®Œæˆä¸¦å·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:00,665 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (31, 3)\n",
      "2025-06-06 12:18:00,665 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: group_by, çµæœå½¢ç‹€: (31, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_by æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:02,589 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 3)\n",
      "2025-06-06 12:18:02,589 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column, çµæœå½¢ç‹€: (315, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully processed and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:04,308 - INFO - å‰µå»º write ç¯€é»: write (å“è³ªåˆ†æ•¸: 0.80)\n",
      "2025-06-06 12:18:07,124 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (0, 3)\n",
      "2025-06-06 12:18:07,124 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_row, çµæœå½¢ç‹€: (0, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸä¿å­˜ä¿®æ”¹å¾Œçš„DataFrameåˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:08,541 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 2)\n",
      "2025-06-06 12:18:08,542 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column, çµæœå½¢ç‹€: (315, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file processed and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:10,514 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 2)\n",
      "2025-06-06 12:18:10,514 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: group_by, çµæœå½¢ç‹€: (315, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç™¼ç”ŸéŒ¯èª¤ï¼šname 'all' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:12,328 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (31, 3)\n",
      "2025-06-06 12:18:12,329 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: group_by, çµæœå½¢ç‹€: (31, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_by æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:13,772 - INFO - å‰µå»º write ç¯€é»: write (å“è³ªåˆ†æ•¸: 0.85)\n",
      "2025-06-06 12:18:16,380 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (0, 3)\n",
      "2025-06-06 12:18:16,380 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_row, çµæœå½¢ç‹€: (0, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully processed and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:18,196 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (31, 2)\n",
      "2025-06-06 12:18:18,197 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column, çµæœå½¢ç‹€: (31, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¬„ä½é¸æ“‡å®Œæˆä¸¦å·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:20,219 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (31, 3)\n",
      "2025-06-06 12:18:20,219 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: group_by, çµæœå½¢ç‹€: (31, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_byæ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:21,917 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 2)\n",
      "2025-06-06 12:18:21,918 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column, çµæœå½¢ç‹€: (315, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¸å–æ¬„ä½å®Œæˆï¼Œä¸¦å·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 12:18:22,185 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 35\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:24,431 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:26,658 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:26,659 - INFO - å‰µå»º write ç¯€é»: write (å“è³ªåˆ†æ•¸: 0.40)\n",
      "2025-06-06 12:18:26,920 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:29,169 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:31,418 - ERROR - ç”Ÿæˆå¤±æ•— (å˜—è©¦ 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 26\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:31,419 - INFO - å‰µå»º write ç¯€é»: write (å“è³ªåˆ†æ•¸: 0.40)\n",
      "2025-06-06 12:18:32,194 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:33,420 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:33,420 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: select_row\n",
      "2025-06-06 12:18:33,646 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:34,877 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:34,878 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: select_column\n",
      "2025-06-06 12:18:35,700 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:36,929 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:36,930 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: group_by\n",
      "2025-06-06 12:18:37,750 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 20\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:38,986 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:38,987 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: select_row\n",
      "2025-06-06 12:18:39,778 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:40,998 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:40,998 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: select_column\n",
      "2025-06-06 12:18:41,884 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:43,123 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:43,124 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: group_by\n",
      "2025-06-06 12:18:43,354 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:44,587 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "]\n",
      "2025-06-06 12:18:44,588 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: group_by\n",
      "2025-06-06 12:18:44,604 - ERROR - ç¨‹åºåŸ·è¡Œå¤±æ•—: Object of type int64 is not JSON serializable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 584, in <module>\n",
      "    enhanced_tree_report.generate_all_texts_enhanced(root)\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 424, in generate_all_texts_enhanced\n",
      "    self.generate_all_texts_enhanced(child)\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 424, in generate_all_texts_enhanced\n",
      "    self.generate_all_texts_enhanced(child)\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 424, in generate_all_texts_enhanced\n",
      "    self.generate_all_texts_enhanced(child)\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 428, in generate_all_texts_enhanced\n",
      "    node.text = self.text_generator.generate_enhanced_write_content(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_15944\\1641214403.py\", line 191, in generate_enhanced_write_content\n",
      "    {json.dumps(insights, ensure_ascii=False, indent=2)}\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 202, in encode\n",
      "    chunks = list(chunks)\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Danie\\.conda\\envs\\badminton\\Lib\\json\\encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import builtins\n",
    "import time\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== æ–°å¢ï¼šæ–°èå¯«ä½œé¢¨æ ¼é…ç½® =====\n",
    "class NewsStyleConfig:\n",
    "    \"\"\"æ–°èå¯«ä½œé¢¨æ ¼é…ç½®é¡\"\"\"\n",
    "    \n",
    "    # ç¾½çƒå°ˆæ¥­è¡“èªå°ç…§è¡¨\n",
    "    BADMINTON_TERMS = {\n",
    "        'net': 'ç¶²å‰å¤±èª¤',\n",
    "        'out': 'å‡ºç•Œ',\n",
    "        'long': 'éåº•ç·š',\n",
    "        'smash': 'æ®ºçƒ',\n",
    "        'clear': 'é«˜é çƒ',\n",
    "        'drop': 'åˆ‡çƒ',\n",
    "        'drive': 'å¹³æŠ½çƒ',\n",
    "        'serve': 'ç™¼çƒ',\n",
    "        'return': 'å›çƒ'\n",
    "    }\n",
    "    \n",
    "    # æ–°èå¸¸ç”¨å‹•è©\n",
    "    ACTION_VERBS = [\n",
    "        'å±•ç¾', 'ç™¼æ®', 'æŒæ¡', 'é‹ç”¨', 'æ–½å±•', 'æ§åˆ¶', 'ä¸»å°', 'å£“åˆ¶',\n",
    "        'çªç ´', 'å‰µé€ ', 'ç· é€ ', 'å¥ å®š', 'ç¢ºç«‹', 'éå›º', 'æ‰­è½‰', 'é€†è½‰'\n",
    "    ]\n",
    "    \n",
    "    # æ–°èå½¢å®¹è©\n",
    "    DESCRIPTIVE_ADJECTIVES = [\n",
    "        'ç²¾å½©', 'æ¿€çƒˆ', 'é—œéµ', 'è‡´å‘½', 'ç²¾æº–', 'ç©©å¥', 'éœ¸æ°£', 'éˆæ´»',\n",
    "        'æœæ±º', 'å†·éœ', 'å¼·å‹¢', 'é©šè‰·', 'å‡ºè‰²', 'å“è¶Š', 'å®Œç¾', 'çµ•ä½³'\n",
    "    ]\n",
    "    \n",
    "    # æ–°èå¥å¼æ¨¡æ¿\n",
    "    SENTENCE_TEMPLATES = [\n",
    "        \"{player}åœ¨{situation}ä¸­{action}ï¼Œ{result}\",\n",
    "        \"æ†‘è—‰{skill}ï¼Œ{player}{achievement}\",\n",
    "        \"{player}ä»¥{score_pattern}{victory_method}ï¼Œ{final_result}\",\n",
    "        \"æ¯”è³½ä¸­{player}{performance}ï¼Œ{impact}\",\n",
    "        \"é—œéµæ™‚åˆ»{player}{key_action}ï¼Œ{outcome}\"\n",
    "    ]\n",
    "\n",
    "# ===== æ”¹é€²çš„æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "class EnhancedTextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "        self.style_config = NewsStyleConfig()\n",
    "        \n",
    "        # æ–°å¢ï¼šæ¯”è³½ä¸Šä¸‹æ–‡è¨˜æ†¶\n",
    "        self.match_context = {\n",
    "            'player_names': ['Player_A', 'Player_B'],\n",
    "            'match_type': 'ç¾½çƒè³½äº‹',\n",
    "            'key_moments': [],\n",
    "            'performance_trends': {}\n",
    "        }\n",
    "\n",
    "    def _convert_to_json_serializable(self, obj):\n",
    "        \"\"\"å°‡numpy/pandasé¡å‹è½‰æ›ç‚ºJSONå¯åºåˆ—åŒ–çš„PythonåŸºæœ¬é¡å‹\"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        if isinstance(obj, (np.integer, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (pd.Series, pd.Index)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.DataFrame):\n",
    "            return obj.to_dict(orient='records')\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    def extract_detailed_insights(self, table: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"æå–æ›´è©³ç´°çš„æ¯”è³½æ´å¯Ÿ\"\"\"\n",
    "        insights = {\n",
    "            'basic_stats': {},\n",
    "            'patterns': {},\n",
    "            'key_moments': [],\n",
    "            'player_performance': {},\n",
    "            'tactical_analysis': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # åŸºæœ¬çµ±è¨ˆ\n",
    "            if 'lose_reason' in table.columns:\n",
    "                lose_reasons = table['lose_reason'].value_counts()\n",
    "                insights['basic_stats']['top_lose_reason'] = {\n",
    "                    'reason': lose_reasons.index[0] if len(lose_reasons) > 0 else 'æœªçŸ¥',\n",
    "                    'count': lose_reasons.iloc[0] if len(lose_reasons) > 0 else 0,\n",
    "                    'percentage': round((lose_reasons.iloc[0] / len(table)) * 100, 1) if len(lose_reasons) > 0 else 0\n",
    "                }\n",
    "                \n",
    "            if 'getpoint_player' in table.columns:\n",
    "                point_winners = table['getpoint_player'].value_counts()\n",
    "                insights['basic_stats']['leading_player'] = {\n",
    "                    'player': point_winners.index[0] if len(point_winners) > 0 else 'æœªçŸ¥',\n",
    "                    'points': point_winners.iloc[0] if len(point_winners) > 0 else 0,\n",
    "                    'advantage': point_winners.iloc[0] - point_winners.iloc[1] if len(point_winners) > 1 else 0\n",
    "                }\n",
    "            \n",
    "            # æ¨¡å¼åˆ†æ\n",
    "            if 'type' in table.columns and 'lose_reason' in table.columns:\n",
    "                pattern_analysis = table.groupby('type')['lose_reason'].value_counts()\n",
    "                insights['patterns']['shot_error_correlation'] = pattern_analysis.to_dict()\n",
    "            \n",
    "            # é—œéµæ™‚åˆ»è­˜åˆ¥ï¼ˆåŸºæ–¼åˆ†æ•¸è®ŠåŒ–ï¼‰\n",
    "            if 'score' in table.columns:\n",
    "                score_changes = table['score'].diff().abs()\n",
    "                critical_moments = table[score_changes > score_changes.quantile(0.8)]\n",
    "                insights['key_moments'] = critical_moments.to_dict('records')[:3]  # å‰3å€‹é—œéµæ™‚åˆ»\n",
    "            \n",
    "            # çƒå“¡è¡¨ç¾åˆ†æ\n",
    "            if 'getpoint_player' in table.columns:\n",
    "                for player in table['getpoint_player'].unique():\n",
    "                    player_data = table[table['getpoint_player'] == player]\n",
    "                    insights['player_performance'][player] = {\n",
    "                        'total_points': len(player_data),\n",
    "                        'winning_shots': player_data['type'].value_counts().to_dict() if 'type' in table.columns else {},\n",
    "                        'consistency': self._calculate_consistency(player_data)\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"æå–æ´å¯Ÿå¤±æ•—: {e}\")\n",
    "            \n",
    "        return self._convert_to_json_serializable(insights)\n",
    "\n",
    "    def _calculate_consistency(self, player_data: pd.DataFrame) -> float:\n",
    "        \"\"\"è¨ˆç®—çƒå“¡ä¸€è‡´æ€§æŒ‡æ¨™\"\"\"\n",
    "        try:\n",
    "            if 'score' in player_data.columns and len(player_data) > 1:\n",
    "                score_variance = player_data['score'].var()\n",
    "                return max(0, 1 - (score_variance / 10))  # æ¨™æº–åŒ–ä¸€è‡´æ€§åˆ†æ•¸\n",
    "            return 0.5  # é è¨­å€¼\n",
    "        except:\n",
    "            return 0.5\n",
    "\n",
    "    def generate_contextual_narrative(self, insights: Dict[str, Any], operation_history: List[str]) -> str:\n",
    "        \"\"\"æ ¹æ“šæ´å¯Ÿç”Ÿæˆæœ‰ä¸Šä¸‹æ–‡çš„æ•˜è¿°\"\"\"\n",
    "        narratives = []\n",
    "        insights = self._convert_to_json_serializable(insights)\n",
    "        # é–‹å ´æ•˜è¿°\n",
    "        if insights.get('basic_stats', {}).get('leading_player'):\n",
    "            leader_info = insights['basic_stats']['leading_player']\n",
    "            if leader_info['advantage'] > 2:\n",
    "                narratives.append(f\"{leader_info['player']}åœ¨æœ¬å±€å±•ç¾å¼·å‹¢è¡¨ç¾ï¼Œä»¥{leader_info['points']}åˆ†é ˜å…ˆå°æ‰‹{leader_info['advantage']}åˆ†\")\n",
    "            else:\n",
    "                narratives.append(f\"é›™æ–¹æˆ°æ³è† è‘—ï¼Œ{leader_info['player']}åƒ…ä»¥{leader_info['advantage']}åˆ†çš„å¾®å¹…å„ªå‹¢é ˜å…ˆ\")\n",
    "        \n",
    "        # å¤±èª¤åˆ†ææ•˜è¿°\n",
    "        if insights.get('basic_stats', {}).get('top_lose_reason'):\n",
    "            error_info = insights['basic_stats']['top_lose_reason']\n",
    "            error_desc = self.style_config.BADMINTON_TERMS.get(error_info['reason'], error_info['reason'])\n",
    "            narratives.append(f\"æ¯”è³½ä¸­{error_desc}æˆç‚ºä¸»è¦å¤±åˆ†å› ç´ ï¼Œä½”ç¸½å¤±èª¤çš„{error_info['percentage']}%\")\n",
    "        \n",
    "        # é—œéµæ™‚åˆ»æ•˜è¿°\n",
    "        if insights.get('key_moments'):\n",
    "            narratives.append(\"é—œéµåˆ†æ•¸è½‰æŠ˜é»å‡ºç¾æ¿€çƒˆæ”»é˜²ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ä¸ç›¸ä¸Šä¸‹\")\n",
    "        \n",
    "        return \"ï¼Œ\".join(narratives) + \"ã€‚\"\n",
    "\n",
    "    def generate_enhanced_write_content(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        \"\"\"ç”Ÿæˆå¢å¼·ç‰ˆçš„å¯«ä½œå…§å®¹\"\"\"\n",
    "        \n",
    "        # æå–è©³ç´°æ´å¯Ÿ\n",
    "        insights = self.extract_detailed_insights(table)\n",
    "        \n",
    "        # ç”Ÿæˆä¸Šä¸‹æ–‡æ•˜è¿°\n",
    "        contextual_narrative = self.generate_contextual_narrative(insights, operation_history)\n",
    "        \n",
    "        table_str = table.to_string()\n",
    "        WRITE_TOKENS = 80  # å¢åŠ tokenæ•¸é‡ä»¥æ”¯æ´æ›´è±å¯Œå…§å®¹\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "System: ä½ æ˜¯å°ˆæ¥­çš„é«”è‚²æ–°èè¨˜è€…ï¼Œå°ˆç²¾ç¾½çƒè³½äº‹å ±å°ã€‚è«‹æ ¹æ“šæ•¸æ“šåˆ†ææ’°å¯«ä¸€æ®µç²¾å½©çš„æ¯”è³½ç‰‡æ®µæè¿°ã€‚\n",
    "\n",
    "# å¯«ä½œè¦æ±‚\n",
    "1. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ–‡ç­†ç”Ÿå‹•æ´»æ½‘\n",
    "2. æ¡ç”¨é«”è‚²æ–°èçš„å°ˆæ¥­èªèª¿ï¼Œé¿å…éæ–¼æŠ€è¡“æ€§çš„ç”¨è©\n",
    "3. é‡é»æè¿°æ¯”è³½ç¯€å¥ã€çƒå“¡è¡¨ç¾å’Œæˆ°è¡“é‹ç”¨\n",
    "4. å­—æ•¸æ§åˆ¶åœ¨{WRITE_TOKENS}å­—ä»¥å…§\n",
    "5. é¿å…ç›´æ¥å¼•ç”¨æ¬„ä½åç¨±ï¼Œä½¿ç”¨è‡ªç„¶çš„æè¿°æ–¹å¼\n",
    "6. çªå‡ºæ¯”è³½çš„æˆ²åŠ‡æ€§å’Œè§€è³æ€§\n",
    "\n",
    "# å°ˆæ¥­è¡“èªå°ç…§\n",
    "{json.dumps(self.style_config.BADMINTON_TERMS, ensure_ascii=False, indent=2)}\n",
    "\n",
    "# æ¯”è³½æ•¸æ“šæ´å¯Ÿ\n",
    "{json.dumps(insights, ensure_ascii=False, indent=2)}\n",
    "\n",
    "# ä¸Šä¸‹æ–‡æ•˜è¿°åƒè€ƒ\n",
    "{contextual_narrative}\n",
    "\n",
    "# åŸå§‹æ•¸æ“šè¡¨æ ¼\n",
    "{table_str}\n",
    "\n",
    "è«‹æ’°å¯«ä¸€æ®µå¼•äººå…¥å‹çš„æ¯”è³½æè¿°ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        return self._retry_generate_with_quality_check(enhanced_prompt)\n",
    "\n",
    "    def _retry_generate_with_quality_check(self, prompt: str, max_retries=3) -> str:\n",
    "        \"\"\"å¸¶é…é¡ç®¡ç†å’ŒæŒ‡æ•¸é€€é¿çš„é‡è©¦ç”Ÿæˆ\"\"\"\n",
    "        import random\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # æ·»åŠ éš¨æ©Ÿå»¶é²ä»¥é¿å…çªç™¼è«‹æ±‚\n",
    "                delay = random.uniform(0.5, 2.0) * (2 ** attempt)\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                response = self.model.generate_content(prompt)\n",
    "                \n",
    "                if response.text:\n",
    "                    text = response.text.strip()\n",
    "                    quality_score = self._assess_text_quality(text)\n",
    "                    \n",
    "                    if quality_score >= 0.7:\n",
    "                        return text\n",
    "                    elif attempt < max_retries - 1:\n",
    "                        logger.info(f\"æ–‡æœ¬å“è³ªä¸é”æ¨™ (åˆ†æ•¸: {quality_score:.2f})ï¼Œé‡æ–°ç”Ÿæˆ...\")\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        return text\n",
    "                        \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  # é…é¡éŒ¯èª¤\n",
    "                    wait_time = 30 + random.randint(0, 10)  # éš¨æ©Ÿç­‰å¾…30-40ç§’\n",
    "                    logger.warning(f\"APIé…é¡ä¸è¶³ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"ç”Ÿæˆå¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2)\n",
    "                    \n",
    "        return \"âš ï¸ å…§å®¹ç”Ÿæˆæš«æ™‚å¤±æ•—ï¼Œè«‹ç¨å¾Œé‡è©¦\"\n",
    "\n",
    "    def _assess_text_quality(self, text: str) -> float:\n",
    "        \"\"\"è©•ä¼°æ–‡æœ¬å“è³ª\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # é•·åº¦æª¢æŸ¥ (20%)\n",
    "        if 30 <= len(text) <= 120:\n",
    "            score += 0.2\n",
    "        \n",
    "        # å°ˆæ¥­è¡“èªä½¿ç”¨ (20%)\n",
    "        term_usage = sum(1 for term in self.style_config.BADMINTON_TERMS.values() if term in text)\n",
    "        if term_usage > 0:\n",
    "            score += min(0.2, term_usage * 0.1)\n",
    "        \n",
    "        # å‹•è©æ´»èºåº¦ (20%)\n",
    "        verb_usage = sum(1 for verb in self.style_config.ACTION_VERBS if verb in text)\n",
    "        if verb_usage > 0:\n",
    "            score += min(0.2, verb_usage * 0.05)\n",
    "        \n",
    "        # é¿å…æŠ€è¡“æ¬„ä½åç¨± (20%)\n",
    "        tech_terms = ['lose_reason', 'getpoint_player', 'type', 'column', 'row']\n",
    "        if not any(term in text for term in tech_terms):\n",
    "            score += 0.2\n",
    "        \n",
    "        # èªå¥æµæš¢åº¦ (20%) - ç°¡å–®æª¢æŸ¥æ¨™é»ç¬¦è™Ÿ\n",
    "        if 'ï¼Œ' in text or 'ã€‚' in text:\n",
    "            score += 0.2\n",
    "        \n",
    "        return min(1.0, score)\n",
    "\n",
    "    def generate_comprehensive_final_report(self, child_texts: List[str], match_metadata: Dict[str, Any]) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¶œåˆæ€§æœ€çµ‚å ±å‘Š\"\"\"\n",
    "        \n",
    "        if not child_texts:\n",
    "            return \"æ¯”è³½æ•¸æ“šåˆ†æå®Œæˆï¼Œè©³ç´°å…§å®¹è«‹åƒè€ƒå„éšæ®µåˆ†æã€‚\"\n",
    "        \n",
    "        # æ•´åˆæ‰€æœ‰å­æ–‡æœ¬\n",
    "        consolidated_content = \"\\n\".join([f\"â€¢ {text.strip()}\" for text in child_texts if text.strip()])\n",
    "        \n",
    "        final_prompt = f\"\"\"\n",
    "System: ä½ æ˜¯è³‡æ·±é«”è‚²è¨˜è€…ï¼Œè² è²¬æ’°å¯«ç¾½çƒè³½äº‹çš„æ·±åº¦å ±å°ã€‚è«‹æ ¹æ“šä»¥ä¸‹åˆ†æå…§å®¹ï¼Œæ’°å¯«ä¸€ç¯‡å®Œæ•´çš„è³½äº‹æ–°èç¨¿ã€‚\n",
    "\n",
    "# æ’°å¯«è¦æ±‚\n",
    "1. æ¡ç”¨æ–°èç¨¿æ ¼å¼ï¼ŒåŒ…å«å°è¨€ã€å…§æ–‡å’Œçµèª\n",
    "2. èªè¨€ç”Ÿå‹•ï¼Œçªå‡ºæ¯”è³½äº®é»å’Œæˆ²åŠ‡æ€§\n",
    "3. é¿å…ä½¿ç”¨æŠ€è¡“æ€§è¡“èªï¼Œæ”¹ç”¨è®€è€…æ˜“æ‡‚çš„æè¿°\n",
    "4. å­—æ•¸æ§åˆ¶åœ¨150-200å­—\n",
    "5. çµæ§‹æ¸…æ™°ï¼Œé‚è¼¯é †æš¢\n",
    "6. ä½¿ç”¨ã€Œé¸æ‰‹Aã€å’Œã€Œé¸æ‰‹Bã€ç¨±å‘¼çƒå“¡\n",
    "\n",
    "# æ–°èå¯«ä½œç¯„ä¾‹é¢¨æ ¼\n",
    "\"åœ¨ä»Šæ—¥çš„ç²¾å½©å°æ±ºä¸­ï¼Œé¸æ‰‹Aå±•ç¾å‡ºè‰²çš„ç¶²å‰æŠ€å·§ï¼Œå¤šæ¬¡é‹ç”¨ç²¾æº–çš„åˆ‡çƒè£½é€ å¾—åˆ†æ©Ÿæœƒã€‚ç„¶è€Œé¸æ‰‹Bä¸¦æœªç¤ºå¼±ï¼Œæ†‘è—‰å¼·å‹çš„å¾Œå ´æ”»æ“ŠåŠ›ï¼Œåœ¨é—œéµæ™‚åˆ»é€£çºŒå¾—åˆ†æ‰³å›åŠ£å‹¢ã€‚æ•´å ´æ¯”è³½é«˜æ½®è¿­èµ·ï¼Œå…©ä½é¸æ‰‹çš„ç²¾å½©è¡¨ç¾è®“ç¾å ´è§€çœ¾å¤§é£½çœ¼ç¦ã€‚\"\n",
    "\n",
    "# åˆ†æå…§å®¹æ‘˜è¦\n",
    "{consolidated_content}\n",
    "\n",
    "è«‹æ’°å¯«å®Œæ•´çš„è³½äº‹æ–°èå ±å°ï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        return self._retry_generate_with_quality_check(final_prompt, max_retries=2)\n",
    "\n",
    "    def merge_child_texts_enhanced(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        \"\"\"å¢å¼·ç‰ˆæ–‡æœ¬åˆä½µ\"\"\"\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        # éæ¿¾å’Œæ¸…ç†æ–‡æœ¬\n",
    "        clean_texts = []\n",
    "        for text in child_texts:\n",
    "            cleaned = text.strip()\n",
    "            if cleaned and len(cleaned) > 10:  # éæ¿¾éçŸ­çš„æ–‡æœ¬\n",
    "                clean_texts.append(cleaned)\n",
    "        \n",
    "        if not clean_texts:\n",
    "            return \"\"\n",
    "\n",
    "        # æ™ºèƒ½åˆä½µé‚è¼¯\n",
    "        if len(clean_texts) == 1:\n",
    "            return clean_texts[0]\n",
    "        \n",
    "        # å¤šæ®µè½åˆä½µ\n",
    "        merge_prompt = f\"\"\"\n",
    "è«‹å°‡ä»¥ä¸‹ç¾½çƒæ¯”è³½åˆ†æç‰‡æ®µæ•´åˆæˆä¸€æ®µé€£è²«çš„æè¿°ï¼š\n",
    "\n",
    "{chr(10).join([f\"{i+1}. {text}\" for i, text in enumerate(clean_texts)])}\n",
    "\n",
    "æ•´åˆè¦æ±‚ï¼š\n",
    "1. ä¿æŒæ‰€æœ‰é—œéµä¿¡æ¯\n",
    "2. ç¢ºä¿é‚è¼¯é †åºåˆç†\n",
    "3. èªè¨€è‡ªç„¶æµæš¢\n",
    "4. é¿å…é‡è¤‡å…§å®¹\n",
    "5. å­—æ•¸æ§åˆ¶åœ¨100å­—ä»¥å…§\n",
    "\n",
    "æ•´åˆçµæœï¼š\n",
    "\"\"\"\n",
    "        \n",
    "        return self._retry_generate_with_quality_check(merge_prompt)\n",
    "\n",
    "# ===== æ”¹é€²çš„TreeOfReporté¡åˆ¥ =====\n",
    "class EnhancedTreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # ä½¿ç”¨å¢å¼·ç‰ˆæ–‡æœ¬ç”Ÿæˆå™¨\n",
    "        self.content_planner = ContentPlanner(api_key)\n",
    "        self.df_operator = SafeDataFrameOperator(api_key)\n",
    "        self.text_generator = EnhancedTextGenerator(api_key, table_description=self.table_description)\n",
    "        \n",
    "        # æ–°å¢ï¼šæ¯”è³½å…ƒæ•¸æ“šè¿½è¹¤\n",
    "        self.match_metadata = {\n",
    "            'total_points': 0,\n",
    "            'analysis_depth': 0,\n",
    "            'key_insights': [],\n",
    "            'generation_quality': []\n",
    "        }\n",
    "        \n",
    "        # è¿½è¹¤åŠŸèƒ½\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        self.node_registry: Dict[str, TreeNode] = {}\n",
    "\n",
    "    def load_configurations(self):\n",
    "        \"\"\"è¼‰å…¥é…ç½®ï¼Œèˆ‡åŸç‰ˆç›¸åŒ\"\"\"\n",
    "        self.table_description = read_text_file(\"filtered_data _description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"ç¾½çƒæ¯”è³½æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å¾—åˆ†æ¨¡å¼ã€å¤±èª¤é¡å‹å’Œçƒå“¡è¡¨ç¾ç­‰é—œéµæŒ‡æ¨™\"\n",
    "\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        if isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['name'] for op in self.operation_description]\n",
    "        else:\n",
    "            self.operation_pool = list(self.operation_description.keys())\n",
    "\n",
    "        logger.info(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "    def create_child_node_enhanced(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"å¢å¼·ç‰ˆå­ç¯€é»å‰µå»º\"\"\"\n",
    "        try:\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            if operation.lower().startswith('write'):\n",
    "                # ä½¿ç”¨å¢å¼·ç‰ˆæ–‡æœ¬ç”Ÿæˆ\n",
    "                text = self.text_generator.generate_enhanced_write_content(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                \n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                self.node_registry[child.node_id] = child\n",
    "                \n",
    "                # è©•ä¼°ç”Ÿæˆè³ªé‡\n",
    "                quality_score = self.text_generator._assess_text_quality(text)\n",
    "                self.match_metadata['generation_quality'].append(quality_score)\n",
    "                \n",
    "                logger.info(f\"å‰µå»º write ç¯€é»: {operation} (å“è³ªåˆ†æ•¸: {quality_score:.2f})\")\n",
    "                return child\n",
    "            else:\n",
    "                # æ•¸æ“šæ“ä½œç¯€é»ï¼ˆèˆ‡åŸç‰ˆç›¸åŒé‚è¼¯ï¼‰\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    self.node_registry[child.node_id] = child\n",
    "                    logger.info(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_all_texts_enhanced(self, node: TreeNode):\n",
    "        \"\"\"å¢å¼·ç‰ˆæ–‡æœ¬ç”Ÿæˆ\"\"\"\n",
    "        # éæ­¸è™•ç†å­ç¯€é»\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts_enhanced(child)\n",
    "        \n",
    "        # è‘‰ç¯€é»è™•ç†\n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_enhanced_write_content(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "        # éè‘‰ç¯€é»åˆä½µ\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts_enhanced(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        \n",
    "        logger.info(f'ç¯€é» {node.node_id} å¢å¼·æ–‡æœ¬ç”Ÿæˆå®Œæˆ')\n",
    "\n",
    "    def generate_final_report_enhanced(self, node: TreeNode) -> str:\n",
    "        \"\"\"ç”Ÿæˆå¢å¼·ç‰ˆæœ€çµ‚å ±å‘Š\"\"\"\n",
    "        if node.level == 0:\n",
    "            # è¨ˆç®—æ•´é«”å“è³ªæŒ‡æ¨™\n",
    "            avg_quality = sum(self.match_metadata['generation_quality']) / len(self.match_metadata['generation_quality']) if self.match_metadata['generation_quality'] else 0\n",
    "            \n",
    "            # ä½¿ç”¨å¢å¼·ç‰ˆæœ€çµ‚å ±å‘Šç”Ÿæˆ\n",
    "            final_text = self.text_generator.generate_comprehensive_final_report(\n",
    "                [node.text] if node.text else [],\n",
    "                {**self.match_metadata, 'average_quality': avg_quality}\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜å ±å‘Šå’Œå…ƒæ•¸æ“š\n",
    "            with open(\"enhanced_tree_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            \n",
    "            with open(\"generation_quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                quality_report = {\n",
    "                    \"average_quality\": avg_quality,\n",
    "                    \"total_generations\": len(self.match_metadata['generation_quality']),\n",
    "                    \"quality_scores\": self.match_metadata['generation_quality'],\n",
    "                    \"metadata\": self.match_metadata\n",
    "                }\n",
    "                json.dump(quality_report, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"æœ€çµ‚å ±å‘Šç”Ÿæˆå®Œæˆï¼Œå¹³å‡å“è³ªåˆ†æ•¸: {avg_quality:.2f}\")\n",
    "            return final_text\n",
    "        else:\n",
    "            # å­ç¯€é»å ±å‘Šï¼ˆèˆ‡åŸç‰ˆç›¸åŒï¼‰\n",
    "            return self._generate_hierarchical_report(node, 0)\n",
    "\n",
    "    def _generate_hierarchical_report(self, node: TreeNode, level: int) -> str:\n",
    "        \"\"\"ç”Ÿæˆéšå±¤å¼å ±å‘Š\"\"\"\n",
    "        indent = \"  \" * level\n",
    "        report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "        if node.text:\n",
    "            report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "        if node.table is not None and not node.table.empty and level < 2:\n",
    "            report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "            if len(node.table) <= 10:\n",
    "                report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "            else:\n",
    "                report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "        for child in node.children:\n",
    "            report += self._generate_hierarchical_report(child, level + 1)\n",
    "\n",
    "        return report\n",
    "    \n",
    "    def build_tree(self, table: pd.DataFrame) -> 'TreeNode':\n",
    "        \"\"\"æ§‹å»ºå ±å‘Šæ¨¹çš„æ ¹ç¯€é»\"\"\"\n",
    "        root = TreeNode(level=0, text=\"\", table=table.copy(), operation=\"root\")\n",
    "        self.node_registry[root.node_id] = root\n",
    "\n",
    "        def expand(node: TreeNode, depth: int):\n",
    "            if depth >= self.max_depth:\n",
    "                return\n",
    "            for operation in self.operation_pool[:self.max_degree]:\n",
    "                child = self.create_child_node_enhanced(node, operation)\n",
    "                if child:\n",
    "                    node.children.append(child)\n",
    "                    expand(child, depth + 1)\n",
    "\n",
    "        expand(root, 0)\n",
    "        return root\n",
    "\n",
    "# ===== ä¸»è¦å‡½æ•¸ä¿æŒä¸è®Šï¼Œåƒ…ä¿®æ”¹é¡åˆ¥èª¿ç”¨ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# æ­¤è™•æ‡‰åŒ…å«åŸæœ‰çš„å…¶ä»–é¡åˆ¥å®šç¾© (TreeNode, OperationParser, ContentPlanner, SafeDataFrameOperator)\n",
    "# ç‚ºç¯€çœç©ºé–“ï¼Œé€™è£¡åªå±•ç¤ºä¸»è¦çš„ä¿®æ”¹éƒ¨åˆ†\n",
    "\n",
    "\n",
    "    \n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "logger.info(\"Enhanced Tree-of-Report for Sports News Generation\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "logger.info(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "\n",
    "try:\n",
    "    TABLES = pd.read_csv('filtered_set1.csv')\n",
    "    logger.info(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"æ‰¾ä¸åˆ° filtered_set1.csvï¼Œä½¿ç”¨ç¤ºä¾‹æ•¸æ“š\")\n",
    "    TABLES = pd.DataFrame({\n",
    "        'type': ['smash', 'clear', 'drop', 'net', 'smash', 'clear', 'drop', 'drive'],\n",
    "        'lose_reason': ['net', 'out', 'net', 'long', 'net', 'out', 'long', 'net'],\n",
    "        'getpoint_player': ['Player_A', 'Player_B', 'Player_A', 'Player_B', 'Player_A', 'Player_B', 'Player_A', 'Player_B'],\n",
    "        'score': [1, 2, 1, 3, 2, 1, 4, 2]\n",
    "    })\n",
    "\n",
    "# ä½¿ç”¨å¢å¼·ç‰ˆç³»çµ±\n",
    "MAX_DEPTH = 3\n",
    "MAX_DEGREE = 3\n",
    "\n",
    "enhanced_tree_report = EnhancedTreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "\n",
    "logger.info(\"é–‹å§‹å»ºæ§‹å¢å¼·ç‰ˆå ±å‘Šæ¨¹...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # å»ºæ§‹æ¨¹ï¼ˆä½¿ç”¨åŸæœ‰çš„build_treeé‚è¼¯ï¼Œä½†æ›¿æ›é—œéµæ–¹æ³•ï¼‰\n",
    "    root = enhanced_tree_report.build_tree(TABLES)\n",
    "    \n",
    "    # ç”Ÿæˆå¢å¼·ç‰ˆæ–‡æœ¬\n",
    "    enhanced_tree_report.generate_all_texts_enhanced(root)\n",
    "    \n",
    "    # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "    final_report = enhanced_tree_report.generate_final_report_enhanced(root)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"ENHANCED TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "    logger.info(\"=\"*60)\n",
    "    print(final_report)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (start_time - end_time).total_seconds()\n",
    "    \n",
    "    logger.info(f\"å¢å¼·ç‰ˆå ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’\")\n",
    "    logger.info(\"ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "    logger.info(\"- enhanced_tree_report.txt: å¢å¼·ç‰ˆæœ€çµ‚å ±å‘Š\")\n",
    "    logger.info(\"- generation_quality_report.json: ç”Ÿæˆå“è³ªåˆ†æ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6b0632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ç¬¬ 1/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 2/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 3/3 æ¬¡ç”Ÿæˆ...\n",
      "\n",
      "âœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\n",
      "[1] åˆ†æ•¸: 0.7 â†’ é€™å ´ç¾½çƒå°æ±ºå¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾ä¸€é–‹å§‹çš„è©¦æ¢æ€§ç™¼çƒèˆ‡è¼•å·§éç¶²çƒï¼Œåˆ°éš¨å¾Œé€æ¼¸åŠ åŠ‡çš„æ”»é˜²è½‰æ›ï¼Œé•·çƒèˆ‡çŸ­çƒçš„äº¤éŒ¯ä½¿ç”¨ï¼Œå±•ç¾äº†å…©ä½é¸æ‰‹å…¨é¢çš„æŠ€è¡“èƒ½åŠ›ã€‚\n",
      "\n",
      "ç¬¬ä¸€åˆ†å°±ç¶“æ­·äº†å¤šæ‹ä¾†å›ï¼Œé›™æ–¹åœ¨ç¶²å‰çŸ­çƒèˆ‡å¾Œå ´é«˜é çƒä¹‹é–“ä¸æ–·è½‰æ›ï¼Œæœ€çµ‚Aé¸æ‰‹æŠŠæ¡æ©Ÿæœƒï¼Œä¸€è¨˜ç²¾æº–çš„è½é»è®“å°æ‰‹æªæ‰‹ä¸åŠï¼Œå…ˆä¸‹ä¸€åŸã€‚\n",
      "\n",
      "éš¨å¾Œï¼Œé›™æ–¹æ¯”åˆ†äº¤æ›¿ä¸Šå‡ã€‚Aé¸æ‰‹æ“…é•·åˆ©ç”¨è½é»åˆé‘½çš„åˆ‡çƒä¾†èª¿å‹•å°æ‰‹ï¼Œè€ŒBé¸æ‰‹å‰‡ä»¥å¼·å‹çš„æ‰£æ®ºä½œç‚ºå›æ‡‰ã€‚æ¯”è³½ä¸­ï¼Œå¤šæ¬¡å‡ºç¾å¤šæ‹ç›¸æŒï¼Œé›™æ–¹é¸æ‰‹éƒ½å±•ç¾äº†æ¥µä½³çš„é«”èƒ½èˆ‡æ§åˆ¶èƒ½åŠ›ã€‚ç²¾é‡‡çš„æ”»é˜²è½‰æ›è®“è§€çœ¾ç›®ä¸æš‡çµ¦ã€‚\n",
      "\n",
      "æ¯”è³½éç¨‹ä¸­ï¼Œé›™æ–¹ä¹Ÿå‡ºç¾äº†ä¸€äº›å¤±èª¤ï¼ŒåŒ…æ‹¬å›çƒå‡ºç•Œã€è§¸ç¶²ç­‰ï¼Œä½†æ•´é«”è€Œè¨€ï¼Œå…©äººè¡¨ç¾å‡ºçš„ç«¶æŠ€æ°´æº–ä»ç„¶ç›¸ç•¶é«˜ã€‚ç‰¹åˆ¥æ˜¯å¾Œæ®µï¼Œé›™æ–¹é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œä½†ä¾ç„¶å …æŒå¿«é€Ÿçš„æ”»é˜²è½‰æ›ï¼Œæ¯”åˆ†ä¹Ÿå› æ­¤å‘ˆç¾è† è‘—ç‹€æ…‹ã€‚æœ€çµ‚ï¼ŒAé¸æ‰‹ä»¥21:15çš„æ¯”åˆ†è‰±é›£å‹å‡ºï¼Œè´å¾—äº†é€™å ´æ¿€çƒˆçš„æ¯”è³½ã€‚\n",
      "[2] åˆ†æ•¸: 0.7 â†’ é€™å ´ç¾½çƒè³½å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œæ¯«ä¸ç›¸è®“ã€‚æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹å°±å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²ï¼Œå¤šæ¬¡å‡ºç¾å¤šæ‹ä¾†å›çš„ç²¾å½©å ´é¢ã€‚ä¸€æ–¹é¸æ‰‹æ“…é•·é‹ç”¨å¤šæ¨£åŒ–çš„çƒè·¯ï¼ŒåŒ…æ‹¬è¼•å·§çš„ç¶²å‰å°çƒå’Œè§’åº¦åˆé‘½çš„æ–œç·šé€²æ”»ï¼Œè©¦åœ–èª¿å‹•å°æ‰‹ï¼›å¦ä¸€æ–¹å‰‡ä»¥å¼·å‹çš„å¾Œå ´æ‰£æ®ºå’Œç©©å¥çš„é˜²å®ˆåæ“Šè¦‹é•·ã€‚\n",
      "\n",
      "æ¯”è³½éç¨‹ä¸­ï¼Œé›™æ–¹æ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œäº’ä¸ç›¸è®“ã€‚é ˜å…ˆå„ªå‹¢å¤šæ¬¡æ˜“æ‰‹ï¼Œæ¯ä¸€æ¬¡å¾—åˆ†éƒ½ä¼´éš¨è‘—å ´é‚Šè§€çœ¾çš„æ­¡å‘¼å’ŒæŒè²ã€‚é¸æ‰‹å€‘åœ¨å ´ä¸Šå¥®åŠ›å¥”è·‘ï¼Œæ¯ä¸€æ¬¡æ“Šçƒéƒ½å‡èšè‘—åŠ›é‡å’ŒæŠ€å·§ï¼ŒåŠ›æ±‚å°‡çƒæ‰“åˆ°å°æ–¹é›£ä»¥è§¸åŠçš„ä½ç½®ã€‚\n",
      "\n",
      "åœ¨é—œéµåˆ†ä¸Šï¼Œé›™æ–¹éƒ½å±•ç¾å‡ºäº†æ¥µé«˜çš„å¿ƒç†ç´ è³ªã€‚ä¸€æ–¹é¸æ‰‹åˆ©ç”¨ä¸€æ¬¡ç²¾æº–çš„åˆ¤æ–·ï¼Œè¿«ä½¿å°æ‰‹å›çƒå‡ºç•Œï¼ŒæˆåŠŸæ‹¿ä¸‹é—œéµä¸€åˆ†ã€‚ç„¶è€Œï¼Œå¦ä¸€æ–¹é¸æ‰‹ä¹Ÿæ¯«ä¸ç¤ºå¼±ï¼Œéš¨å¾Œåˆ©ç”¨ä¸€è¨˜å‹¢å¤§åŠ›æ²‰çš„æ‰£æ®ºï¼Œç›´æ¥å¾—åˆ†ï¼Œå°‡æ¯”åˆ†è¿½å¹³ã€‚\n",
      "\n",
      "æ¯”è³½æœ€å¾Œéšæ®µï¼Œé›™æ–¹é«”èƒ½éƒ½å·²æ¥è¿‘æ¥µé™ï¼Œä½†ä¾èˆŠå …æŒè‘—ã€‚æœ€çµ‚ï¼Œåœ¨ä¸€é€£ä¸²çš„ç²¾å½©æ”»é˜²å¾Œï¼Œä¸€æ–¹é¸æ‰‹æŠ“ä½æ©Ÿæœƒï¼Œåˆ©ç”¨å°æ‰‹çš„ä¸€å€‹å¤±èª¤ï¼ŒæˆåŠŸæ‹¿ä¸‹åˆ¶å‹åˆ†ï¼Œè´å¾—äº†é€™å ´è‰±è‹¦çš„æ¯”è³½ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹å¯¦åŠ›æ¥è¿‘ï¼Œç‚ºè§€çœ¾å¥‰ç»äº†ä¸€å ´ç²¾å½©çµ•å€«çš„ç¾½çƒç››å®´ã€‚\n",
      "[3] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œæ”»é˜²è½‰æ›è¿…é€Ÿã€‚å¾ä¸€é–‹å§‹çš„æ¯”åˆ†è† è‘—ï¼Œåˆ°å¾Œé¢çš„é€æ¼¸æ‹‰é‹¸ï¼Œæ¯ä¸€çƒéƒ½å……æ»¿äº†ç«¶çˆ­ã€‚\n",
      "\n",
      "é¦–å±€é–‹å§‹ï¼Œé›™æ–¹éƒ½ç›¸ç•¶è¬¹æ…ï¼Œè©¦åœ–é€šéç™¼çƒå’Œéæ¸¡çƒä¾†æ§åˆ¶ç¯€å¥ã€‚Aé¸æ‰‹ç‡å…ˆå–å¾—é ˜å…ˆï¼Œä½†Bé¸æ‰‹ç·Šè¿½ä¸æ¨ï¼Œå¤šæ¬¡é€šéç©æ¥µçš„é€²æ”»å°‡æ¯”åˆ†æ‰³å¹³ã€‚æ¯”è³½éç¨‹ä¸­ï¼Œå¤šæ¬¡å‡ºç¾å¤šæ‹ä¾†å›ï¼Œé›™æ–¹åœ¨ç¶²å‰å°çƒå’Œå¾Œå ´é«˜é çƒçš„è™•ç†ä¸Šéƒ½å±•ç¾äº†é«˜è¶…çš„æŠ€å·§ã€‚Aé¸æ‰‹çš„çªæ“Šå’ŒBé¸æ‰‹çš„é ‘å¼·é˜²å®ˆï¼Œå½¢æˆäº†é®®æ˜çš„å°æ¯”ã€‚\n",
      "\n",
      "ä¸­æ®µï¼ŒAé¸æ‰‹åˆ©ç”¨ä¸€é€£ä¸²çš„æ”»å‹¢ï¼ŒåŒ…æ‹¬å·§å¦™çš„åŠçƒå’Œå…‡çŒ›çš„æ‰£æ®ºï¼Œé€æ¼¸å°‡æ¯”åˆ†æ‹‰é–‹ã€‚ç„¶è€Œï¼ŒBé¸æ‰‹ä¸¦æ²’æœ‰å› æ­¤æ°£é¤’ï¼Œè€Œæ˜¯ç©©ç´®ç©©æ‰“ï¼Œé€šéç²¾æº–çš„è½é»æ§åˆ¶å’Œç©æ¥µçš„è·‘å‹•ä¾†å°‹æ‰¾åæ“Šçš„æ©Ÿæœƒã€‚åœ¨ä¸€æ¬¡å¤šæ‹çš„åƒµæŒä¸­ï¼ŒBé¸æ‰‹å·§å¦™é‹ç”¨å°çƒï¼Œè¿«ä½¿Aé¸æ‰‹å›çƒæ›ç¶²ï¼Œè´å¾—å¯¶è²´ä¸€åˆ†ã€‚\n",
      "\n",
      "æ¯”è³½æœ«æ®µï¼Œé›™æ–¹é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œå¤±èª¤ä¹Ÿé–‹å§‹å¢å¤šã€‚Aé¸æ‰‹çš„é€²æ”»é›–ç„¶ä¾èˆŠçŠ€åˆ©ï¼Œä½†ç©©å®šæ€§æœ‰æ‰€ä¸‹é™ï¼Œå¤šæ¬¡å‡ºç¾å›çƒå‡ºç•Œæˆ–ä¸‹ç¶²çš„æƒ…æ³ã€‚è€ŒBé¸æ‰‹å‰‡æŠ“ä½æ©Ÿæœƒï¼ŒåŠ å¼·äº†ç¶²å‰çš„æ§åˆ¶ï¼Œé »é »åˆ©ç”¨å°çƒå¾—åˆ†ã€‚æœ€çµ‚ï¼ŒAé¸æ‰‹é ‚ä½å£“åŠ›ï¼Œåœ¨ä¸€æ¬¡é—œéµçš„å°æ”»ä¸­ï¼ŒæˆåŠŸè¿«ä½¿å°æ‰‹å›çƒå‡ºç•Œï¼Œè‰±é›£åœ°è´ä¸‹äº†é€™ä¸€å±€ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œå……æ»¿äº†æ‡¸å¿µï¼Œé›™æ–¹é¸æ‰‹éƒ½å±•ç¾äº†æ¥µé«˜çš„ç«¶æŠ€æ°´å¹³å’Œé ‘å¼·çš„é¬¥å¿—ã€‚ç„¡è«–æ˜¯ç²¾æº–çš„ç¶²å‰å°çƒï¼Œé‚„æ˜¯å‹¢å¤§åŠ›æ²‰çš„å¾Œå ´æ‰£æ®ºï¼Œéƒ½çµ¦è§€çœ¾ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡ã€‚\n",
      "\n",
      "ğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ 3 æ¬¡ï¼šé€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œæ”»é˜²è½‰æ›è¿…é€Ÿã€‚å¾ä¸€é–‹å§‹çš„æ¯”åˆ†è† è‘—ï¼Œåˆ°å¾Œé¢çš„é€æ¼¸æ‹‰é‹¸ï¼Œæ¯ä¸€çƒéƒ½å……æ»¿äº†ç«¶çˆ­ã€‚\n",
      "\n",
      "é¦–å±€é–‹å§‹ï¼Œé›™æ–¹éƒ½ç›¸ç•¶è¬¹æ…ï¼Œè©¦åœ–é€šéç™¼çƒå’Œéæ¸¡çƒä¾†æ§åˆ¶ç¯€å¥ã€‚Aé¸æ‰‹ç‡å…ˆå–å¾—é ˜å…ˆï¼Œä½†Bé¸æ‰‹ç·Šè¿½ä¸æ¨ï¼Œå¤šæ¬¡é€šéç©æ¥µçš„é€²æ”»å°‡æ¯”åˆ†æ‰³å¹³ã€‚æ¯”è³½éç¨‹ä¸­ï¼Œå¤šæ¬¡å‡ºç¾å¤šæ‹ä¾†å›ï¼Œé›™æ–¹åœ¨ç¶²å‰å°çƒå’Œå¾Œå ´é«˜é çƒçš„è™•ç†ä¸Šéƒ½å±•ç¾äº†é«˜è¶…çš„æŠ€å·§ã€‚Aé¸æ‰‹çš„çªæ“Šå’ŒBé¸æ‰‹çš„é ‘å¼·é˜²å®ˆï¼Œå½¢æˆäº†é®®æ˜çš„å°æ¯”ã€‚\n",
      "\n",
      "ä¸­æ®µï¼ŒAé¸æ‰‹åˆ©ç”¨ä¸€é€£ä¸²çš„æ”»å‹¢ï¼ŒåŒ…æ‹¬å·§å¦™çš„åŠçƒå’Œå…‡çŒ›çš„æ‰£æ®ºï¼Œé€æ¼¸å°‡æ¯”åˆ†æ‹‰é–‹ã€‚ç„¶è€Œï¼ŒBé¸æ‰‹ä¸¦æ²’æœ‰å› æ­¤æ°£é¤’ï¼Œè€Œæ˜¯ç©©ç´®ç©©æ‰“ï¼Œé€šéç²¾æº–çš„è½é»æ§åˆ¶å’Œç©æ¥µçš„è·‘å‹•ä¾†å°‹æ‰¾åæ“Šçš„æ©Ÿæœƒã€‚åœ¨ä¸€æ¬¡å¤šæ‹çš„åƒµæŒä¸­ï¼ŒBé¸æ‰‹å·§å¦™é‹ç”¨å°çƒï¼Œè¿«ä½¿Aé¸æ‰‹å›çƒæ›ç¶²ï¼Œè´å¾—å¯¶è²´ä¸€åˆ†ã€‚\n",
      "\n",
      "æ¯”è³½æœ«æ®µï¼Œé›™æ–¹é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œå¤±èª¤ä¹Ÿé–‹å§‹å¢å¤šã€‚Aé¸æ‰‹çš„é€²æ”»é›–ç„¶ä¾èˆŠçŠ€åˆ©ï¼Œä½†ç©©å®šæ€§æœ‰æ‰€ä¸‹é™ï¼Œå¤šæ¬¡å‡ºç¾å›çƒå‡ºç•Œæˆ–ä¸‹ç¶²çš„æƒ…æ³ã€‚è€ŒBé¸æ‰‹å‰‡æŠ“ä½æ©Ÿæœƒï¼ŒåŠ å¼·äº†ç¶²å‰çš„æ§åˆ¶ï¼Œé »é »åˆ©ç”¨å°çƒå¾—åˆ†ã€‚æœ€çµ‚ï¼ŒAé¸æ‰‹é ‚ä½å£“åŠ›ï¼Œåœ¨ä¸€æ¬¡é—œéµçš„å°æ”»ä¸­ï¼ŒæˆåŠŸè¿«ä½¿å°æ‰‹å›çƒå‡ºç•Œï¼Œè‰±é›£åœ°è´ä¸‹äº†é€™ä¸€å±€ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œå……æ»¿äº†æ‡¸å¿µï¼Œé›™æ–¹é¸æ‰‹éƒ½å±•ç¾äº†æ¥µé«˜çš„ç«¶æŠ€æ°´å¹³å’Œé ‘å¼·çš„é¬¥å¿—ã€‚ç„¡è«–æ˜¯ç²¾æº–çš„ç¶²å‰å°çƒï¼Œé‚„æ˜¯å‹¢å¤§åŠ›æ²‰çš„å¾Œå ´æ‰£æ®ºï¼Œéƒ½çµ¦è§€çœ¾ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡ã€‚\n",
      "âœ”ï¸ å·²å„²å­˜è‡³ï¼šbest_of_three_report_20250606_123050.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === å¯«ä½œé¢¨æ ¼è©å½™ ===\n",
    "BADMINTON_TERMS = {\n",
    "    'net': 'ç¶²å‰å¤±èª¤', 'out': 'å‡ºç•Œ', 'long': 'éåº•ç·š', 'smash': 'æ®ºçƒ',\n",
    "    'clear': 'é«˜é çƒ', 'drop': 'åˆ‡çƒ', 'drive': 'å¹³æŠ½çƒ', 'serve': 'ç™¼çƒ', 'return': 'å›çƒ'\n",
    "}\n",
    "ACTION_VERBS = ['å±•ç¾', 'ç™¼æ®', 'æŒæ¡', 'é‹ç”¨', 'æ–½å±•', 'æ§åˆ¶', 'ä¸»å°', 'å£“åˆ¶', 'çªç ´', 'å‰µé€ ', 'ç· é€ ', 'å¥ å®š', 'ç¢ºç«‹', 'éå›º', 'æ‰­è½‰', 'é€†è½‰']\n",
    "TECHNICAL_TERMS = ['lose_reason', 'getpoint_player', 'type', 'column', 'row']\n",
    "\n",
    "# === Gemini æ¨¡å‹åˆå§‹åŒ– ===\n",
    "def init_model(api_key: str):\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# === å“è³ªè©•ä¼° ===\n",
    "def assess_text_quality(text: str) -> float:\n",
    "    score = 0.0\n",
    "    if 30 <= len(text) <= 120:\n",
    "        score += 0.2\n",
    "    score += min(0.2, sum(1 for t in BADMINTON_TERMS.values() if t in text) * 0.1)\n",
    "    score += min(0.2, sum(1 for v in ACTION_VERBS if v in text) * 0.05)\n",
    "    if not any(t in text for t in TECHNICAL_TERMS):\n",
    "        score += 0.2\n",
    "    if 'ï¼Œ' in text or 'ã€‚' in text:\n",
    "        score += 0.2\n",
    "    return round(min(score, 1.0), 2)\n",
    "\n",
    "# === ä¸»æµç¨‹ï¼šé‡è¤‡3æ¬¡ç”Ÿæˆä¸¦è©•ä¼° ===\n",
    "def generate_best_of_three(df: pd.DataFrame, api_key: str):\n",
    "    model = init_model(api_key)\n",
    "    table_str = df.to_string(index=False)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­é«”è‚²æ–°èè¨˜è€…ï¼Œæ“…é•·æ’°å¯«ç¾½çƒæ¯”è³½å ±å°ã€‚\n",
    "è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šè¡¨æ ¼æ’°å¯«è³½äº‹æè¿°ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…å‡ºç¾æŠ€è¡“æ¬„ä½åç¨±ã€‚\n",
    "\n",
    "# è³½äº‹æ•¸æ“šè¡¨æ ¼ï¼š\n",
    "{table_str}\n",
    "\n",
    "è«‹æ’°å¯«æè¿°ï¼š\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            print(f\"â³ ç¬¬ {i+1}/3 æ¬¡ç”Ÿæˆ...\")\n",
    "            response = model.generate_content(prompt_template)\n",
    "            time.sleep(1)\n",
    "            text = response.text.strip() if response.text else \"âš ï¸ ç„¡å…§å®¹\"\n",
    "        except Exception as e:\n",
    "            text = f\"âš ï¸ ç”ŸæˆéŒ¯èª¤: {e}\"\n",
    "        score = assess_text_quality(text)\n",
    "        results.append({'index': i+1, 'text': text, 'score': score})\n",
    "\n",
    "    # é¸å‡ºæœ€ä½³çµæœ\n",
    "    best = max(results, key=lambda x: x['score'])\n",
    "\n",
    "    # è¼¸å‡ºåˆ°æª”æ¡ˆ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"best_of_three_report_{timestamp}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(f\"[ç‰ˆæœ¬ {r['index']}] å“è³ªåˆ†æ•¸: {r['score']}\\n{r['text']}\\n\\n\")\n",
    "        f.write(f\"ğŸ† æœ€ä½³ç‰ˆæœ¬ç‚ºç¬¬ {best['index']} æ¬¡ï¼Œåˆ†æ•¸: {best['score']}\\n\")\n",
    "        f.write(best['text'])\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\")\n",
    "    for r in results:\n",
    "        print(f\"[{r['index']}] åˆ†æ•¸: {r['score']} â†’ {r['text']}\")\n",
    "    print(f\"\\nğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ {best['index']} æ¬¡ï¼š{best['text']}\")\n",
    "    print(f\"âœ”ï¸ å·²å„²å­˜è‡³ï¼š{file_name}\")\n",
    "    return best\n",
    "\n",
    "# === æ¸¬è©¦å…¥å£ ===\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"è«‹è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\"filtered_set1.csv\")\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame({\n",
    "            'type': ['smash', 'clear', 'drop', 'net', 'smash', 'clear', 'drop', 'drive'],\n",
    "            'lose_reason': ['net', 'out', 'net', 'long', 'net', 'out', 'long', 'net'],\n",
    "            'getpoint_player': ['Player_A', 'Player_B', 'Player_A', 'Player_B', 'Player_A', 'Player_B', 'Player_A', 'Player_B'],\n",
    "            'score': [1, 2, 1, 3, 2, 1, 4, 2]\n",
    "        })\n",
    "\n",
    "    generate_best_of_three(df, api_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
