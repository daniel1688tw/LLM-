{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc75c21b",
   "metadata": {},
   "source": [
    "## ç°¡ä»‹ ##\n",
    "æ­¤ä»£ç¢¼ç”¨ä¾†è®“LLMæ ¹æ“šè¡¨æ ¼è³‡æ–™èˆ‡ä½¿ç”¨è€…çš„æå•è¦æ±‚ï¼Œé€épiplineèˆ‡tree stuctureï¼Œç”Ÿæˆå ±å°æˆ–åˆ†æè³‡æ–™\n",
    "\n",
    "æ­¤ç¯‡ç ”ç©¶åªéœ€æä¾›\n",
    "\"main.txt\"ç‚ºä½¿ç”¨è€…çš„å¤§ç¶±èˆ‡ç°¡çŸ­æƒ³æ³•\n",
    "\"data_description.txt\"ç‚ºè¦åˆ†æçš„table columnsæ‰€ä»£è¡¨çš„æ„ç¾©\n",
    "å°±å¯ç”¢ç”Ÿå®Œæ•´å ±å°\n",
    "(å¯ä½¿ç”¨åœ¨ç”¢ç”Ÿä»»ä½•å ±å°ä¸Šä¸é™æ–¼ç¾½çƒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0e02",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "\n",
    "åˆªæ¸›ä¸å¿…è¦çš„columns\n",
    "\n",
    "çµæœä¿ç•™['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88a047de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"Gemini_API\")\n",
    "if not api_key:\n",
    "    print(\"âŒ Gemini_API ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ca3c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­£å¼\n",
    "import dspy\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "class GeminiOpenAI(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        # ä½¿ç”¨ Google çš„ OpenAI å…¼å®¹ç«¯é»\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "        super().__init__(model=model_name)\n",
    "     \n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "         \n",
    "        # Convert messages to OpenAI format\n",
    "        if isinstance(messages, list):\n",
    "            formatted_messages = []\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, dict) and 'content' in msg:\n",
    "                    role = msg.get('role', 'user')\n",
    "                    formatted_messages.append({\n",
    "                        'role': role,\n",
    "                        'content': msg['content']\n",
    "                    })\n",
    "                else:\n",
    "                    formatted_messages.append({\n",
    "                        'role': 'user',\n",
    "                        'content': str(msg)\n",
    "                    })\n",
    "        else:\n",
    "            formatted_messages = [{'role': 'user', 'content': str(messages)}]\n",
    "         \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=formatted_messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            if not response.choices or not response.choices[0].message.content:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            \n",
    "            return [{\n",
    "                'text': response.choices[0].message.content,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "     \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                **kwargs\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key, model_name=\"gemini-2.0-flash\"):\n",
    "    lm = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def parse_list_from_response(response_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a Python list from various response formats including markdown code blocks\n",
    "    \"\"\"\n",
    "    if not response_text or response_text.strip() == \"\":\n",
    "        print(\"âš ï¸ å›æ‡‰ç‚ºç©º\")\n",
    "        return []\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```(?:python|json)?\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    \n",
    "    # Remove any additional backticks\n",
    "    text = text.strip('`').strip()\n",
    "    \n",
    "    # Try to find a list pattern in the text\n",
    "    list_match = re.search(r'\\[.*?\\]', text, re.DOTALL)\n",
    "    \n",
    "    if list_match:\n",
    "        list_text = list_match.group(0)\n",
    "    else:\n",
    "        print(f\"âš ï¸ ç„¡æ³•åœ¨å›æ‡‰ä¸­æ‰¾åˆ°åˆ—è¡¨æ ¼å¼\")\n",
    "        print(f\"å®Œæ•´å›æ‡‰: {text[:200]}...\")\n",
    "        return []\n",
    "    \n",
    "    # Clean up the list text\n",
    "    list_text = list_text.strip()\n",
    "    \n",
    "    # Try multiple parsing strategies\n",
    "    try:\n",
    "        # Strategy 1: Parse as-is\n",
    "        return json.loads(list_text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 2: Convert single quotes to double quotes\n",
    "        list_text_double = list_text.replace(\"'\", '\"')\n",
    "        return json.loads(list_text_double)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 3: Manual parsing for simple cases\n",
    "        # Remove brackets and split by comma\n",
    "        content = list_text.strip('[]').strip()\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        # Split by comma and clean each item\n",
    "        items = []\n",
    "        for item in content.split(','):\n",
    "            item = item.strip().strip('\"').strip(\"'\").strip()\n",
    "            if item:\n",
    "                items.append(item)\n",
    "        \n",
    "        if items:\n",
    "            print(f\"âœ“ ä½¿ç”¨æ‰‹å‹•è§£ææˆåŠŸ\")\n",
    "            return items\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ‰‹å‹•è§£æå¤±æ•—: {e}\")\n",
    "    \n",
    "    print(f\"âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±æ•—äº†\")\n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {list_text[:200]}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_news_relevant_fields(description_path: str, main_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    å¾æè¿°æ–‡ä»¶å’Œå¤§ç¶±æ–‡ä»¶ä¸­æå–ç›¸é—œæ¬„ä½\n",
    "    \n",
    "    Args:\n",
    "        description_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: ç¯©é¸å‡ºçš„æ¬„ä½åˆ—è¡¨\n",
    "    \"\"\"\n",
    "     \n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(description_path)\n",
    "    \n",
    "    prompt = f\"\"\"Using the following outline and list of data column descriptions, select only the columns that are useful for the outline.\n",
    "\n",
    "## outline\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Please return only a Python list of column names, like this:\n",
    "['player_name', 'match_score', 'duration', ...]\n",
    "\n",
    "Do not include explanations or any other text. Return only the list.\"\"\"\n",
    "     \n",
    "    result = lm.basic_request(prompt)\n",
    "    \n",
    "    print(f\"ğŸ” åŸå§‹å›æ‡‰:\\n{result}\\n\")\n",
    "    \n",
    "    selected_fields = parse_list_from_response(result)\n",
    "    \n",
    "    if selected_fields:\n",
    "        print(\"âœ… ç¯©é¸å‡ºçš„æ¬„ä½:\", selected_fields)\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè§£ææ¬„ä½åˆ—è¡¨\")\n",
    "    \n",
    "    return selected_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "692ab381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åŸå§‹å›æ‡‰:\n",
      "```python\n",
      "['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n",
      "```\n",
      "\n",
      "âœ… ç¯©é¸å‡ºçš„æ¬„ä½: ['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n",
      "æœ€çµ‚æ¬„ä½æ¸…å–®: ['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']\n"
     ]
    }
   ],
   "source": [
    "# ç›´æ¥èª¿ç”¨å‡½å¼\n",
    "fields = extract_news_relevant_fields(\"data_description.txt\", \"main.txt\")\n",
    "print(\"æœ€çµ‚æ¬„ä½æ¸…å–®:\", fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c560a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"set1.csv\")\n",
    "filtered_df = df[fields]\n",
    "filtered_df.to_csv(\"filtered_set1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc7c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ filtered_data_description.txt\n"
     ]
    }
   ],
   "source": [
    "def extract_descriptions_for_fields(fields: List[str], desc_path: str, output_path: str):\n",
    "    description_text = read_text_file(desc_path)\n",
    "\n",
    "    field_desc = {}\n",
    "    for line in description_text.splitlines():\n",
    "        for field in fields:\n",
    "            if line.lower().startswith(field.lower() + \":\"):\n",
    "                field_desc[field] = line.strip()\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for field in fields:\n",
    "                f.write(field_desc.get(field, f\"{field}: [Description not found]\") + \"\\n\")\n",
    "        print(f\"âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯«å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "\n",
    "extract_descriptions_for_fields(fields, 'data_description.txt', \"filtered_data_description.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c668469",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "\n",
    "è—‰ç”±äººç‚ºè¼¸å…¥å•é¡Œèˆ‡æ–¹å‘æç¤ºï¼Œçµ¦LLMåšå®Œæ•´åˆ†æå•é¡Œèˆ‡æ–¹å‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e639240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain_of_thought_response(main_path: str, desc_path: str, output_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆ Chain-of-Thought åˆ†æå›æ‡‰\n",
    "    \n",
    "    Args:\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        desc_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        str: ç”Ÿæˆçš„å›æ‡‰å…§å®¹,å¦‚æœå¤±æ•—å‰‡è¿”å› None\n",
    "    \"\"\"\n",
    "\n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(desc_path)\n",
    "\n",
    "    chain_prompt = f\"\"\"\n",
    "You are a planning assistant.\n",
    "Analyze the following outline and column descriptions.\n",
    "\n",
    "## Outline & Ideas:\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Step-by-step:\n",
    "1. Reflect on the structure and meaning of the content.\n",
    "2. Formulate relevant and meaningful questions or planning strategies.\n",
    "3. Be explicit and detailed, use Chain-of-Thought reasoning.\n",
    "4. Output all thoughts and questions in English only.\n",
    "\"\"\"\n",
    "\n",
    "    result = lm.basic_request(chain_prompt)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "        print(f\"âœ… Response saved to: {output_path}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to write output: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cbf0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response saved to: analyze_response.txt\n"
     ]
    }
   ],
   "source": [
    "response = generate_chain_of_thought_response(\n",
    "    main_path=\"main.txt\",\n",
    "    desc_path=\"filtered_data_description.txt\",\n",
    "    output_path=\"analyze_response.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1496f5f",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "\n",
    "è«‹LLMæ ¹æ“š\"analyze_response.txt\"æ€è€ƒå¯ä»¥ä½¿ç”¨çš„operationä¸¦å°‡çµæœå­˜æ–¼ \"operations_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6399f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ operations_info.json\n",
      "\n",
      "âœ… æ“ä½œåç¨±é™£åˆ—:\n",
      "['write', 'select_row', 'select_column', 'groupby', 'aggregate', 'sort', 'filter', 'calculate', 'merge', 'join', 'pivot_table', 'rolling_window', 'shift', 'value_counts', 'corr']\n"
     ]
    }
   ],
   "source": [
    "def analyze_operations(analyze_path: str, output_json: str) -> List[str]:\n",
    "    lm = setup_gemini_api(api_key)\n",
    "    analysis = read_text_file(analyze_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a news journalist want to analyze data not forecaster.\n",
    "Based on the following text analysis, identify multiple useful table operations\n",
    "and describe the direct meaning of each operation.\n",
    "\n",
    "## Text Analysis:\n",
    "{analysis}\n",
    "\n",
    "---\n",
    "\n",
    "Please output a numbered list in this format:\n",
    "1. write: If the table is clear or small enough, generates text based on the tables using the LLM.\n",
    "2. select_row: Description\n",
    "3. select_column: Description\n",
    "4. operation_name: Description\n",
    "5. operation_name: Description\n",
    "...\n",
    "\n",
    "IMPORTANT: operation must contain select_row, select_column, and write in the first three operation.\n",
    "\n",
    "Give important operations and at most 15 operations.\n",
    "operation_name should be different and each operation can not be similar.\n",
    "operation can be apply on many columns is better.\n",
    "Description just give the original definition of the operation name and give some useful functions name in pandas.\n",
    "Only include operations and their descriptions. Be concise and clear.\n",
    "\"\"\"\n",
    "\n",
    "    response = lm.basic_request(prompt)\n",
    "\n",
    "    operations = []\n",
    "    operations_dict = {}\n",
    "\n",
    "    try:\n",
    "        for line in response.strip().split('\\n'):\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            if \".\" in line:\n",
    "                num, rest = line.split(\".\", 1)\n",
    "                if \":\" in rest:\n",
    "                    name, desc = rest.strip().split(\":\", 1)\n",
    "                    name = name.strip()\n",
    "                    desc = desc.strip()\n",
    "                    operations.append(name)\n",
    "                    operations_dict[num.strip()] = {\"operation\": name, \"description\": desc}\n",
    "\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(operations_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ {output_json}\")\n",
    "        return operations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å›æ‡‰è™•ç†å¤±æ•—: {e}\\nåŸå§‹å›æ‡‰:\\n{response}\")\n",
    "        return []\n",
    "\n",
    "ops = analyze_operations(\"analyze_response.txt\", \"operations_info.json\")\n",
    "print(\"\\nâœ… æ“ä½œåç¨±é™£åˆ—:\")\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cebfb8",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "\n",
    "ä½¿LLMè‡ªå‹•åˆ†ætableé¸å‡ºåˆé©çš„operationæ”¾å…¥æ“ä½œæ± (operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14dcba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperationSignature(dspy.Signature):\n",
    "    \"\"\"Identify suitable operations for analyzing badminton match data.\"\"\"\n",
    "    data_description = dspy.InputField(desc=\"Overview and sample of the dataset\")\n",
    "    column_descriptions = dspy.InputField(desc=\"Descriptions of each column in the dataset\")\n",
    "    rules = dspy.InputField(desc=\"Rules for selecting operations\")\n",
    "    operations_list = dspy.OutputField(desc=\"A list of suitable operations number (e.g., [1, 2, 3, 4])\")\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: CSV æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: è®€å–çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    è®€å– JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: JSON æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "def parse_column_descriptions(description_text):\n",
    "    \"\"\"\n",
    "    è§£ææ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        description_text: æ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Returns:\n",
    "        dict: æ¬„ä½åç¨±åˆ°æè¿°çš„æ˜ å°„\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    pattern = r'''\n",
    "        ^                # Line start\n",
    "        (\\w+)            # Column name\n",
    "        :\\s+             # Colon and space\n",
    "        (.+?)            # Description text\n",
    "        (?=\\n\\w+:\\s+|\\Z) # Lookahead for next column or end of file\n",
    "    '''\n",
    "    matches = re.findall(pattern, description_text, flags=re.M | re.X)\n",
    "    for col_name, desc in matches:\n",
    "        clean_desc = ' '.join(desc.split()).strip()\n",
    "        descriptions[col_name] = clean_desc\n",
    "    return descriptions\n",
    "\n",
    "class BadmintonOperationSelector(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.chain_of_thought = dspy.ChainOfThought(OperationSignature)\n",
    "\n",
    "    def forward(self, data_description, column_descriptions, rules):\n",
    "        result = self.chain_of_thought(\n",
    "            data_description=data_description,\n",
    "            column_descriptions=str(column_descriptions),\n",
    "            rules=str(rules)\n",
    "        )\n",
    "        return self.extract_operations_from_result(result.operations_list)\n",
    "\n",
    "    def extract_operations_from_result(self, operations_text):\n",
    "        \"\"\"\n",
    "        å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        æ”¯æ´å¤šç¨®æ ¼å¼:\n",
    "        - [1, 2, 3, 4]\n",
    "        - 1, 2, 3, 4\n",
    "        - 1 2 3 4\n",
    "        - Operation 1, Operation 2, etc.\n",
    "        \"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç§»é™¤ markdown ä»£ç¢¼å¡Šæ¨™è¨˜\n",
    "        operations_text = re.sub(r'```(?:python|json)?\\s*', '', operations_text)\n",
    "        operations_text = operations_text.strip('`').strip()\n",
    "        \n",
    "        # å˜—è©¦è§£æ JSON æ ¼å¼ [1, 2, 3]\n",
    "        try:\n",
    "            # å°‹æ‰¾æ–¹æ‹¬è™Ÿä¸­çš„å…§å®¹\n",
    "            list_match = re.search(r'\\[([^\\]]+)\\]', operations_text)\n",
    "            if list_match:\n",
    "                list_content = list_match.group(1)\n",
    "                # æå–æ‰€æœ‰æ•¸å­—\n",
    "                numbers = re.findall(r'\\d+', list_content)\n",
    "                operations = [int(num) for num in numbers]\n",
    "                if operations:\n",
    "                    return operations\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # å¦‚æœæ²’æœ‰æ–¹æ‹¬è™Ÿ,å˜—è©¦ç›´æ¥æå–æ‰€æœ‰æ•¸å­—\n",
    "        numbers = re.findall(r'\\d+', operations_text)\n",
    "        if numbers:\n",
    "            operations = [int(num) for num in numbers]\n",
    "            return operations\n",
    "        \n",
    "        # å¦‚æœä»¥ä¸Šéƒ½å¤±æ•—,å˜—è©¦é€è¡Œè™•ç†\n",
    "        lines = operations_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # æå–è©²è¡Œä¸­çš„æ‰€æœ‰æ•¸å­—\n",
    "            line_numbers = re.findall(r'\\d+', line)\n",
    "            operations.extend([int(num) for num in line_numbers])\n",
    "        \n",
    "        # å»é‡ä¸¦æ’åº\n",
    "        if operations:\n",
    "            operations = sorted(list(set(operations)))\n",
    "        \n",
    "        return operations\n",
    "\n",
    "\n",
    "def analyze_badminton_match(data_path, column_desc_path, rules_path, model_name=\"gemini-2.0-flash-exp\"):\n",
    "    \"\"\"\n",
    "    åˆ†æç¾½çƒæ¯”è³½æ•¸æ“šä¸¦è­˜åˆ¥é©åˆçš„æ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        data_path: æ¯”è³½æ•¸æ“š CSV æ–‡ä»¶è·¯å¾‘\n",
    "        column_desc_path: æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        rules_path: æ“ä½œè¦å‰‡ JSON æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        list: è­˜åˆ¥å‡ºçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (æ•´æ•¸)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Reading badminton match data...\")\n",
    "    try:\n",
    "        match_data = read_badminton_data(data_path)\n",
    "        columns_desc_content = read_text_file(column_desc_path)\n",
    "        rules = read_json_file(rules_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading files: {e}\")\n",
    "        return []\n",
    "\n",
    "    column_descriptions = parse_column_descriptions(columns_desc_content)\n",
    "    setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    data_sample = match_data.to_string()\n",
    "    data_description = f\"\"\"\n",
    "    one match data:\n",
    "    {data_sample}\n",
    "\n",
    "    Data shape: {match_data.shape[0]} rows, {match_data.shape[1]} columns\n",
    "    Columns: {', '.join(match_data.columns)}\n",
    "    \"\"\"\n",
    "\n",
    "    selector = BadmintonOperationSelector()\n",
    "    operations = selector.forward(data_description, column_descriptions, rules)\n",
    "\n",
    "    print(f\"âœ… Identified {len(operations)} suitable operations:\")\n",
    "    for i, op in enumerate(operations, 1):\n",
    "        print(f\"{i}. Operation {op}\")\n",
    "\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e586506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading badminton match data...\n",
      "âœ… Identified 6 suitable operations:\n",
      "1. Operation 14\n",
      "2. Operation 4\n",
      "3. Operation 5\n",
      "4. Operation 15\n",
      "5. Operation 6\n",
      "6. Operation 8\n",
      "\n",
      "Final operations array: [14, 4, 5, 15, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "operations = analyze_badminton_match(\n",
    "    data_path=\"set1.csv\",\n",
    "    column_desc_path=\"data_description.txt\",\n",
    "    rules_path=\"operations_info.json\"\n",
    ")\n",
    "print(\"\\nFinal operations array:\", operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d09a",
   "metadata": {},
   "source": [
    "å°‡æ‰€æŒ‘é¸å‡ºä¾†çš„æ“ä½œå¯«å…¥\"operations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "689bc29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… operations.json has been created with 6 operations.\n",
      "Selected operations: [14, 4, 5, 15, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# å¾ JSON æª”æ¡ˆè®€å– operations\n",
    "original_operations_dict = read_json_file(\"operations_info.json\")\n",
    "\n",
    "# ä½ æƒ³è¦æŒ‘é¸çš„ operation ç·¨è™Ÿï¼ˆæ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹é€™å€‹ listï¼‰\n",
    "selected_numbers = operations\n",
    "\n",
    "# æ ¹æ“š selected_numbers é¸å‡ºå°æ‡‰æ“ä½œï¼Œä¸¦å¾ 1 é–‹å§‹é‡æ–°ç·¨è™Ÿ\n",
    "filtered_operations = []\n",
    "for new_number, original_number in enumerate(selected_numbers, start=1):\n",
    "    # å°‡æ•¸å­—è½‰æ›ç‚ºå­—ä¸²éµä¾†æŸ¥æ‰¾\n",
    "    key = str(original_number)\n",
    "    if key in original_operations_dict:\n",
    "        op_data = original_operations_dict[key]\n",
    "        filtered_operations.append({\n",
    "            \"number\": new_number,\n",
    "            \"operation\": op_data[\"operation\"],\n",
    "            \"description\": op_data[\"description\"]\n",
    "        })\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Operation {original_number} not found in operations_info.json\")\n",
    "\n",
    "# æ–°çš„ JSON çµæ§‹\n",
    "output_json = {\n",
    "    \"description\": \"Selected operations for badminton data analysis.\",\n",
    "    \"requirements\": [\n",
    "        \"The output must be based on the input data; do not hallucinate.\",\n",
    "        \"Give me the list of numbers.\"\n",
    "    ],\n",
    "    \"operations\": filtered_operations\n",
    "}\n",
    "\n",
    "# å¯«å…¥ JSON æª”æ¡ˆ\n",
    "with open(\"operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… operations.json has been created with {len(filtered_operations)} operations.\")\n",
    "print(f\"Selected operations: {selected_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743109f4",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "\n",
    "ç¯©é¸å‡ºæœ€åˆé©çš„1/2 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e3f7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def load_operations_from_json(json_file_path):\n",
    "    \"\"\"\n",
    "    Load operations from JSON file\n",
    "    æ”¯æ´å…©ç¨®æ ¼å¼:\n",
    "    1. èˆŠæ ¼å¼: {\"1\": {\"operation\": \"...\", \"description\": \"...\"}, ...}\n",
    "    2. æ–°æ ¼å¼: {\"operations\": [{\"number\": 1, \"name\": \"...\", \"description\": \"...\"}, ...]}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = read_json_file(json_file_path)\n",
    "        \n",
    "        operations_data = []\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯æ–°æ ¼å¼é‚„æ˜¯èˆŠæ ¼å¼\n",
    "        if 'operations' in data and isinstance(data['operations'], list):\n",
    "            # æ–°æ ¼å¼\n",
    "            operations_data = data['operations']\n",
    "        else:\n",
    "            # èˆŠæ ¼å¼: æ•¸å­—å­—ä¸²ä½œç‚ºéµ\n",
    "            for key in sorted(data.keys(), key=lambda x: int(x) if x.isdigit() else 0):\n",
    "                if key.isdigit():\n",
    "                    op_data = data[key]\n",
    "                    operations_data.append({\n",
    "                        'number': int(key),\n",
    "                        'name': op_data.get('operation', ''),\n",
    "                        'description': op_data.get('description', '')\n",
    "                    })\n",
    "        \n",
    "        # Create formatted operation strings for LLM processing\n",
    "        operation_strings = []\n",
    "        operation_details = []\n",
    "        \n",
    "        for op in operations_data:\n",
    "            number = op.get('number', '')\n",
    "            name = op.get('name', '')\n",
    "            description = op.get('description', '')\n",
    "            \n",
    "            # Format as: \"number. name: description\"\n",
    "            if number and name and description:\n",
    "                formatted_op = f\"{number}. {name}: {description}\"\n",
    "                operation_strings.append(formatted_op)\n",
    "                operation_details.append({\n",
    "                    'number': number,\n",
    "                    'name': name,\n",
    "                    'description': description,\n",
    "                    'formatted': formatted_op\n",
    "                })\n",
    "        \n",
    "        print(f\"å¾ {json_file_path} æˆåŠŸè¼‰å…¥ {len(operation_strings)} å€‹æ“ä½œ\")\n",
    "        return operation_details, operation_strings\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_file_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"éŒ¯èª¤: {json_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def filter_operations_direct_gemini(api_key, operations_list, operation_details, data_sample, data_info, removal_percentage=0.25, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Use Gemini API directly to filter operations and return operation numbers\n",
    "    \"\"\"\n",
    "    gemini_lm = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    \n",
    "    operations_count = len(operations_list)\n",
    "    operations_to_remove = int(operations_count * removal_percentage)\n",
    "    operations_to_keep = operations_count - operations_to_remove\n",
    "    \n",
    "    # Create numbered list of operations for easier reference\n",
    "    numbered_operations = \"\\n\".join([f\"{i+1}. {op}\" for i, op in enumerate(operations_list)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "æˆ‘æœ‰ä¸€å€‹ç¾½çƒæ¯”è³½çš„è³‡æ–™é›†å’Œ {operations_count} å€‹åˆ†ææ“ä½œã€‚\n",
    "\n",
    "è³‡æ–™æ¨£æœ¬:\n",
    "{data_sample}\n",
    "\n",
    "è³‡æ–™é›†è³‡è¨Š:\n",
    "{data_info}\n",
    "\n",
    "æ“ä½œæ¸…å–®:\n",
    "{numbered_operations}\n",
    "\n",
    "è«‹å¹«æˆ‘åˆ†æä¸¦ç§»é™¤ {operations_to_remove} å€‹æœ€ä¸åˆé©çš„æ“ä½œï¼ˆç´„ {removal_percentage*100:.0f}%ï¼‰ï¼Œä¿ç•™ {operations_to_keep} å€‹æœ€é©åˆçš„æ“ä½œã€‚\n",
    "\n",
    "è«‹è€ƒæ…®ä»¥ä¸‹æ¨™æº–ä¾†æ±ºå®šç§»é™¤å“ªäº›æ“ä½œï¼š\n",
    "1. èˆ‡å¯¦éš›è³‡æ–™æ¬„ä½çš„ç›¸é—œæ€§\n",
    "2. åœ¨çµ¦å®šè³‡æ–™é›†çµæ§‹ä¸‹çš„å¯è¡Œæ€§\n",
    "3. å°ç¾½çƒæ¯”è³½åˆ†æçš„å¯¦ç”¨åƒ¹å€¼\n",
    "4. é¿å…é‡è¤‡æˆ–éæ–¼ç›¸ä¼¼çš„æ“ä½œ\n",
    "\n",
    "è«‹å…ˆèªªæ˜ä½ çš„åˆ†ææ€è·¯ï¼Œç„¶å¾Œ**åªæä¾›è¦ä¿ç•™æ“ä½œçš„ç·¨è™Ÿ**ï¼ˆå¾æ“ä½œæè¿°é–‹é ­æå–çš„ç·¨è™Ÿï¼‰ã€‚\n",
    "\n",
    "è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "\n",
    "åˆ†ææ€è·¯ï¼š\n",
    "[ä½ çš„åˆ†æ]\n",
    "\n",
    "ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
    "[ç·¨è™Ÿ1, ç·¨è™Ÿ2, ç·¨è™Ÿ3, ...]\n",
    "\"\"\"\n",
    "    \n",
    "    response = gemini_lm.basic_request(prompt)\n",
    "    \n",
    "    # Extract kept operation numbers from the response\n",
    "    kept_operation_numbers = extract_operation_numbers_from_response(response, operation_details)\n",
    "    \n",
    "    return kept_operation_numbers, response\n",
    "\n",
    "\n",
    "def extract_operation_numbers_from_response(response, operation_details):\n",
    "    \"\"\"\n",
    "    Extract the operation numbers to keep from Gemini's response\n",
    "    \"\"\"\n",
    "    kept_numbers = []\n",
    "    \n",
    "    # Look for the section with kept operation numbers\n",
    "    lines = response.split('\\n')\n",
    "    \n",
    "    # Find the start of the operations list\n",
    "    start_extracting = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Look for section headers\n",
    "        if any(keyword in line.lower() for keyword in ['ä¿ç•™çš„æ“ä½œç·¨è™Ÿ', 'ä¿ç•™æ“ä½œç·¨è™Ÿ', 'ç·¨è™Ÿ', 'numbers']):\n",
    "            start_extracting = True\n",
    "            # æª¢æŸ¥æ¨™é¡Œè¡Œæœ¬èº«æ˜¯å¦åŒ…å«æ•¸å­—\n",
    "            bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if bracket_match:\n",
    "                numbers_text = bracket_match.group(1)\n",
    "                for item in numbers_text.split(','):\n",
    "                    number = re.search(r'(\\d+)', item.strip())\n",
    "                    if number:\n",
    "                        kept_numbers.append(int(number.group(1)))\n",
    "                if kept_numbers:\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        if start_extracting and line:\n",
    "            # Try to extract numbers from various formats\n",
    "            bracket_match = re.search(r'\\[(.*?)\\]', line)\n",
    "            if bracket_match:\n",
    "                numbers_text = bracket_match.group(1)\n",
    "                for item in numbers_text.split(','):\n",
    "                    number = re.search(r'(\\d+)', item.strip())\n",
    "                    if number:\n",
    "                        kept_numbers.append(int(number.group(1)))\n",
    "                break\n",
    "            \n",
    "            # Format 2: Numbered list or comma-separated numbers\n",
    "            numbers = re.findall(r'\\b(\\d+)\\b', line)\n",
    "            if numbers:\n",
    "                kept_numbers.extend([int(n) for n in numbers])\n",
    "                break\n",
    "    \n",
    "    # Remove duplicates and validate against available operations\n",
    "    valid_numbers = []\n",
    "    available_numbers = [detail['number'] for detail in operation_details]\n",
    "    \n",
    "    for num in kept_numbers:\n",
    "        if num in available_numbers and num not in valid_numbers:\n",
    "            valid_numbers.append(num)\n",
    "    \n",
    "    return valid_numbers\n",
    "\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataset\n",
    "    \"\"\"\n",
    "    summary = f\"\"\"\n",
    "è³‡æ–™é›†æ¦‚è¦:\n",
    "- ç¸½è¡Œæ•¸: {dataframe.shape[0]}\n",
    "- ç¸½åˆ—æ•¸: {dataframe.shape[1]}\n",
    "- æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\n",
    "\n",
    "å„æ¬„ä½è³‡è¨Š:\n",
    "\"\"\"\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        col_info = f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            unique_values = dataframe[col].unique()[:10]\n",
    "            col_info += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, unique_values))}\"\n",
    "        else:\n",
    "            col_info += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\"\n",
    "        \n",
    "        summary += col_info + \"\\n\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def filter_badminton_operations(operations_list, operation_details, dataframe, removal_percentage=0.25, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Main function to filter operations using Gemini LLM and return operation numbers\n",
    "    (åªä¿ç•™æ–¹æ³•äºŒ: ç›´æ¥ Gemini API)\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    \n",
    "    print(f\"åŸå§‹æ“ä½œæ•¸é‡: {len(operations_list)}\")\n",
    "    print(\"åŸå§‹æ“ä½œæ¸…å–®:\")\n",
    "    for i, op in enumerate(operations_list, 1):\n",
    "        print(f\"  {i}. {op}\")\n",
    "    \n",
    "    # Get data summary\n",
    "    data_summary = get_data_summary(dataframe)\n",
    "    data_sample = dataframe.head(5).to_string()\n",
    "    \n",
    "    print(f\"\\nä½¿ç”¨ Gemini LLM éæ¿¾æ“ä½œ (ç§»é™¤ {removal_percentage*100:.0f}%)...\")\n",
    "    \n",
    "    # Method 2: Direct Gemini API call\n",
    "    try:\n",
    "        print(\"æ–¹æ³•: ç›´æ¥ä½¿ç”¨ Gemini API...\")\n",
    "        direct_filtered_numbers, gemini_response = filter_operations_direct_gemini(\n",
    "            api_key, \n",
    "            operations_list, \n",
    "            operation_details,\n",
    "            data_sample, \n",
    "            data_summary, \n",
    "            removal_percentage,\n",
    "            model_name\n",
    "        )\n",
    "        print(f\"ç›´æ¥ API æ–¹æ³•ä¿ç•™äº† {len(direct_filtered_numbers)} å€‹æ“ä½œç·¨è™Ÿ\")\n",
    "        \n",
    "        print(\"\\nGemini å›æ‡‰:\")\n",
    "        print(\"=\"*50)\n",
    "        print(gemini_response)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ç›´æ¥ API æ–¹æ³•å¤±æ•—: {e}\")\n",
    "        direct_filtered_numbers = []\n",
    "    \n",
    "    # Final results\n",
    "    if direct_filtered_numbers:\n",
    "        final_operation_numbers = direct_filtered_numbers\n",
    "        print(f\"\\nä½¿ç”¨ç›´æ¥ API æ–¹æ³•çš„çµæœ\")\n",
    "    else:\n",
    "        # Fallback\n",
    "        target_count = int(len(operations_list) * (1 - removal_percentage))\n",
    "        final_operation_numbers = [detail['number'] for detail in operation_details[:target_count]]\n",
    "        print(f\"\\nç›´æ¥ API æ–¹æ³•å¤±æ•—ï¼Œä½¿ç”¨å‰ {target_count} å€‹æ“ä½œç·¨è™Ÿä½œç‚ºå‚™æ¡ˆ\")\n",
    "    \n",
    "    print(f\"\\næœ€çµ‚ä¿ç•™çš„æ“ä½œç·¨è™Ÿ ({len(final_operation_numbers)} å€‹):\")\n",
    "    for i, number in enumerate(final_operation_numbers, 1):\n",
    "        for detail in operation_details:\n",
    "            if detail['number'] == number:\n",
    "                print(f\"  {i}. ç·¨è™Ÿ {number}: {detail['name']}\")\n",
    "                break\n",
    "    \n",
    "    return final_operation_numbers\n",
    "\n",
    "\n",
    "def create_filtered_operations_json(original_json_path, filtered_numbers, output_path=\"filtered_operations.json\"):\n",
    "    \"\"\"\n",
    "    å‰µå»ºéæ¿¾å¾Œçš„æ“ä½œ JSON æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    # è®€å–åŸå§‹ JSON\n",
    "    all_data = read_json_file(original_json_path)\n",
    "    \n",
    "    # è™•ç†å…©ç¨®æ ¼å¼\n",
    "    if 'operations' in all_data and isinstance(all_data['operations'], list):\n",
    "        # æ–°æ ¼å¼\n",
    "        original_operations = all_data['operations']\n",
    "    else:\n",
    "        # èˆŠæ ¼å¼: è½‰æ›ç‚ºæ–°æ ¼å¼\n",
    "        original_operations = []\n",
    "        for key in sorted(all_data.keys(), key=lambda x: int(x) if x.isdigit() else 0):\n",
    "            if key.isdigit():\n",
    "                op_data = all_data[key]\n",
    "                original_operations.append({\n",
    "                    'number': int(key),\n",
    "                    'name': op_data.get('operation', ''),\n",
    "                    'description': op_data.get('description', '')\n",
    "                })\n",
    "    \n",
    "    # æ ¹æ“š filtered_numbers é¸å‡ºå°æ‡‰æ“ä½œï¼Œä¸¦å¾ 1 é–‹å§‹é‡æ–°ç·¨è™Ÿ\n",
    "    filtered_operations = []\n",
    "    for new_number, original_number in enumerate(filtered_numbers, start=1):\n",
    "        for op in original_operations:\n",
    "            if op[\"number\"] == original_number:\n",
    "                # ä½¿ç”¨ 'name' æˆ– 'operation' æ¬„ä½\n",
    "                op_name = op.get('name') or op.get('operation', '')\n",
    "                filtered_operations.append({\n",
    "                    \"number\": new_number,\n",
    "                    \"operation\": op_name,\n",
    "                    \"description\": op[\"description\"]\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    # æ–°çš„ JSON çµæ§‹\n",
    "    output_json = {\n",
    "        \"description\": \"Selected operations for badminton data analysis.\",\n",
    "        \"requirements\": [\n",
    "            \"The output must be based on the input data; do not hallucinate.\",\n",
    "            \"Give me the list of numbers.\"\n",
    "        ],\n",
    "        \"operations\": filtered_operations\n",
    "    }\n",
    "    \n",
    "    # å¯«å…¥ JSON æª”æ¡ˆ\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… {output_path} has been created with {len(filtered_operations)} operations.\")\n",
    "    return filtered_operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc4facc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations_info.json æˆåŠŸè¼‰å…¥ 15 å€‹æ“ä½œ\n",
      "åŸå§‹æ“ä½œæ•¸é‡: 15\n",
      "åŸå§‹æ“ä½œæ¸…å–®:\n",
      "  1. 1. write: Generate text based on the table after other operations, providing insights and conclusions. This operation transforms data into a readable news story. Useful functions: N/A\n",
      "  2. 2. select_row: Select a subset of rows based on specific criteria (e.g., score range, player, rally length). This allows focusing on particular game situations. Useful functions: `df.loc[]`, `df.iloc[]`\n",
      "  3. 3. select_column: Select specific columns relevant to the analysis (e.g., `type`, `time`, `roundscore_A`, `roundscore_B`). Useful functions: `df[['column1', 'column2']]`\n",
      "  4. 4. groupby: Group rows based on one or more columns (e.g., `player`, `type`, `lose_reason`) to calculate aggregate statistics. This helps identify trends and patterns. Useful functions: `df.groupby()`\n",
      "  5. 5. aggregate: Calculate summary statistics (e.g., mean, median, standard deviation, count) for grouped data. This quantifies trends and differences. Useful functions: `df.agg()`\n",
      "  6. 6. sort: Sort the data by one or more columns (e.g., `time`, `rally`, `roundscore_A`) to identify trends and patterns over time or score progression. Useful functions: `df.sort_values()`\n",
      "  7. 7. filter: Remove rows based on specified conditions (e.g., rallies shorter than a certain time, specific `lose_reasons`). Allows focusing on relevant data subsets. Useful functions: `df[df['column'] > value]`\n",
      "  8. 8. calculate: Create new columns based on existing data (e.g., rally duration, time between shots, score difference). This generates features for further analysis. Useful functions: `df['new_column'] = ...`\n",
      "  9. 9. merge: Combine data from different sources or tables based on a common column. Useful functions: `pd.merge()`\n",
      "  10. 10. join: Combine columns of two potentially different-sized DataFrames into a single DataFrame. Useful functions: `df.join()`\n",
      "  11. 11. pivot_table: Summarize data by creating a table that aggregates values based on two or more columns. Useful functions: `pd.pivot_table()`\n",
      "  12. 12. rolling_window: Calculate statistics over a moving window of rows (e.g., average rally duration over the last 10 rallies). Useful functions: `df.rolling()`\n",
      "  13. 13. shift: Shift the values in a column by a certain number of rows.  Useful functions: `df['column'].shift()`\n",
      "  14. 14. value_counts: Count the occurrences of each unique value in a column. Useful functions: `df['column'].value_counts()`\n",
      "  15. 15. corr: Calculate the correlation between columns. Useful functions: `df.corr()`\n",
      "\n",
      "ä½¿ç”¨ Gemini LLM éæ¿¾æ“ä½œ (ç§»é™¤ 20%)...\n",
      "æ–¹æ³•: ç›´æ¥ä½¿ç”¨ Gemini API...\n",
      "ç›´æ¥ API æ–¹æ³•ä¿ç•™äº† 12 å€‹æ“ä½œç·¨è™Ÿ\n",
      "\n",
      "Gemini å›æ‡‰:\n",
      "==================================================\n",
      "åˆ†ææ€è·¯ï¼š\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘æœƒè€ƒæ…®è³‡æ–™é›†çš„ç‰¹æ€§ã€‚è³‡æ–™é›†åŒ…å«ç¾½çƒæ¯”è³½ä¸­æ¯ä¸€å›åˆçš„è©³ç´°è³‡è¨Šï¼Œä¾‹å¦‚æ“Šçƒé¡å‹ã€ä½ç½®ã€å¾—åˆ†ç­‰ã€‚åŸºæ–¼é€™äº›è³‡è¨Šï¼Œä¸€äº›æ“ä½œæ¯”å…¶ä»–æ“ä½œæ›´é©åˆé€²è¡Œæ·±å…¥åˆ†æã€‚\n",
      "\n",
      "1. **ä¸ç›¸é—œæ€§**: `merge` å’Œ `join` æ“ä½œéœ€è¦é¡å¤–çš„è³‡æ–™é›†é€²è¡Œåˆä½µã€‚ç”±æ–¼æˆ‘å€‘åªæœ‰ä¸€å€‹è³‡æ–™é›†ï¼Œå› æ­¤é€™å…©å€‹æ“ä½œä¸ç›´æ¥é©ç”¨ã€‚é›–ç„¶å¯ä»¥è€ƒæ…®è‡ªåˆä½µï¼Œä½†å…¶åƒ¹å€¼ä¸å¤§ï¼Œä¸å¦‚å…¶ä»–æ“ä½œã€‚\n",
      "2. **å¯è¡Œæ€§**: `rolling_window` é›–ç„¶ç†è«–ä¸Šå¯è¡Œï¼Œä¾‹å¦‚å¯ä»¥åˆ†æéå»å¹¾å›åˆçš„å¹³å‡æ“Šçƒé«˜åº¦ï¼Œä½†åœ¨é€™å€‹è³‡æ–™é›†ä¸­ï¼Œå›åˆçš„é †åºå¯èƒ½ä¸¦ä¸ä»£è¡¨æ™‚é–“ä¸Šçš„é€£çºŒæ€§ï¼Œå› ç‚ºè³‡æ–™æ˜¯æŒ‰ç…§`rally`å’Œ`ball_round`æ’åºï¼Œç„¡æ³•åæ˜ æ¯”è³½çš„çœŸå¯¦æ™‚é–“æµé€ï¼Œå› æ­¤å…¶å¯¦ç”¨æ€§è¼ƒä½ã€‚\n",
      "3. **å¯¦ç”¨åƒ¹å€¼**: å‰©ä¸‹çš„æ“ä½œéƒ½ç›´æ¥æˆ–é–“æ¥åœ°æä¾›äº†æœ‰åƒ¹å€¼çš„ç¾½çƒæ¯”è³½åˆ†æè³‡è¨Šã€‚ä¾‹å¦‚ï¼Œ`groupby` å¯ä»¥åˆ†æä¸åŒæ“Šçƒé¡å‹æˆ–å¤±èª¤åŸå› çš„é »ç‡ï¼Œ`calculate` å¯ä»¥è¨ˆç®—å›åˆæŒçºŒæ™‚é–“ï¼Œ`corr` å¯ä»¥æ‰¾å‡ºä¸åŒè®Šæ•¸ä¹‹é–“çš„é—œè¯æ€§ã€‚`value_counts` å¯ä»¥æä¾›é—œæ–¼ç‰¹å®šæ¬„ä½ï¼ˆä¾‹å¦‚ï¼Œçƒçš„ç¨®é¡ï¼‰çš„çµ±è¨ˆè³‡è¨Šï¼Œè€Œ `pivot_table` å¯ä»¥ç”¨äºŒç¶­çš„æ–¹å¼å‘ˆç¾æ•¸æ“šï¼Œæ›´å®¹æ˜“ç†è§£å’Œåˆ†æã€‚`select_row`ï¼Œ`select_column`ï¼Œ`sort`å’Œ`filter`æ˜¯åŸºæœ¬ä½†æœ‰åŠ›çš„å·¥å…·ï¼Œå¹«åŠ©æˆ‘å€‘èšç„¦æ–¼æ•¸æ“šçš„å­é›†ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ†æã€‚æœ€å¾Œï¼Œ`write`å¯ä»¥å°‡åˆ†æçµæœè½‰åŒ–ç‚ºæ˜“æ–¼ç†è§£çš„æ–‡å­—å ±å‘Šã€‚\n",
      "\n",
      "ç¶œä¸Šæ‰€è¿°ï¼Œ`merge`ã€`join`å’Œ `rolling_window` ç›¸å°ä¸é©åˆï¼Œå› ç‚ºå®ƒå€‘æˆ–è€…éœ€è¦é¡å¤–çš„è³‡æ–™é›†ï¼Œæˆ–è€…åœ¨é€™å€‹è³‡æ–™é›†ä¸Šçš„æ‡‰ç”¨åƒ¹å€¼æœ‰é™ã€‚\n",
      "\n",
      "ä¿ç•™çš„æ“ä½œç·¨è™Ÿï¼š\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 11, 13, 14, 15]\n",
      "\n",
      "==================================================\n",
      "\n",
      "ä½¿ç”¨ç›´æ¥ API æ–¹æ³•çš„çµæœ\n",
      "\n",
      "æœ€çµ‚ä¿ç•™çš„æ“ä½œç·¨è™Ÿ (12 å€‹):\n",
      "  1. ç·¨è™Ÿ 1: write\n",
      "  2. ç·¨è™Ÿ 2: select_row\n",
      "  3. ç·¨è™Ÿ 3: select_column\n",
      "  4. ç·¨è™Ÿ 4: groupby\n",
      "  5. ç·¨è™Ÿ 5: aggregate\n",
      "  6. ç·¨è™Ÿ 6: sort\n",
      "  7. ç·¨è™Ÿ 7: filter\n",
      "  8. ç·¨è™Ÿ 8: calculate\n",
      "  9. ç·¨è™Ÿ 11: pivot_table\n",
      "  10. ç·¨è™Ÿ 13: shift\n",
      "  11. ç·¨è™Ÿ 14: value_counts\n",
      "  12. ç·¨è™Ÿ 15: corr\n",
      "\n",
      "ä¿ç•™çš„æ“ä½œç·¨è™Ÿæ¸…å–®: [1, 2, 3, 4, 5, 6, 7, 8, 11, 13, 14, 15]\n",
      "âœ… filtered_operations.json has been created with 6 operations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'number': 1,\n",
       "  'operation': 'value_counts',\n",
       "  'description': \"Count the occurrences of each unique value in a column. Useful functions: `df['column'].value_counts()`\"},\n",
       " {'number': 2,\n",
       "  'operation': 'groupby',\n",
       "  'description': 'Group rows based on one or more columns (e.g., `player`, `type`, `lose_reason`) to calculate aggregate statistics. This helps identify trends and patterns. Useful functions: `df.groupby()`'},\n",
       " {'number': 3,\n",
       "  'operation': 'aggregate',\n",
       "  'description': 'Calculate summary statistics (e.g., mean, median, standard deviation, count) for grouped data. This quantifies trends and differences. Useful functions: `df.agg()`'},\n",
       " {'number': 4,\n",
       "  'operation': 'corr',\n",
       "  'description': 'Calculate the correlation between columns. Useful functions: `df.corr()`'},\n",
       " {'number': 5,\n",
       "  'operation': 'sort',\n",
       "  'description': 'Sort the data by one or more columns (e.g., `time`, `rally`, `roundscore_A`) to identify trends and patterns over time or score progression. Useful functions: `df.sort_values()`'},\n",
       " {'number': 6,\n",
       "  'operation': 'calculate',\n",
       "  'description': \"Create new columns based on existing data (e.g., rally duration, time between shots, score difference). This generates features for further analysis. Useful functions: `df['new_column'] = ...`\"}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load operations from JSON file\n",
    "json_file_path = \"operations_info.json\"  # æ”¯æ´èˆŠæ ¼å¼\n",
    "operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "\n",
    "# Load badminton data\n",
    "example_df = read_badminton_data('set1.csv')\n",
    "\n",
    "# Filter operations and get operation numbers\n",
    "filtered_operation_numbers = filter_badminton_operations(\n",
    "    operation_strings,\n",
    "    operation_details,\n",
    "    example_df, \n",
    "    removal_percentage=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\nä¿ç•™çš„æ“ä½œç·¨è™Ÿæ¸…å–®: {filtered_operation_numbers}\")\n",
    "\n",
    "# å‰µå»ºéæ¿¾å¾Œçš„ JSON æ–‡ä»¶\n",
    "create_filtered_operations_json(\n",
    "    \"operations.json\",\n",
    "    filtered_operation_numbers,\n",
    "    \"filtered_operations.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c31ebe",
   "metadata": {},
   "source": [
    "# STEP 6\n",
    "\n",
    "æ ¹æ“šçœŸå¯¦tableåªä¿ç•™é‡è¦70%æ“ä½œï¼Œä¿ç•™'write' 'select_col' 'select_row'ä¸‰å€‹é‡è¦æ“ä½œï¼Œåˆ°'selected_operations.json'\n",
    "\n",
    "æ“ä½œæå–å·²å®Œæˆ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4cef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£å¼ç‰ˆ - ä½¿ç”¨ Gemini éæ¿¾ç¾½çƒæ¯”è³½åˆ†ææ“ä½œ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataset\n",
    "    \"\"\"\n",
    "    summary = f\"è³‡æ–™é›†æ¦‚è¦:\\n- ç¸½è¡Œæ•¸: {dataframe.shape[0]}\\n- ç¸½åˆ—æ•¸: {dataframe.shape[1]}\\n- æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\\n\\nå„æ¬„ä½è³‡è¨Š:\\n\"\n",
    "    for col in dataframe.columns:\n",
    "        summary += f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            summary += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, dataframe[col].unique()[:10]))}\\n\"\n",
    "        else:\n",
    "            summary += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\\n\"\n",
    "    return summary\n",
    "\n",
    "def extract_operation_numbers_from_response(response):\n",
    "    \"\"\"\n",
    "    å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "    æ”¯æ´å¤šç¨®æ ¼å¼\n",
    "    \"\"\"\n",
    "    # æ–¹æ³•1: åŒ¹é…ä»£ç¢¼å¡Šä¸­çš„æ•¸çµ„\n",
    "    pattern1 = r'```\\s*\\[([\\d,\\s]+)\\]\\s*```'\n",
    "    match = re.search(pattern1, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',')]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•2: åŒ¹é…æ™®é€šæ–¹æ‹¬è™Ÿä¸­çš„æ•¸çµ„\n",
    "    pattern2 = r'\\[([\\d,\\s]+)\\]'\n",
    "    match = re.search(pattern2, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',')]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•3: æå–æ‰€æœ‰æ•¸å­—\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        operation_list = [int(num) for num in numbers]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ’åºæ•¸çµ„\")\n",
    "    return []\n",
    "\n",
    "def filter_badminton_operations(operation_details, operation_strings, df, api_key, outline_path='outline.txt', model_name=\"gemini-2.0-flash\", max_retries=3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Gemini æ ¹æ“šé‡è¦æ€§æ’åºæ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        operation_strings: æ“ä½œæ ¼å¼åŒ–å­—ä¸²åˆ—è¡¨\n",
    "        df: æ•¸æ“šæ¡†\n",
    "        api_key: API é‡‘é‘°\n",
    "        outline_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: æ¨¡å‹åç¨±\n",
    "        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨, å®Œæ•´å›æ‡‰)\n",
    "    \"\"\"\n",
    "    gemini = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    data_summary = get_data_summary(df)\n",
    "    \n",
    "    # é™åˆ¶è³‡æ–™æ¨£æœ¬å¤§å°\n",
    "    data_sample = df.head(10).to_string()\n",
    "    if len(data_sample) > 3000:\n",
    "        data_sample = data_sample[:3000] + \"...\\n[è³‡æ–™å·²æˆªæ–·]\"\n",
    "    \n",
    "    outline = read_text_file(outline_path)\n",
    "    \n",
    "    print(f\"æ“ä½œæ•¸é‡: {len(operation_strings)}\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "æˆ‘æœ‰ä¸€å€‹æ’°å¯«æ–°èçš„å¤§ç¶±èˆ‡æ¯”è³½çš„è³‡æ–™é›†å’Œ {len(operation_strings)} å€‹åˆ†ææ“ä½œï¼Œè«‹ä¾æ“šæ“ä½œé‡è¦æ€§æ’åº(ç”±é«˜åˆ°ä½)ã€‚\n",
    "\n",
    "å¤§ç¶±:\n",
    "{outline}\n",
    "\n",
    "è³‡æ–™æ¨£æœ¬:\n",
    "{data_sample}\n",
    "\n",
    "è³‡æ–™é›†è³‡è¨Š:\n",
    "{data_summary}\n",
    "\n",
    "æ“ä½œæ¸…å–®:\n",
    "{chr(10).join(operation_strings)}\n",
    "\n",
    "è«‹å…ˆæ ¹æ“š chain-of-thought åˆ†æï¼Œç„¶å¾Œå°‡æ“ä½œç·¨è™Ÿæ ¹æ“šé‡è¦æ€§æ’åºï¼Œæ¯å€‹ç·¨è™Ÿåƒ…åœ¨é™£åˆ—ä¸­å‡ºç¾ä¸€æ¬¡ï¼Œé™£åˆ—é•·åº¦æ‡‰ç‚º {len(operation_strings)}ã€‚\n",
    "\n",
    "æœ€å¾Œè«‹ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºæ’åºçµæœ:[1, 2, 3, ...]\"\"\"\n",
    "    \n",
    "    response = gemini.basic_request(prompt, max_retries=max_retries)\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤å›æ‡‰\n",
    "    if \"âš ï¸\" in response:\n",
    "        print(f\"âŒ API å›æ‡‰éŒ¯èª¤\")\n",
    "        return [], response\n",
    "    \n",
    "    return extract_operation_numbers_from_response(response), response\n",
    "\n",
    "def create_selected_operations_json(operation_details, sorted_numbers, keep_percentage=0.7, force_include=[1, 2, 3], output_path=\"selected_operations.json\"):\n",
    "    \"\"\"\n",
    "    å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        sorted_numbers: æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        keep_percentage: ä¿ç•™æ¯”ä¾‹\n",
    "        force_include: å¼·åˆ¶åŒ…å«çš„æ“ä½œç·¨è™Ÿ\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        list: é¸æ“‡çš„æ“ä½œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # è¨ˆç®—è¦ä¿ç•™çš„æ“ä½œæ•¸é‡\n",
    "    keep_count = int(keep_percentage * len(sorted_numbers))\n",
    "    \n",
    "    # é¸æ“‡å‰ N å€‹æ“ä½œ\n",
    "    selected_numbers = sorted_numbers[:keep_count]\n",
    "    \n",
    "    # ç¢ºä¿å¼·åˆ¶åŒ…å«çš„æ“ä½œåœ¨åˆ—è¡¨ä¸­\n",
    "    selected_numbers = list(set(selected_numbers) | set(force_include))\n",
    "    \n",
    "    print(f\"é¸æ“‡äº† {len(selected_numbers)} å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: {keep_percentage*100:.0f}%)\")\n",
    "    print(f\"é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: {selected_numbers}\")\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„æ“ä½œåˆ—è¡¨\n",
    "    new_operations = []\n",
    "    for new_id, num in enumerate(selected_numbers, 1):\n",
    "        for detail in operation_details:\n",
    "            if int(detail['number']) == int(num):\n",
    "                new_operations.append({\n",
    "                    'number': new_id,\n",
    "                    'operation': detail['operation'],\n",
    "                    'description': detail['description']\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    # å¯«å…¥ JSON æ–‡ä»¶\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_operations, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… {output_path} has been created with {len(new_operations)} operations.\")\n",
    "    return new_operations\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7229122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations_info.json æˆåŠŸè¼‰å…¥ 15 å€‹æ“ä½œ\n",
      "æ“ä½œæ•¸é‡: 15\n",
      "æå–åˆ°æ“ä½œåˆ—è¡¨: [1, 3, 4, 5, 14, 8, 6, 2, 7, 11, 12, 13, 15, 9, 10]\n",
      "å®Œæ•´å›æ‡‰:\n",
      "Let's analyze the importance of each operation for generating badminton news from the given dataset. The goal is to extract insights that are newsworthy and insightful for badminton fans, while adhering to the restriction of avoiding common sense observations.\n",
      "\n",
      "Here's a chain of thought process:\n",
      "\n",
      "* **Essential for initial overview and context:**\n",
      "    * **1. Observe the outcome of the game and the final score:** This is the most basic information. Understanding who won and by how much is fundamental to any news report. Thus, calculating final scores is vital.\n",
      "    * **2. The duration of the entire competition:** The length of the match puts the result into context. A long, grueling match tells a different story than a quick victory.  So, total time is vital.\n",
      "\n",
      "* **Core for understanding game dynamics:**\n",
      "    * **Analyzing point scoring patterns and trends:** How were points scored? Where there extended rallies, or was one player dominating?\n",
      "        * **Calculate/Aggregate score differences and trends:** How did one player lead? Were there comebacks?\n",
      "    * **Understanding the rally structure and common plays:**\n",
      "        * **Analyzing common shot types and how points were won or lost:** What were the most common plays leading to points scored or mistakes?\n",
      "\n",
      "* **Operations contributing to specific insights:**\n",
      "    * Operations contributing to understanding the most common point-scoring patterns.\n",
      "        * **`value_counts`**: This operation is crucial for determining the frequency of different shot types (`type`) and the reasons for losing points (`lose_reason`). This will reveal which strategies are most effective and where players make the most mistakes.\n",
      "        * **`groupby` and `aggregate`**: Grouping by player and then by shot type to find out common play/strategy. We can also analyze win/lose reasons for each player to identify weaknesses and strengths.\n",
      "        * **`calculate`**: Create new columns for score difference and rally duration. This helps in analyzing momentum swings and the impact of long rallies.\n",
      "\n",
      "* **Operations useful for deeper analysis (but less critical for basic reporting):**\n",
      "    * **`sort`**: Sorting is useful for examining the chronological order of events, especially score changes. It allows for understanding momentum and crucial turning points.\n",
      "    * **`select_row`**: This will enable focusing on important moments (e.g., when a player is close to winning), which may reveal interesting strategies or psychological factors.\n",
      "    * **`filter`**: If certain anomalies needed filtering out (e.g., very short or long rallies, outliers).\n",
      "\n",
      "* **Less critical operations for initial news report generation:**\n",
      "    * **`pivot_table`**:  Pivot tables can be useful for summarizing data in a more organized way, but not essential.\n",
      "    * **`rolling_window`**: Calculating rolling statistics could be useful for identifying trends in performance over time, but this is a more advanced analysis.\n",
      "    * **`shift`**: Shifting data can be used to calculate differences between consecutive rallies or scores, providing insights into momentum changes. Could enhance reporting, but not essential at the start.\n",
      "    * **`merge` and `join`**: These operations are only useful if there are additional data sources.  Since we only have one dataset, they are not essential at all.\n",
      "    * **`corr`**: Correlation may not be too relevant in generating the initial news.\n",
      "\n",
      "* **Final Step:**\n",
      "    * **`write`**: Translates analysis into readable article.\n",
      "\n",
      "Therefore, based on this chain of thought, the operations can be ranked in the following order:\n",
      "\n",
      "[1, 3, 4, 5, 14, 8, 6, 2, 7, 11, 12, 13, 15, 9, 10]\n",
      "\n",
      "\n",
      "æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: [1, 3, 4, 5, 14, 8, 6, 2, 7, 11, 12, 13, 15, 9, 10]\n",
      "é¸æ“‡äº† 10 å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: 70%)\n",
      "é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: [1, 2, 3, 4, 5, 6, 7, 8, 11, 14]\n",
      "âœ… selected_operations.json has been created with 10 operations.\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥æ“ä½œ\n",
    "json_file_path = \"operations_info.json\"\n",
    "operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“š\n",
    "df = read_badminton_data(\"filtered_set1.csv\")\n",
    "\n",
    "# ç²å– API é‡‘é‘°\n",
    "api_key = os.getenv(\"Gemini_API\") \n",
    "# æ’åºæ“ä½œ\n",
    "sorted_numbers, response = filter_badminton_operations(\n",
    "    operation_details, \n",
    "    operation_strings, \n",
    "    df, \n",
    "    api_key, \n",
    "    outline_path='main.txt'\n",
    ")\n",
    "\n",
    "print(f\"å®Œæ•´å›æ‡‰:\\n{response}\\n\")\n",
    "print(f\"æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: {sorted_numbers}\")\n",
    "\n",
    "# å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON\n",
    "selected_ops = create_selected_operations_json(\n",
    "    operation_details,\n",
    "    sorted_numbers,\n",
    "    keep_percentage=0.7,\n",
    "    force_include=[1, 2, 3],\n",
    "    output_path=\"selected_operations.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93c578c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰æ“ä½œç·¨è™Ÿ: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "ç¸½æ“ä½œæ•¸é‡: 9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# è®€å– filtered_operations.json\n",
    "with open('filtered_operations.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# å–å¾—ç›®å‰æœ€å¤§ number\n",
    "existing_numbers = [op['number'] for op in data['operations']]\n",
    "max_number = max(existing_numbers) if existing_numbers else 0\n",
    "\n",
    "# æ–°å¢çš„ operations (æ³¨æ„ä½¿ç”¨ 'operation' è€Œé 'name')\n",
    "new_operations = [\n",
    "    {\n",
    "        \"number\": max_number + 1,\n",
    "        \"operation\": \"select_row\",\n",
    "        \"description\": \"Selects rows based on their row indices.\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": max_number + 2,\n",
    "        \"operation\": \"select_col\",\n",
    "        \"description\": \"Selects columns based on their column names.\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": max_number + 3,\n",
    "        \"operation\": \"write\",\n",
    "        \"description\": \"If the table is small enough, generates text based on the tables using the LLM; represents the leaf node of the tree.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# å°‡æ–°æ“ä½œåŠ å…¥åŸå§‹è³‡æ–™\n",
    "data['operations'].extend(new_operations)\n",
    "\n",
    "# å¯«å› JSON æª”\n",
    "with open('selected_operations.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# è¼¸å‡ºæ‰€æœ‰æ“ä½œçš„ number åˆ—è¡¨\n",
    "all_numbers = [op['number'] for op in data['operations']]\n",
    "print(f\"æ‰€æœ‰æ“ä½œç·¨è™Ÿ: {all_numbers}\")\n",
    "print(f\"ç¸½æ“ä½œæ•¸é‡: {len(all_numbers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf7a14",
   "metadata": {},
   "source": [
    "# STEP 7\n",
    "\n",
    "æ ¹æ“šTable,å¾—åˆ°è¦åŸ·è¡Œçš„æ“ä½œèˆ‡åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a65ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨Geminiç”Ÿæˆoperationså’Œarguments\n",
    "        \"\"\"\n",
    "        \n",
    "        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}.\n",
    "6. Only select Operations from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path)}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.basic_request(prompt)\n",
    "            \n",
    "            if response and \"âš ï¸\" not in response:\n",
    "                print(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                return response.strip()\n",
    "            else:\n",
    "                print(\"Geminiå›æ‡‰ç‚ºç©ºæˆ–å‡ºç¾éŒ¯èª¤\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return None\n",
    "\n",
    "def run_content_planner(csv_path='filtered_set1.csv', \n",
    "                       table_desc_path='filtered_data_description.txt',\n",
    "                       operations_path='selected_operations.json',\n",
    "                       outline_path='analyze_response.txt',\n",
    "                       max_depth=5,\n",
    "                       max_degree=5):\n",
    "    \"\"\"\n",
    "    é‹è¡Œå…§å®¹è¦åŠƒå™¨\n",
    "    \n",
    "    Args:\n",
    "        csv_path: CSV æ•¸æ“šæ–‡ä»¶è·¯å¾‘\n",
    "        table_desc_path: è¡¨æ ¼æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        operations_path: æ“ä½œæè¿° JSON æ–‡ä»¶è·¯å¾‘\n",
    "        outline_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        max_depth: æœ€å¤§æ·±åº¦\n",
    "        max_degree: æœ€å¤§åˆ†æ”¯åº¦\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ“ä½œå’Œåƒæ•¸å­—ä¸², æ›´æ–°å¾Œçš„æ“ä½œæ­·å², ç•¶å‰å±¤ç´š)\n",
    "    \"\"\"\n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ Gemini_API ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Content Planner for Badminton Game Report\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "    TABLES = pd.read_csv(csv_path)\n",
    "    tables_str = TABLES.head(10).to_string()  # é™åˆ¶é¡¯ç¤ºå‰10è¡Œ\n",
    "    if len(tables_str) > 3000:\n",
    "        tables_str = tables_str[:3000] + \"...\\n[è³‡æ–™å·²æˆªæ–·]\"\n",
    "    print(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "    \n",
    "    # è®€å–è¡¨æ ¼æè¿°\n",
    "    TABLE_DESCRIPTION = read_text_file(table_desc_path)\n",
    "    if not TABLE_DESCRIPTION:\n",
    "        TABLE_DESCRIPTION = \"No table description available\"\n",
    "    print(f\"è¼‰å…¥è¡¨æ ¼æè¿°: {len(TABLE_DESCRIPTION)} å­—ç¬¦\")\n",
    "    \n",
    "    # è®€å–æ“ä½œæè¿°\n",
    "    OPERATION_DESCRIPTION = read_json_file(operations_path)\n",
    "    print(f\"è¼‰å…¥æ“ä½œæè¿° JSON\")\n",
    "    \n",
    "    # è¨­ç½®å…¶ä»–è®Šæ•¸\n",
    "    OPERATION_HISTORY = ['root(None)']\n",
    "    Level = 0\n",
    "    \n",
    "    # å¾æ“ä½œæè¿°ä¸­æå–æ“ä½œæ± \n",
    "    # è™•ç†æ–°æ ¼å¼: ç›´æ¥æ˜¯æ“ä½œåˆ—è¡¨\n",
    "    if isinstance(OPERATION_DESCRIPTION, list):\n",
    "        OPERATION_POOL = [op.get('operation', op.get('name', '')) for op in OPERATION_DESCRIPTION if op.get('operation') or op.get('name')]\n",
    "    # è™•ç†èˆŠæ ¼å¼: {\"operations\": [...]}\n",
    "    elif isinstance(OPERATION_DESCRIPTION, dict) and 'operations' in OPERATION_DESCRIPTION:\n",
    "        OPERATION_POOL = [op.get('operation', op.get('name', '')) for op in OPERATION_DESCRIPTION['operations'] if op.get('operation') or op.get('name')]\n",
    "    else:\n",
    "        print(\"âŒ ç„¡æ³•è­˜åˆ¥çš„ JSON æ ¼å¼\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"æ“ä½œæ±  ({len(OPERATION_POOL)} å€‹): {OPERATION_POOL}\")\n",
    "    print(f\"æ“ä½œæ­·å²: {OPERATION_HISTORY}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–å…§å®¹è¦åŠƒå™¨\n",
    "    planner = ContentPlanner(api_key)\n",
    "    \n",
    "    # ç”Ÿæˆæ“ä½œå’Œåƒæ•¸\n",
    "    print(\"\\né–‹å§‹ç”Ÿæˆæ“ä½œå’Œåƒæ•¸...\")\n",
    "    operations_and_arguments = planner.generate_operations(\n",
    "        tables=tables_str,\n",
    "        table_description=TABLE_DESCRIPTION,\n",
    "        operation_description=OPERATION_DESCRIPTION,\n",
    "        operation_history=OPERATION_HISTORY,\n",
    "        operation_pool=OPERATION_POOL,\n",
    "        max_depth=max_depth,\n",
    "        max_degree=max_degree,\n",
    "        outline_path=outline_path\n",
    "    )\n",
    "    \n",
    "    # æ›´æ–°æ“ä½œæ­·å²\n",
    "    if operations_and_arguments:\n",
    "        OPERATION_HISTORY.append(operations_and_arguments)\n",
    "        Level += 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"GEMINI è¼¸å‡ºçµæœ:\")\n",
    "        print(\"=\"*50)\n",
    "        print(operations_and_arguments)\n",
    "        print(\"=\"*50)\n",
    "        print(f\"ç•¶å‰å±¤ç´š: {Level}\")\n",
    "        print(f\"æ›´æ–°å¾Œçš„æ“ä½œæ­·å²: {OPERATION_HISTORY}\")\n",
    "        \n",
    "        return operations_and_arguments, OPERATION_HISTORY, Level\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½ç”Ÿæˆæ“ä½œå’Œåƒæ•¸\")\n",
    "        return None, OPERATION_HISTORY, Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Planner for Badminton Game Report\n",
      "==================================================\n",
      "æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "è¼‰å…¥è¡¨æ ¼æè¿°: 481 å­—ç¬¦\n",
      "è¼‰å…¥æ“ä½œæè¿° JSON\n",
      "æ“ä½œæ±  (9 å€‹): ['value_counts', 'groupby', 'aggregate', 'corr', 'sort', 'calculate', 'select_row', 'select_col', 'write']\n",
      "æ“ä½œæ­·å²: ['root(None)']\n",
      "\n",
      "é–‹å§‹ç”Ÿæˆæ“ä½œå’Œåƒæ•¸...\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "å˜—è©¦ 1/3 å¤±æ•—: Error code: 503 - [{'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}]\n",
      "æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œç­‰å¾… 2 ç§’å¾Œé‡è©¦...\n",
      "å˜—è©¦ 2/3 å¤±æ•—: Error code: 503 - [{'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}]\n",
      "æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œç­‰å¾… 4 ç§’å¾Œé‡è©¦...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "\n",
      "==================================================\n",
      "GEMINI è¼¸å‡ºçµæœ:\n",
      "==================================================\n",
      "[select_col ( type , player , getpoint_player )]\n",
      "==================================================\n",
      "ç•¶å‰å±¤ç´š: 1\n",
      "æ›´æ–°å¾Œçš„æ“ä½œæ­·å²: ['root(None)', '[select_col ( type , player , getpoint_player )]']\n",
      "Content Planner for Badminton Game Report\n",
      "==================================================\n",
      "æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "è¼‰å…¥è¡¨æ ¼æè¿°: 481 å­—ç¬¦\n",
      "è¼‰å…¥æ“ä½œæè¿° JSON\n",
      "æ“ä½œæ±  (9 å€‹): ['value_counts', 'groupby', 'aggregate', 'corr', 'sort', 'calculate', 'select_row', 'select_col', 'write']\n",
      "æ“ä½œæ­·å²: ['root(None)']\n",
      "\n",
      "é–‹å§‹ç”Ÿæˆæ“ä½œå’Œåƒæ•¸...\n",
      "æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "\n",
      "==================================================\n",
      "GEMINI è¼¸å‡ºçµæœ:\n",
      "==================================================\n",
      "[select_col ( player, type, lose_reason, getpoint_player ), value_counts ( player ), value_counts ( type ), value_counts ( lose_reason ), value_counts ( getpoint_player )]\n",
      "==================================================\n",
      "ç•¶å‰å±¤ç´š: 1\n",
      "æ›´æ–°å¾Œçš„æ“ä½œæ­·å²: ['root(None)', '[select_col ( player, type, lose_reason, getpoint_player ), value_counts ( player ), value_counts ( type ), value_counts ( lose_reason ), value_counts ( getpoint_player )]']\n",
      "\n",
      "ç”Ÿæˆçš„æ“ä½œ: [select_col ( player, type, lose_reason, getpoint_player ), value_counts ( player ), value_counts ( type ), value_counts ( lose_reason ), value_counts ( getpoint_player )]\n",
      "æ“ä½œæ­·å²é•·åº¦: 2\n",
      "ç•¶å‰å±¤ç´š: 1\n"
     ]
    }
   ],
   "source": [
    "# æˆ–æŒ‡å®šè‡ªå®šç¾©åƒæ•¸\n",
    "operations_result, history, level = run_content_planner(\n",
    "    csv_path='filtered_set1.csv',\n",
    "    table_desc_path='filtered_data_description.txt',\n",
    "    operations_path='selected_operations.json',\n",
    "    outline_path='analyze_response.txt',\n",
    "    max_depth=5,\n",
    "    max_degree=5\n",
    ")\n",
    "\n",
    "if operations_result:\n",
    "    print(f\"\\nç”Ÿæˆçš„æ“ä½œ: {operations_result}\")\n",
    "    print(f\"æ“ä½œæ­·å²é•·åº¦: {len(history)}\")\n",
    "    print(f\"ç•¶å‰å±¤ç´š: {level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38a100",
   "metadata": {},
   "source": [
    "è§£æLLM responseå…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59f439c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select_col ( player, type, lose_reason, getpoint_player )', 'value_counts ( player )', 'value_counts ( type )', 'value_counts ( lose_reason )', 'value_counts ( getpoint_player )']\n"
     ]
    }
   ],
   "source": [
    "# æå–æ–¹æ‹¬å·å†…çš„å†…å®¹\n",
    "start = operations_result.find('[') + 1\n",
    "end = operations_result.rfind(']')\n",
    "content = operations_result[start:end].strip()\n",
    "\n",
    "elements = []\n",
    "current = []\n",
    "stack = 0\n",
    "\n",
    "# éå†å­—ç¬¦è¿›è¡Œè§£æ\n",
    "for char in content:\n",
    "    if char == '(':\n",
    "        stack += 1\n",
    "        current.append(char)\n",
    "    elif char == ')':\n",
    "        stack -= 1\n",
    "        current.append(char)\n",
    "    elif char == ',' and stack == 0:\n",
    "        elements.append(''.join(current).strip())\n",
    "        current = []\n",
    "    else:\n",
    "        current.append(char)\n",
    "\n",
    "# æ·»åŠ æœ€åä¸€ä¸ªå…ƒç´ \n",
    "if current:\n",
    "    elements.append(''.join(current).strip())\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a56c58",
   "metadata": {},
   "source": [
    "# STEP 8\n",
    "\n",
    "æ ¹æ“šæ¬„ä½å‹æ…‹èˆ‡'operation_name' å’Œ 'operation_argument'ï¼Œè«‹LLMæ’°å¯«å¯ä»¥åŸ·è¡Œçš„æ“ä½œç¨‹å¼ç¢¼\n",
    "\n",
    "å–æ¬„ä½å‹æ…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "093b3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          int64\n",
      "rally               int64\n",
      "time               object\n",
      "roundscore_A        int64\n",
      "roundscore_B        int64\n",
      "player             object\n",
      "type               object\n",
      "lose_reason        object\n",
      "getpoint_player    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_copy = pd.read_csv(\"filtered_set1.csv\")\n",
    "df = df_copy\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "557c4675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 315 entries, 0 to 314\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       315 non-null    int64 \n",
      " 1   rally            315 non-null    int64 \n",
      " 2   time             315 non-null    object\n",
      " 3   roundscore_A     315 non-null    int64 \n",
      " 4   roundscore_B     315 non-null    int64 \n",
      " 5   player           315 non-null    object\n",
      " 6   type             315 non-null    object\n",
      " 7   lose_reason      36 non-null     object\n",
      " 8   getpoint_player  36 non-null     object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 22.3+ KB\n",
      "ç”Ÿæˆçš„ç¨‹å¼ç¢¼ï¼š\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# è®€å–CSVæ•¸æ“šé›†\n",
      "df = pd.read_csv('filtered_set1.csv')\n",
      "\n",
      "# è¦åŸ·è¡Œçš„æ“ä½œï¼šselect_col ( player, type, lose_reason, getpoint_player )\n",
      "# å‡è¨­ select_col çš„åŠŸèƒ½æ˜¯é¸æ“‡æŒ‡å®šçš„æ¬„ä½\n",
      "\n",
      "selected_columns = ['player', 'type', 'lose_reason', 'getpoint_player']\n",
      "df_selected = df[selected_columns]\n",
      "\n",
      "# å°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥ 'tmp.csv'\n",
      "df_selected.to_csv('tmp.csv', index=False)\n",
      "\n",
      "# (å¯é¸) é¡¯ç¤ºDataFrameçš„å‰å¹¾è¡Œï¼Œä»¥ç¢ºèªçµæœ\n",
      "print(df_selected.head())\n",
      "```\n",
      "  player type lose_reason getpoint_player\n",
      "0      B  ç™¼é•·çƒ         NaN             NaN\n",
      "1      A   åˆ‡çƒ         NaN             NaN\n",
      "2      B   æŒ‘çƒ         NaN             NaN\n",
      "3      A   é•·çƒ         NaN             NaN\n",
      "4      B   æ®ºçƒ         NaN             NaN\n",
      "\n",
      "è™•ç†çµæœï¼š\n",
      "    player  type lose_reason getpoint_player\n",
      "0        B   ç™¼é•·çƒ         NaN             NaN\n",
      "1        A    åˆ‡çƒ         NaN             NaN\n",
      "2        B    æŒ‘çƒ         NaN             NaN\n",
      "3        A    é•·çƒ         NaN             NaN\n",
      "4        B    æ®ºçƒ         NaN             NaN\n",
      "..     ...   ...         ...             ...\n",
      "310      B  æœªçŸ¥çƒç¨®         NaN             NaN\n",
      "311      A    åˆ‡çƒ         NaN             NaN\n",
      "312      B    æŒ‘çƒ         NaN             NaN\n",
      "313      A    é•·çƒ         NaN             NaN\n",
      "314      B    é•·çƒ          å‡ºç•Œ               A\n",
      "\n",
      "[315 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameOperator:\n",
    "    def __init__(self, api_key):\n",
    "        self.lm = setup_gemini_api(api_key)\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'ï¼Œæ’°å¯«å®Œæ•´python code.\n",
    "        åˆ‡å¿Œæ¯å€‹æ“ä½œåƒæ•¸éƒ½éœ€è¦ä½¿ç”¨\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self.lm.basic_request(prompt)\n",
    "\n",
    "    def safe_execute(self, code, df):\n",
    "        try:\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆä½œç‚ºæ¨¡æ“¬ df.csv è·¯å¾‘\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            # å»ºç«‹å®‰å…¨åŸ·è¡Œç’°å¢ƒ\n",
    "            exec_globals = {'pd': pd}\n",
    "            exec_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œç”Ÿæˆçš„ç¨‹å¼ç¢¼\n",
    "            exec(code, exec_globals, exec_locals)\n",
    "\n",
    "            # å¾ tmp.csv è®€å–è™•ç†å¾Œçš„çµæœ\n",
    "            result_df = pd.read_csv(\"tmp.csv\")\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "API_KEY = os.getenv(\"Gemini_API\")\n",
    "operator = DataFrameOperator(API_KEY)\n",
    "\n",
    "# ç²å–è³‡æ–™è³‡è¨Š\n",
    "df_info = df.info()\n",
    "df_path = \"filtered_set1.csv\"\n",
    "operation_def = elements[0]\n",
    "#print(operation)\n",
    "\n",
    "generated_code = operator.generate_code(\n",
    "    operation=operation_def,\n",
    "    df_info=df_info,\n",
    "    df_path=df_path\n",
    ")\n",
    "\n",
    "print(\"ç”Ÿæˆçš„ç¨‹å¼ç¢¼ï¼š\")\n",
    "print(generated_code)\n",
    "\n",
    "# åŸ·è¡Œæ“ä½œ\n",
    "processed_df = operator.safe_execute(generated_code, df)\n",
    "\n",
    "print(\"\\nè™•ç†çµæœï¼š\")\n",
    "print(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1, Node 1: 1 children\n",
      "Level 2, Node 2: 2 children\n",
      "Level 3, Node 4: 5 children\n",
      "Level 4, Node 6: 1 children\n",
      "Level 4, Node 10: 5 children\n",
      "\n",
      "æ¨¹ç‹€çµæ§‹:\n",
      "- Node(1, Level=1)\n",
      "  - Node(2, Level=2)\n",
      "    - Node(3, Level=3)\n",
      "    - Node(4, Level=3)\n",
      "      - Node(5, Level=4)\n",
      "      - Node(6, Level=4)\n",
      "        - Node(7, Level=5)\n",
      "      - Node(8, Level=4)\n",
      "      - Node(9, Level=4)\n",
      "      - Node(10, Level=4)\n",
      "        - Node(11, Level=5)\n",
      "        - Node(12, Level=5)\n",
      "        - Node(13, Level=5)\n",
      "        - Node(14, Level=5)\n",
      "        - Node(15, Level=5)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#å¯åˆª\n",
    "class TreeNode:\n",
    "    def __init__(self, value, level=0, text=\"\", table=None):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "        self.text = text\n",
    "        self.table = table\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode({self.value}, level={self.level})\"\n",
    "\n",
    "def build_random_tree(current_depth=1, max_depth=5, max_degree=5, value_counter=None):\n",
    "    if value_counter is None:\n",
    "        value_counter = [0]\n",
    "    \n",
    "    value_counter[0] += 1\n",
    "    node = TreeNode(value_counter[0], level=current_depth)\n",
    "\n",
    "    if current_depth >= max_depth or random.random() < 0.3:\n",
    "        return node  # è‘‰ç¯€é»\n",
    "\n",
    "    degree = random.randint(1, max_degree)\n",
    "    print(f\"Level {current_depth}, Node {value_counter[0]}: {degree} children\")\n",
    "    \n",
    "    for _ in range(degree):\n",
    "        child = build_random_tree(current_depth + 1, max_depth, max_degree, value_counter)\n",
    "        node.children.append(child)\n",
    "\n",
    "    return node\n",
    "\n",
    "def print_tree(node, level=0):\n",
    "    print(\"  \" * level + f\"- Node({node.value}, Level={node.level})\")\n",
    "    for child in node.children:\n",
    "        print_tree(child, level + 1)\n",
    "\n",
    "# å»ºç«‹ä¸¦å°å‡ºéš¨æ©Ÿæ¨¹\n",
    "random.seed(42)  # å¯é‡ç¾æ€§\n",
    "root = build_random_tree()\n",
    "print(\"\\næ¨¹ç‹€çµæ§‹:\")\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a7919",
   "metadata": {},
   "source": [
    "# STEP final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:28,229 - INFO - Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\n",
      "2025-09-28 18:24:28,230 - INFO - ==================================================\n",
      "2025-09-28 18:24:28,231 - INFO - æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "2025-09-28 18:24:28,234 - INFO - æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 9 åˆ—\n",
      "2025-09-28 18:24:28,235 - INFO - æœ€å¤§æ·±åº¦: 3\n",
      "2025-09-28 18:24:28,235 - INFO - æœ€å¤§åˆ†æ”¯åº¦: 4\n",
      "2025-09-28 18:24:28,237 - INFO - è¼‰å…¥æ“ä½œæ± : ['description', 'requirements', 'operations']\n",
      "2025-09-28 18:24:28,238 - INFO - é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\n",
      "2025-09-28 18:24:28,239 - INFO - è™•ç†ç¯€é» - Level: 0, Operation: root(None)\n",
      "2025-09-28 18:24:28,261 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:24:30,509 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:24:30,510 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'value_counts(lose_reason)', 'value_counts(getpoint_player)']\n",
      "2025-09-28 18:24:32,694 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:32,695 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:32,695 - INFO - æ·»åŠ å­ç¯€é»: 80f7ab6b to 74b9da25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type') æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:34,318 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:24:34,318 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(lose_reason), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:24:34,319 - INFO - æ·»åŠ å­ç¯€é»: a03b196e to 74b9da25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:36,517 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:36,518 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(getpoint_player), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:36,519 - INFO - æ·»åŠ å­ç¯€é»: a7746a65 to 74b9da25\n",
      "2025-09-28 18:24:36,519 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(type)\n",
      "2025-09-28 18:24:36,521 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_countsæ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:37,269 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:24:37,269 - WARNING - æœªçŸ¥æ“ä½œ: groupby\n",
      "2025-09-28 18:24:37,270 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: groupby(type)\n",
      "2025-09-28 18:24:37,271 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'aggregate(count)']\n",
      "2025-09-28 18:24:38,932 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:38,932 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:38,933 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: value_counts(type)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts æ“ä½œå·²å®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:40,234 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:40,235 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:24:40,236 - INFO - æ·»åŠ å­ç¯€é»: 853d93ae to 80f7ab6b\n",
      "2025-09-28 18:24:40,236 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(lose_reason)\n",
      "2025-09-28 18:24:40,239 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ“ä½œæˆåŠŸå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:41,307 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:24:41,307 - INFO - ç”Ÿæˆæ“ä½œ: ['aggregate(count)', 'sort(count, ascending=False)', 'write()']\n",
      "2025-09-28 18:24:45,342 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (3, 2)\n",
      "2025-09-28 18:24:45,343 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count), çµæœå½¢ç‹€: (3, 2)\n",
      "2025-09-28 18:24:45,344 - INFO - æ·»åŠ å­ç¯€é»: b58fd399 to a03b196e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully aggregated data from 'input_tmp.csv' and saved to 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:46,631 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:24:46,632 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count, ascending=False), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:24:46,633 - INFO - æ·»åŠ å­ç¯€é»: 7cad4196 to a03b196e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully processed and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:47,809 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:24:47,810 - INFO - æ·»åŠ å­ç¯€é»: b8dd5319 to a03b196e\n",
      "2025-09-28 18:24:47,810 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(getpoint_player)\n",
      "2025-09-28 18:24:47,813 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:24:49,401 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:24:49,402 - INFO - ç”Ÿæˆæ“ä½œ: ['aggregate(count)', 'sort(count)', 'write()']\n",
      "2025-09-28 18:24:51,702 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:51,703 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:51,704 - INFO - æ·»åŠ å­ç¯€é»: 4f0385c0 to a7746a65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame aggregated and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:53,389 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:53,390 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:24:53,391 - INFO - æ·»åŠ å­ç¯€é»: 30009f71 to a7746a65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameæ’åºå®Œæˆä¸¦å·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:24:54,526 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:24:54,526 - INFO - æ·»åŠ å­ç¯€é»: 683fa451 to a7746a65\n",
      "2025-09-28 18:24:54,527 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-09-28 18:24:54,529 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:24:55,195 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:24:55,196 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'write()']\n",
      "2025-09-28 18:24:55,265 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 1/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 4.990579594s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "2025-09-28 18:24:56,942 - WARNING - ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ 2/2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 3.303939243s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "2025-09-28 18:24:56,943 - WARNING - ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: value_counts(type)\n",
      "2025-09-28 18:24:57,005 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 3.246608793s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "2025-09-28 18:24:57,005 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "2025-09-28 18:25:27,091 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 33.160406608s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "2025-09-28 18:25:27,092 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (2/3)...\n",
      "2025-09-28 18:25:58,442 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:25:58,443 - INFO - æ·»åŠ å­ç¯€é»: dfb875fe to 7571620e\n",
      "2025-09-28 18:25:58,444 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: aggregate(count)\n",
      "2025-09-28 18:25:58,446 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:25:59,482 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:25:59,483 - WARNING - æœªçŸ¥æ“ä½œ: select_col\n",
      "2025-09-28 18:25:59,483 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: select_col(type, count)\n",
      "2025-09-28 18:25:59,484 - INFO - ç”Ÿæˆæ“ä½œ: ['sort(count, ascending=False)', 'write()']\n",
      "2025-09-28 18:26:01,293 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:26:01,294 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count, ascending=False), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-28 18:26:01,294 - INFO - æ·»åŠ å­ç¯€é»: 0044ff01 to 853d93ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVæª”æ¡ˆè™•ç†å®Œæˆï¼Œå·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_27704\\2190742969.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-28 18:26:02,283 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:02,283 - INFO - æ·»åŠ å­ç¯€é»: 5c760aab to 853d93ae\n",
      "2025-09-28 18:26:02,284 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: aggregate(count)\n",
      "2025-09-28 18:26:02,286 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:26:03,064 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:26:03,064 - WARNING - æœªçŸ¥æ“ä½œ: select_col\n",
      "2025-09-28 18:26:03,065 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: select_col(lose_reason, count)\n",
      "2025-09-28 18:26:03,065 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-28 18:26:04,024 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:04,025 - INFO - æ·»åŠ å­ç¯€é»: 06f5bc63 to b58fd399\n",
      "2025-09-28 18:26:04,026 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: sort(count, ascending=False)\n",
      "2025-09-28 18:26:04,028 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:26:04,836 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:26:04,838 - WARNING - æœªçŸ¥æ“ä½œ: groupby\n",
      "2025-09-28 18:26:04,839 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: groupby(lose_reason)\n",
      "2025-09-28 18:26:04,839 - INFO - ç”Ÿæˆæ“ä½œ: ['aggregate(count)', 'write()']\n",
      "2025-09-28 18:26:06,655 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:26:06,655 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count), çµæœå½¢ç‹€: (4, 2)\n",
      "2025-09-28 18:26:06,656 - INFO - æ·»åŠ å­ç¯€é»: 20bd6840 to 7cad4196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èšåˆæ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:07,794 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:07,795 - INFO - æ·»åŠ å­ç¯€é»: 9908ae30 to 7cad4196\n",
      "2025-09-28 18:26:07,796 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: aggregate(count)\n",
      "2025-09-28 18:26:07,798 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-28 18:26:08,633 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:26:08,634 - WARNING - æœªçŸ¥æ“ä½œ: select_col\n",
      "2025-09-28 18:26:08,634 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: select_col(getpoint_player, count)\n",
      "2025-09-28 18:26:08,635 - WARNING - æœªçŸ¥æ“ä½œ: corr\n",
      "2025-09-28 18:26:08,636 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: corr()\n",
      "2025-09-28 18:26:08,637 - INFO - ç”Ÿæˆæ“ä½œ: ['sort(count)']\n",
      "2025-09-28 18:26:10,345 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:26:10,346 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: sort(count), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-28 18:26:10,346 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['è¡¨æ ¼å…§å®¹æœªç™¼ç”Ÿè®ŠåŒ–ä½†éå¯«ä½œæ“ä½œ']\n",
      "2025-09-28 18:26:10,347 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: sort(count)\n",
      "2025-09-28 18:26:10,349 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ’åºå®Œæˆï¼Œä¸¦å·²å°‡çµæœå„²å­˜åˆ° 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:11,226 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-28 18:26:11,226 - WARNING - æœªçŸ¥æ“ä½œ: select_col\n",
      "2025-09-28 18:26:11,227 - WARNING - ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: select_col(getpoint_player, count)\n",
      "2025-09-28 18:26:11,227 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-28 18:26:12,222 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:12,223 - INFO - æ·»åŠ å­ç¯€é»: d6b2e02e to 30009f71\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_27704\\2190742969.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-28 18:26:13,515 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:13,516 - INFO - æ·»åŠ å­ç¯€é»: ec945e12 to 0044ff01\n",
      "2025-09-28 18:26:14,592 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:14,593 - INFO - æ·»åŠ å­ç¯€é»: 9d8e71de to 20bd6840\n",
      "2025-09-28 18:26:15,464 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-28 18:26:15,465 - INFO - æ·»åŠ å­ç¯€é»: 2ea717ac to d39ff5e0\n",
      "2025-09-28 18:26:15,466 - INFO - ç¯€é» ec945e12 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13     æ’²çƒ      5\n",
      "14   é˜²å®ˆå›æŠ½      5\n",
      "15     é»æ‰£      4\n",
      "16     å¹³çƒ      2\n",
      "17   é˜²å®ˆå›æŒ‘      2\n",
      "ç¯€é»æ–‡æœ¬: å¾æ•¸æ“šä¾†çœ‹ï¼Œé•·çƒçš„ä½¿ç”¨æ¬¡æ•¸æœ€å¤šï¼Œé”åˆ°55æ¬¡ï¼Œè€Œæ®ºçƒä¹Ÿç›¸ç•¶é »ç¹ï¼Œæœ‰36æ¬¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæœªçŸ¥çƒç¨®å‡ºç¾äº†16æ¬¡ï¼Œé€™éƒ¨åˆ†å¯èƒ½å½±éŸ¿äº†æ›´ç²¾ç¢ºçš„æˆ°è¡“åˆ†æã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:15,979 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 44.326981714s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "2025-09-28 18:26:15,980 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "2025-09-28 18:26:53,092 - INFO - ç¯€é» 0044ff01 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:26:53,094 - INFO - ç¯€é» 5c760aab æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13     æ’²çƒ      5\n",
      "14   é˜²å®ˆå›æŠ½      5\n",
      "15     é»æ‰£      4\n",
      "16     å¹³çƒ      2\n",
      "17   é˜²å®ˆå›æŒ‘      2\n",
      "ç¯€é»æ–‡æœ¬: å¾æ•¸æ“šåˆ†æï¼Œé•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œå…±55æ¬¡ï¼Œæ®ºçƒæ¬¡æ•¸ä¹Ÿå¤šï¼Œæœ‰36æ¬¡ã€‚å¦å¤–ï¼ŒæœªçŸ¥çƒç¨®å‡ºç¾16æ¬¡ï¼Œå¯èƒ½å½±éŸ¿æˆ°è¡“åˆ†æçš„æº–ç¢ºæ€§ã€‚\n",
      "node.table:      type  count\n",
      "0      åˆ‡çƒ     31\n",
      "1      å‹¾çƒ     12\n",
      "2      å¹³çƒ      2\n",
      "3   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "4      æŒ‘çƒ     35\n",
      "5      æ¨çƒ     31\n",
      "6      æ’²çƒ      5\n",
      "7     æ“‹å°çƒ     20\n",
      "8     æ”¾å°çƒ     28\n",
      "9    æœªçŸ¥çƒç¨®     16\n",
      "10     æ®ºçƒ     36\n",
      "11    ç™¼çŸ­çƒ     10\n",
      "12    ç™¼é•·çƒ     10\n",
      "13   éåº¦åˆ‡çƒ      6\n",
      "14     é•·çƒ     55\n",
      "15   é˜²å®ˆå›æŠ½      5\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     é»æ‰£      4\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ®ºçƒå¾—åˆ†æ¬¡æ•¸é”åˆ°36æ¬¡ï¼Œè€Œé•·çƒçš„ä½¿ç”¨æ›´æ˜¯é«˜é”55æ¬¡ï¼Œä½†è¦æ³¨æ„çš„æ˜¯ï¼Œä¸æ˜åŸå› çš„å¤±åˆ†ä¹Ÿä¸å°‘ï¼Œé«˜é”16æ¬¡ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†ææ”¹é€²ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:55,245 - INFO - ç¯€é» 853d93ae æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      åˆ‡çƒ     31\n",
      "1      å‹¾çƒ     12\n",
      "2      å¹³çƒ      2\n",
      "3   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "4      æŒ‘çƒ     35\n",
      "5      æ¨çƒ     31\n",
      "6      æ’²çƒ      5\n",
      "7     æ“‹å°çƒ     20\n",
      "8     æ”¾å°çƒ     28\n",
      "9    æœªçŸ¥çƒç¨®     16\n",
      "10     æ®ºçƒ     36\n",
      "11    ç™¼çŸ­çƒ     10\n",
      "12    ç™¼é•·çƒ     10\n",
      "13   éåº¦åˆ‡çƒ      6\n",
      "14     é•·çƒ     55\n",
      "15   é˜²å®ˆå›æŠ½      5\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     é»æ‰£      4\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šé¡¯ç¤ºï¼Œçƒå“¡å¤§é‡ä½¿ç”¨é•·çƒï¼Œæ¬¡æ•¸é«˜é”55æ¬¡ï¼Œæ®ºçƒæ¬¡æ•¸ä¹Ÿå¤šï¼Œæœ‰36æ¬¡å¾—åˆ†ã€‚ç„¶è€Œï¼Œæœ‰16æ¬¡ä¸æ˜åŸå› çš„å¤±åˆ†ï¼Œä»¥åŠ16æ¬¡æœªçŸ¥çƒç¨®ï¼Œé€™äº›éƒ½éœ€é€²ä¸€æ­¥åˆ†æå’Œæ”¹é€²ï¼Œä»¥æå‡æˆ°è¡“åŸ·è¡Œçš„æº–ç¢ºæ€§ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:56,426 - INFO - ç¯€é» 80f7ab6b æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:26:56,429 - INFO - ç¯€é» 06f5bc63 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šé¡¯ç¤ºï¼Œçƒå“¡å¤§é‡ä½¿ç”¨é•·çƒï¼ˆ55æ¬¡ï¼‰å’Œæ®ºçƒï¼ˆ36æ¬¡å¾—åˆ†ï¼‰ã€‚ç„¶è€Œï¼Œå‡ºç¾äº†16æ¬¡ä¸æ˜åŸå› çš„å¤±åˆ†å’Œ16æ¬¡æœªçŸ¥çƒç¨®ï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†æå’Œæ”¹é€²ï¼Œä»¥æå‡æˆ°è¡“åŸ·è¡Œçš„æº–ç¢ºæ€§ã€‚\n",
      "node.table:   lose_reason  count\n",
      "0           a      4\n",
      "1           b      2\n",
      "2           c      4\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½å¤±åˆ†ä¸»è¦é›†ä¸­åœ¨\"a\"åŸå› ï¼Œé«˜é”4æ¬¡ï¼Œéœ€è¦é‡é»æª¢è¨ã€‚åŒæ™‚ï¼ŒæœªçŸ¥çƒå“¡è¡¨ç¾äº®çœ¼ï¼Œå¤šæ¬¡å¾—åˆ†ï¼Œå€¼å¾—é€²ä¸€æ­¥åˆ†æå…¶æ‰“æ³•å’Œå„ªå‹¢ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:57,775 - INFO - ç¯€é» b58fd399 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:26:57,776 - INFO - ç¯€é» 9d8e71de æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   lose_reason  count\n",
      "0           a      4\n",
      "1           b      2\n",
      "2           c      4\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½å¤±åˆ†é›†ä¸­åœ¨\"a\"åŸå› ï¼Œå…±4æ¬¡ï¼Œéœ€é‡é»æª¢è¨ã€‚åŒæ™‚ï¼ŒæœªçŸ¥çƒå“¡è¡¨ç¾äº®çœ¼ï¼Œå¤šæ¬¡å¾—åˆ†ï¼Œå…¶æ‰“æ³•å’Œå„ªå‹¢å€¼å¾—é€²ä¸€æ­¥åˆ†æã€‚\n",
      "node.table:   lose_reason  count\n",
      "0          å‡ºç•Œ      1\n",
      "1      å°æ‰‹è½åœ°è‡´å‹      1\n",
      "2          æ›ç¶²      1\n",
      "3         æœªéç¶²      1\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½å¤±èª¤ç’°ç¯€ï¼Œå‡ºç•Œæ¬¡æ•¸ç¨å¤šï¼Œéœ€è¦å¤šåŠ æ³¨æ„ã€‚å°æ‰‹é€²æ”»çŠ€åˆ©ï¼Œå¤šæ¬¡æ¡ç”¨è½åœ°è‡´å‹çš„ç­–ç•¥ï¼Œæˆ‘æ–¹éœ€æå‡é˜²å®ˆè³ªé‡ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:26:59,088 - INFO - ç¯€é» 20bd6840 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:26:59,091 - INFO - ç¯€é» 9908ae30 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   lose_reason  count\n",
      "0          å‡ºç•Œ      1\n",
      "1      å°æ‰‹è½åœ°è‡´å‹      1\n",
      "2          æ›ç¶²      1\n",
      "3         æœªéç¶²      1\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æˆ‘æ–¹å¤±èª¤è¼ƒå¤šï¼Œå°¤å…¶å‡ºç•Œæ¬¡æ•¸åé«˜ï¼Œéœ€åŠ å¼·æ§åˆ¶ã€‚å°æ‰‹é€²æ”»çŠ€åˆ©ï¼Œé »ç¹åˆ©ç”¨è½åœ°å¾—åˆ†ï¼Œå› æ­¤æˆ‘æ–¹éœ€æå‡é˜²å®ˆè³ªé‡ã€‚\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: å„ä½è§€çœ¾ï¼Œæœ¬å ´æ¯”è³½é›™æ–¹äº’æœ‰æ”»é˜²ï¼Œå¤±èª¤æ–¹é¢ã€Œå°æ‰‹è½åœ°è‡´å‹ã€èˆ‡ã€Œå‡ºç•Œã€æ¬¡æ•¸æœ€å¤šï¼Œéƒ½æ˜¯12æ¬¡ï¼Œå¯è¦‹é˜²å®ˆå’Œæ§çƒä»æœ‰åŠ å¼·ç©ºé–“ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:02,624 - INFO - ç¯€é» 7cad4196 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:27:02,626 - INFO - ç¯€é» b8dd5319 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: å„ä½è§€çœ¾ï¼Œæœ¬å ´æ¯”è³½é›™æ–¹äº’æœ‰æ”»é˜²ï¼Œæˆ‘æ–¹å¤±èª¤è¼ƒå¤šï¼Œå°¤å…¶å‡ºç•Œæ¬¡æ•¸åé«˜ï¼Œéœ€åŠ å¼·æ§åˆ¶ã€‚å°æ‰‹é€²æ”»çŠ€åˆ©ï¼Œé »ç¹åˆ©ç”¨è½åœ°å¾—åˆ†ï¼Œå› æ­¤æˆ‘æ–¹éœ€æå‡é˜²å®ˆè³ªé‡ã€‚å¤±èª¤æ–¹é¢ï¼Œã€Œå°æ‰‹è½åœ°è‡´å‹ã€èˆ‡ã€Œå‡ºç•Œã€æ¬¡æ•¸æœ€å¤šï¼Œéƒ½æ˜¯12æ¬¡ï¼Œå¯è¦‹é˜²å®ˆå’Œæ§çƒä»æœ‰åŠ å¼·ç©ºé–“ã€‚\n",
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: æ²’éŒ¯ï¼å°æ‰‹ä»Šå¤©åœ¨é€²æ”»ç«¯çµ¦è¶³äº†å£“åŠ›ï¼Œã€Œå°æ‰‹è½åœ°è‡´å‹ã€å°è‡´äº†12åˆ†ä¸Ÿå¤±ï¼ŒåŒæ™‚ä¹Ÿè¦ç•™æ„è‡ªèº«å¤±èª¤ï¼Œå‡ºç•Œå’Œæ›ç¶²åŒæ¨£é€ æˆäº†ä¸å°çš„æå¤±ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:05,247 - INFO - ç¯€é» a03b196e æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   lose_reason  count\n",
      "0      å°æ‰‹è½åœ°è‡´å‹     12\n",
      "1          å‡ºç•Œ     12\n",
      "2          æ›ç¶²     10\n",
      "3         æœªéç¶²      2\n",
      "ç¯€é»æ–‡æœ¬: å„ä½è§€çœ¾ï¼Œæœ¬å ´æ¯”è³½é›™æ–¹äº’æœ‰æ”»é˜²ï¼Œæˆ‘æ–¹å¤±èª¤è¼ƒå¤šï¼Œå°¤å…¶å‡ºç•Œæ¬¡æ•¸åé«˜ï¼Œéœ€åŠ å¼·æ§åˆ¶ã€‚å°æ‰‹é€²æ”»çŠ€åˆ©ï¼Œé »ç¹åˆ©ç”¨è½åœ°å¾—åˆ†ï¼Œæˆ‘æ–¹éœ€æå‡é˜²å®ˆè³ªé‡ã€‚ã€Œå°æ‰‹è½åœ°è‡´å‹ã€èˆ‡ã€Œå‡ºç•Œã€æ¬¡æ•¸æœ€å¤šï¼Œçš†ç‚º12æ¬¡ï¼Œé˜²å®ˆå’Œæ§çƒä»æœ‰åŠ å¼·ç©ºé–“ã€‚å¤±åˆ†é›†ä¸­åœ¨\"a\"åŸå› ï¼Œå…±4æ¬¡ï¼Œéœ€é‡é»æª¢è¨ã€‚æœªçŸ¥çƒå“¡è¡¨ç¾äº®çœ¼ï¼Œå¤šæ¬¡å¾—åˆ†ï¼Œå…¶æ‰“æ³•å’Œå„ªå‹¢å€¼å¾—åˆ†æã€‚æ­¤å¤–ï¼Œå‡ºç•Œå’Œæ›ç¶²ä¹Ÿé€ æˆä¸å°‘æå¤±ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:06,527 - INFO - ç¯€é» 4f0385c0 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:27:06,529 - INFO - ç¯€é» d6b2e02e æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node table:   getpoint_player  count\n",
      "0               A      1\n",
      "1               B      1\n",
      "node.table:   getpoint_player  count\n",
      "0               A      1\n",
      "1               B      1\n",
      "ç¯€é»æ–‡æœ¬: AéšŠå¾—åˆ†ä¸€æ¬¡ï¼ŒBéšŠä¹Ÿç·Šè¿½åœ¨å¾Œï¼Œå„å¾—ä¸€åˆ†ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ï¼\n",
      "node.table:   getpoint_player  count\n",
      "0               B     15\n",
      "1               A     21\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½Aé¸æ‰‹å¾—åˆ†æ¬¡æ•¸ç¨ä½”å„ªå‹¢ï¼Œå…±è¨ˆ21åˆ†ï¼ŒBé¸æ‰‹ç·Šéš¨å…¶å¾Œï¼Œæ‹¿ä¸‹15åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé›™æ–¹åœ¨å¾—åˆ†æ¨¡å¼ä¸Šä»æœ‰æå‡ç©ºé–“ï¼Œä»æœ‰éƒ¨åˆ†ã€Œç„¡è³‡æ–™ã€æƒ…æ³ï¼Œéœ€é€²ä¸€æ­¥åˆ†æå…·é«”åŸå› ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:07,885 - INFO - ç¯€é» 30009f71 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:27:07,886 - INFO - ç¯€é» 683fa451 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   getpoint_player  count\n",
      "0               B     15\n",
      "1               A     21\n",
      "ç¯€é»æ–‡æœ¬: Aé¸æ‰‹åœ¨æœ¬å ´æ¯”è³½ä¸­ä»¥21åˆ†ç•¥å‹Bé¸æ‰‹çš„15åˆ†ã€‚é›™æ–¹å¾—åˆ†æ¨¡å¼æœ‰å¾…åŠ å¼·ï¼Œéƒ¨åˆ†æ•¸æ“šç¼ºå¤±ï¼ˆâ€œç„¡è³‡æ–™â€ï¼‰ï¼Œéœ€é€²ä¸€æ­¥åˆ†æåŸå› ã€‚\n",
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: A éšŠä»Šå¤©åœ¨å ´ä¸Šå±•ç¾äº†å¼·å‹çš„é€²æ”»ç«åŠ›ï¼Œå¾—åˆ†é«˜é” 21 åˆ†ï¼Œé è¶… B éšŠçš„ 15 åˆ†ã€‚A éšŠå“¡åœ¨å¤šæ‹ç›¸æŒä¸­ï¼Œç¸½èƒ½æŠ“ä½æ©Ÿæœƒï¼Œä¸€èˆ‰å¾—åˆ†ï¼Œå¥ å®šäº†å‹å±€ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:09,800 - INFO - ç¯€é» a7746a65 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: AéšŠèˆ‡BéšŠæ¯”åˆ†ç·Šå’¬ï¼Œäº’ä¸ç›¸è®“ï¼Œå„å¾—ä¸€åˆ†ï¼AéšŠåœ¨æœ¬å ´æ¯”è³½ä¸­ä»¥21åˆ†ç•¥å‹BéšŠçš„15åˆ†ã€‚AéšŠä»Šå¤©åœ¨å ´ä¸Šå±•ç¾å¼·å‹é€²æ”»ç«åŠ›ï¼Œé è¶…BéšŠã€‚AéšŠå“¡åœ¨å¤šæ‹ç›¸æŒä¸­ç¸½èƒ½æŠ“ä½æ©Ÿæœƒå¾—åˆ†ï¼Œå¥ å®šå‹å±€ã€‚é›™æ–¹å¾—åˆ†æ¨¡å¼æœ‰å¾…åŠ å¼·ï¼Œéƒ¨åˆ†æ•¸æ“šç¼ºå¤±ï¼Œéœ€é€²ä¸€æ­¥åˆ†æåŸå› ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:11,932 - INFO - ç¯€é» 74b9da25 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-28 18:27:11,936 - INFO - ç”Ÿæˆæœ€çµ‚å ±å‘Š...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      Unnamed: 0  rally      time  roundscore_A  roundscore_B player  type  \\\n",
      "0             0      1  00:05:47             1             0      B   ç™¼é•·çƒ   \n",
      "1             1      1  00:05:49             1             0      A    åˆ‡çƒ   \n",
      "2             2      1  00:05:50             1             0      B    æŒ‘çƒ   \n",
      "3             3      1  00:05:51             1             0      A    é•·çƒ   \n",
      "4             4      1  00:05:52             1             0      B    æ®ºçƒ   \n",
      "..          ...    ...       ...           ...           ...    ...   ...   \n",
      "310         310     36  00:24:44            21            15      B  æœªçŸ¥çƒç¨®   \n",
      "311         311     36  00:24:58            21            15      A    åˆ‡çƒ   \n",
      "312         312     36  00:25:00            21            15      B    æŒ‘çƒ   \n",
      "313         313     36  00:25:01            21            15      A    é•·çƒ   \n",
      "314         314     36  00:25:02            21            15      B    é•·çƒ   \n",
      "\n",
      "    lose_reason getpoint_player  \n",
      "0           NaN             NaN  \n",
      "1           NaN             NaN  \n",
      "2           NaN             NaN  \n",
      "3           NaN             NaN  \n",
      "4           NaN             NaN  \n",
      "..          ...             ...  \n",
      "310         NaN             NaN  \n",
      "311         NaN             NaN  \n",
      "312         NaN             NaN  \n",
      "313         NaN             NaN  \n",
      "314          å‡ºç•Œ               A  \n",
      "\n",
      "[315 rows x 9 columns]\n",
      "ç¯€é»æ–‡æœ¬: è³‡æ–™åˆ†æå ±å‘Š\n",
      "\n",
      "æœ¬å ´æ¯”è³½æ•¸æ“šé¡¯ç¤ºï¼Œçƒå“¡å¤§é‡ä½¿ç”¨é•·çƒï¼ˆ55æ¬¡ï¼‰å’Œæ®ºçƒï¼ˆ36æ¬¡å¾—åˆ†ï¼‰ã€‚ç„¶è€Œï¼Œå‡ºç¾16æ¬¡ä¸æ˜åŸå› å¤±åˆ†å’Œ16æ¬¡æœªçŸ¥çƒç¨®ï¼Œéœ€é€²ä¸€æ­¥åˆ†ææ”¹é€²ã€‚é›™æ–¹äº’æœ‰æ”»é˜²ï¼Œæˆ‘æ–¹å¤±èª¤è¼ƒå¤šï¼Œå°¤å…¶å‡ºç•Œæ¬¡æ•¸ï¼ˆ12æ¬¡ï¼‰åé«˜ï¼Œéœ€åŠ å¼·æ§åˆ¶ã€‚å°æ‰‹é »ç¹åˆ©ç”¨è½åœ°å¾—åˆ†ï¼ˆ12æ¬¡ï¼‰ï¼Œæˆ‘æ–¹éœ€æå‡é˜²å®ˆè³ªé‡ã€‚å¤±åˆ†é›†ä¸­åœ¨\"a\"åŸå› ï¼ˆ4æ¬¡ï¼‰ï¼Œéœ€é‡é»æª¢è¨ã€‚æœªçŸ¥çƒå“¡è¡¨ç¾äº®çœ¼ï¼Œå¤šæ¬¡å¾—åˆ†ã€‚AéšŠä»¥21åˆ†ç•¥å‹BéšŠçš„15åˆ†ï¼Œå±•ç¾å¼·å‹é€²æ”»ç«åŠ›ï¼Œå¤šæ‹ç›¸æŒä¸­ç¸½èƒ½æŠ“ä½æ©Ÿæœƒå¾—åˆ†ã€‚é›™æ–¹å¾—åˆ†æ¨¡å¼æœ‰å¾…åŠ å¼·ï¼Œéƒ¨åˆ†æ•¸æ“šç¼ºå¤±ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:27:16,086 - INFO - æ¨¹çµæ§‹å·²å°å‡ºè‡³: tree_structure.json\n",
      "2025-09-28 18:27:16,089 - INFO - \n",
      "==================================================\n",
      "2025-09-28 18:27:16,090 - INFO - TREE-OF-REPORT æœ€çµ‚å ±å‘Š\n",
      "2025-09-28 18:27:16,090 - INFO - ==================================================\n",
      "2025-09-28 18:27:16,099 - INFO - å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: 167.86 ç§’\n",
      "2025-09-28 18:27:16,100 - INFO - ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "2025-09-28 18:27:16,100 - INFO - - tree_of_report.md: æœ€çµ‚å ±å‘Š\n",
      "2025-09-28 18:27:16,101 - INFO - - tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\n",
      "2025-09-28 18:27:16,102 - INFO - - tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\n",
      "2025-09-28 18:27:16,103 - INFO - - execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\n",
      "2025-09-28 18:27:16,103 - INFO - - tree_visualization.html: å¯è¦–åŒ–é é¢\n",
      "2025-09-28 18:27:16,106 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: input_tmp.csv\n",
      "2025-09-28 18:27:16,107 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: tmp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish generate report\n",
      "## å¼·æ”»åˆ¶å‹ï¼AéšŠéšªå‹BéšŠï¼Œé—œéµåœ¨æ–¼é•·çƒèˆ‡æ®ºçƒçš„ç²¾æº–é‹ç”¨\n",
      "\n",
      "ä¸€å ´æ¿€çƒˆçš„ç¾½çƒå°æ±ºæ˜¨æ—¥è½å¹•ï¼ŒAéšŠä»¥21æ¯”15çš„æ¯”åˆ†éšªå‹BéšŠã€‚ç¸±è§€å…¨å ´ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ï¼Œä½†AéšŠæ†‘è—‰æ›´å…·ä¾µç•¥æ€§çš„é€²æ”»ï¼Œæœ€çµ‚æˆåŠŸé–å®šå‹å±€ã€‚\n",
      "\n",
      "æœ¬å ´æ¯”è³½çš„ç„¦é»åœ¨æ–¼çƒå“¡å€‘å°é•·çƒå’Œæ®ºçƒçš„é »ç¹é‹ç”¨ã€‚çƒå“¡å€‘å¤šæ¬¡åˆ©ç”¨é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¼ºæ©Ÿä»¥é›·éœ†è¬éˆçš„æ®ºçƒç›´å–åˆ†æ•¸ï¼Œç¸½è¨ˆ36æ¬¡çš„æ®ºçƒå¾—åˆ†è­‰æ˜äº†é€™ä¸€æˆ°è¡“çš„æœ‰æ•ˆæ€§ã€‚AéšŠåœ¨å¤šæ‹ç›¸æŒä¸­å±•ç¾å‡ºæ›´å¼·çš„å¾—åˆ†èƒ½åŠ›ï¼Œå¾€å¾€èƒ½åœ¨é—œéµæ™‚åˆ»æŠ“ä½æ©Ÿæœƒï¼Œçµ¦äºˆå°æ‰‹è‡´å‘½ä¸€æ“Šã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œæ¯”è³½ä¸­ä¹Ÿæš´éœ²å‡ºä¸€äº›å•é¡Œã€‚é›™æ–¹çƒå“¡éƒ½å‡ºç¾äº†éå—è¿«æ€§å¤±èª¤ï¼Œå…¶ä¸­å‡ºç•Œæ¬¡æ•¸åé«˜ï¼Œé¡¯ç¤ºçƒå“¡åœ¨æ§çƒæ–¹é¢ä»æœ‰æå‡ç©ºé–“ã€‚æ­¤å¤–ï¼Œä¸€äº›å¤±åˆ†åŸå› ä¸æ˜ï¼Œä»¥åŠæœªçŸ¥çƒç¨®çš„å‡ºç¾ï¼Œéƒ½æé†’æ•™ç·´åœ˜éšŠéœ€è¦æ·±å…¥åˆ†ææ¯”è³½éŒ„å½±ï¼Œæ‰¾å‡ºæ½›åœ¨çš„æŠ€è¡“æ¼æ´ã€‚\n",
      "\n",
      "BéšŠæ–¹é¢ï¼Œé›–ç„¶åœ¨é˜²å®ˆç«¯åšå‡ºäº†åŠªåŠ›ï¼Œä½†ä»é›£ä»¥æŠµæ“‹AéšŠå¦‚æ½®æ°´èˆ¬çš„æ”»å‹¢ã€‚å°æ‰‹é »ç¹åˆ©ç”¨è½åœ°å¾—åˆ†ï¼Œä½¿å¾—BéšŠåœ¨é˜²å®ˆä¸Šç–²æ–¼å¥”å‘½ã€‚æ›´ä»¤äººæ“”æ†‚çš„æ˜¯ï¼ŒBéšŠåœ¨â€œaâ€åŸå› ä¸Šå¤±åˆ†è¼ƒå¤šï¼Œé€™å°‡æ˜¯ä»–å€‘æœªä¾†è¨“ç·´ä¸­éœ€è¦é‡é»æª¢è¨çš„ç’°ç¯€ã€‚\n",
      "\n",
      "å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæœ‰\"æœªçŸ¥çƒå“¡\"åœ¨æœ¬å ´æ¯”è³½ä¸­è¡¨ç¾äº®çœ¼ï¼Œå¤šæ¬¡å¾—åˆ†ï¼Œç‚ºæ¯”è³½å¢æ·»äº†ä¸å°‘çœ‹é»ã€‚\n",
      "\n",
      "ç¸½é«”è€Œè¨€ï¼ŒAéšŠæ†‘è—‰æ›´å¼·å¤§çš„é€²æ”»ç«åŠ›ï¼Œç‰¹åˆ¥æ˜¯é•·çƒå’Œæ®ºçƒçš„æœ‰æ•ˆé…åˆï¼Œè´å¾—äº†é€™å ´æ¯”è³½çš„å‹åˆ©ã€‚ç„¶è€Œï¼Œé›™æ–¹åœ¨å¾—åˆ†æ¨¡å¼ä¸Šéƒ½æœ‰å¾…åŠ å¼·ï¼Œä¸¦éœ€é‡å°å„è‡ªçš„å¼±é»é€²è¡Œæ”¹é€²ï¼ŒæœŸå¾…ä»–å€‘åœ¨æœªä¾†çš„æ¯”è³½ä¸­å¸¶ä¾†æ›´ç²¾å½©çš„è¡¨ç¾ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import builtins\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„å‡½æ•¸ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # è¿”å›é»˜èªæ“ä½œé›†åˆ\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# ===== æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ =====\n",
    "class TreeNode:\n",
    "    \"\"\"æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ï¼Œå¢åŠ èªæ„é©—è­‰å’Œè¿½è¹¤åŠŸèƒ½\"\"\"\n",
    "    def __init__(self, level: int = 0, text: str = \"\", table: pd.DataFrame = None, operation: str = None):\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.level: int = level\n",
    "        self.text: str = text\n",
    "        self.table: pd.DataFrame = table if table is not None else pd.DataFrame()\n",
    "        self.operation: str = operation\n",
    "        self.parent: Optional['TreeNode'] = None\n",
    "        self.operation_history: List[str] = []\n",
    "        \n",
    "        # æ–°å¢å±¬æ€§ç”¨æ–¼æ”¹é€²åŠŸèƒ½\n",
    "        self.node_id: str = self._generate_node_id()\n",
    "        self.created_at: datetime = datetime.now()\n",
    "        self.validation_errors: List[str] = []\n",
    "        self.table_hash: str = self._calculate_table_hash()\n",
    "        self.semantic_score: float = 0.0\n",
    "        \n",
    "    def _generate_node_id(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆå”¯ä¸€ç¯€é»ID\"\"\"\n",
    "        content = f\"{self.level}_{self.operation}_{datetime.now().isoformat()}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        \n",
    "    def _calculate_table_hash(self) -> str:\n",
    "        \"\"\"è¨ˆç®—è¡¨æ ¼å…§å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨æ–¼æª¢æ¸¬é‡è¤‡\"\"\"\n",
    "        if self.table.empty:\n",
    "            return \"\"\n",
    "        try:\n",
    "            return hashlib.md5(str(self.table.values.tobytes()).encode()).hexdigest()[:8]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def add_child(self, child: 'TreeNode'):\n",
    "        \"\"\"æ·»åŠ å­ç¯€é»ä¸¦é€²è¡Œé©—è­‰\"\"\"\n",
    "        if self._validate_child(child):\n",
    "            child.parent = self\n",
    "            self.children.append(child)\n",
    "            logger.info(f\"æ·»åŠ å­ç¯€é»: {child.node_id} to {self.node_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"å­ç¯€é»é©—è­‰å¤±æ•—: {child.validation_errors}\")\n",
    "    \n",
    "    def _validate_child(self, child: 'TreeNode') -> bool:\n",
    "        \"\"\"é©—è­‰å­ç¯€é»çš„åˆç†æ€§\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„è¡¨æ ¼ç‹€æ…‹\n",
    "        if child.table_hash and child.table_hash == self.table_hash:\n",
    "            if not child.operation.lower().startswith('write'):\n",
    "                errors.append(\"è¡¨æ ¼å…§å®¹æœªç™¼ç”Ÿè®ŠåŒ–ä½†éå¯«ä½œæ“ä½œ\")\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œæ˜¯å¦é‚è¼¯åˆç†\n",
    "        if self._is_redundant_operation(child.operation):\n",
    "            errors.append(f\"æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: {child.operation}\")\n",
    "        \n",
    "        child.validation_errors = errors\n",
    "        return len(errors) == 0\n",
    "    \n",
    "    def _is_redundant_operation(self, operation: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æ“ä½œæ˜¯å¦å†—é¤˜\"\"\"\n",
    "        if len(self.operation_history) < 2:\n",
    "            return False\n",
    "            \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ“ä½œåœ¨è¿‘æœŸæ­·å²ä¸­\n",
    "        recent_ops = self.operation_history[-3:]  # æª¢æŸ¥æœ€è¿‘3å€‹æ“ä½œ\n",
    "        op_name = operation.split('(')[0].lower()\n",
    "        \n",
    "        for hist_op in recent_ops:\n",
    "            if hist_op.split('(')[0].lower() == op_name:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºè‘‰ç¯€é»\"\"\"\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"å°‡ç¯€é»è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self.node_id,\n",
    "            \"level\": self.level,\n",
    "            \"operation\": self.operation,\n",
    "            \"text_preview\": self.text[:100] + \"...\" if len(self.text) > 100 else self.text,\n",
    "            \"table_shape\": list(self.table.shape) if not self.table.empty else [0, 0],\n",
    "            \"table_columns\": list(self.table.columns) if not self.table.empty else [],\n",
    "            \"children_count\": len(self.children),\n",
    "            \"validation_errors\": self.validation_errors,\n",
    "            \"semantic_score\": self.semantic_score,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"table_hash\": self.table_hash\n",
    "        }\n",
    "\n",
    "# ===== æ”¹é€²çš„æ“ä½œè§£æå™¨ =====\n",
    "class OperationParser:\n",
    "    \"\"\"å°ˆé–€è² è²¬è§£æå’Œé©—è­‰æ“ä½œçš„é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.valid_operations = {\n",
    "            'select_column', 'select_row',  'sort', 'calculate',\n",
    "            'group_by', 'value_counts', 'aggregate', 'crosstab','pivot_table', 'write'\n",
    "        }\n",
    "        \n",
    "    def parse_operations(self, response_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"æ”¹é€²çš„æ“ä½œè§£æï¼Œè¿”å›çµæ§‹åŒ–çµæœ\"\"\"\n",
    "        try:\n",
    "            parsed_operations = []\n",
    "            \n",
    "            # å¤šç¨®è§£æç­–ç•¥\n",
    "            operations = self._extract_operations_multiple_strategies(response_text)\n",
    "            \n",
    "            for op_str in operations:\n",
    "                parsed_op = self._parse_single_operation(op_str)\n",
    "                if parsed_op and self._validate_operation(parsed_op):\n",
    "                    parsed_operations.append(parsed_op)\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: {op_str}\")\n",
    "            \n",
    "            return parsed_operations[:5]  # é™åˆ¶æœ€å¤š5å€‹æ“ä½œ\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œå¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_operations_multiple_strategies(self, text: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨å¤šç¨®ç­–ç•¥æå–æ“ä½œ\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç­–ç•¥1: å°‹æ‰¾æ–¹æ‹¬è™Ÿå…§å®¹\n",
    "        bracket_match = re.search(r'\\[(.*?)\\]', text, re.DOTALL)\n",
    "        if bracket_match:\n",
    "            content = bracket_match.group(1)\n",
    "            # ä½¿ç”¨æ­£å‰‡æå–å‡½æ•¸èª¿ç”¨æ ¼å¼\n",
    "            pattern = r'([a-zA-Z_]+\\([^)]*\\))'\n",
    "            ops = re.findall(pattern, content)\n",
    "            operations.extend(ops)\n",
    "        \n",
    "        # ç­–ç•¥2: é€è¡Œè§£æ\n",
    "        if not operations:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '(' in line and ')' in line:\n",
    "                    operations.append(line)\n",
    "        \n",
    "        # ç­–ç•¥3: é€—è™Ÿåˆ†å‰²\n",
    "        if not operations:\n",
    "            parts = text.replace('[', '').replace(']', '').split(',')\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part and '(' in part:\n",
    "                    operations.append(part)\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def _parse_single_operation(self, op_str: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"è§£æå–®å€‹æ“ä½œå­—ç¬¦ä¸²\"\"\"\n",
    "        try:\n",
    "            # ç§»é™¤å¤šé¤˜çš„å­—ç¬¦\n",
    "            op_str = op_str.strip().rstrip(',').strip()\n",
    "            \n",
    "            # æå–æ“ä½œåç¨±å’Œåƒæ•¸\n",
    "            if '(' not in op_str:\n",
    "                return {\"name\": op_str, \"args\": [], \"raw\": op_str}\n",
    "            \n",
    "            name_part = op_str.split('(')[0].strip()\n",
    "            args_part = op_str[op_str.find('(')+1:op_str.rfind(')')].strip()\n",
    "            \n",
    "            # è§£æåƒæ•¸\n",
    "            args = []\n",
    "            if args_part:\n",
    "                # ç°¡å–®çš„åƒæ•¸åˆ†å‰²ï¼ˆå¯ä»¥é€²ä¸€æ­¥æ”¹é€²ï¼‰\n",
    "                for arg in args_part.split(','):\n",
    "                    arg = arg.strip().strip('\\'\"')\n",
    "                    if arg:\n",
    "                        args.append(arg)\n",
    "            \n",
    "            return {\n",
    "                \"name\": name_part.lower(),\n",
    "                \"args\": args,\n",
    "                \"raw\": op_str\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œ '{op_str}' å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_operation(self, operation: Dict[str, Any]) -> bool:\n",
    "        \"\"\"é©—è­‰æ“ä½œçš„æœ‰æ•ˆæ€§\"\"\"\n",
    "        name = operation.get(\"name\", \"\").lower()\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "        if name not in self.valid_operations:\n",
    "            logger.warning(f\"æœªçŸ¥æ“ä½œ: {name}\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥ç‰¹å®šæ“ä½œçš„åƒæ•¸\n",
    "        args = operation.get(\"args\", [])\n",
    "        \n",
    "        if name in ['select_column', 'sort_values'] and not args:\n",
    "            logger.warning(f\"{name} æ“ä½œéœ€è¦åƒæ•¸\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ===== æ”¹é€²çš„å…§å®¹è¦åŠƒå™¨ =====\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.parser = OperationParser()\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        æ”¹é€²çš„æ“ä½œç”Ÿæˆï¼ŒåŠ å…¥é‡è¤‡æª¢æ¸¬å’Œèªæ„é©—è­‰\n",
    "        \"\"\"\n",
    "        \n",
    "        # æª¢æ¸¬è¿‘æœŸæ“ä½œï¼Œé¿å…é‡è¤‡\n",
    "        recent_operations = self._extract_recent_operations(operation_history)\n",
    "        \n",
    "        # æ§‹å»ºæ”¹é€²çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}  and more than zero.\n",
    "6. Only select Opertions from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "13. AVOID repeating recent operations: {recent_operations}\n",
    "14. Prioritize operations that will meaningfully transform the data.\n",
    "15. Avoid give the arguments that not match by the operation.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path) if os.path.exists(outline_path) else \"Generate comprehensive data analysis\"}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                logger.info(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                parsed_ops = self.parser.parse_operations(response.text.strip())\n",
    "                return [op[\"raw\"] for op in parsed_ops]  # è¿”å›åŸå§‹å­—ç¬¦ä¸²æ ¼å¼\n",
    "            else:\n",
    "                logger.warning(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_recent_operations(self, operation_history: List[str]) -> List[str]:\n",
    "        \"\"\"æå–æœ€è¿‘çš„æ“ä½œåç¨±\"\"\"\n",
    "        recent = []\n",
    "        for op in operation_history[-3:]:  # æœ€è¿‘3å€‹æ“ä½œ\n",
    "            if '(' in op:\n",
    "                name = op.split('(')[0].strip()\n",
    "                recent.append(name)\n",
    "        return recent\n",
    "\n",
    "# ===== å®‰å…¨çš„DataFrameæ“ä½œå™¨ =====\n",
    "class SafeDataFrameOperator:\n",
    "    \"\"\"å®‰å…¨çš„DataFrameæ“ä½œå™¨ï¼Œä½¿ç”¨ASTé©—è­‰è€Œéç›´æ¥exec\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.allowed_modules = {'pandas', 'numpy', 're'}\n",
    "        self.allowed_functions = {\n",
    "            'pd.read_csv', 'pd.DataFrame', 'df.head', 'df.tail', 'df.sort_values',\n",
    "            'df.groupby', 'df.filter', 'df.select', 'df.drop', 'df.fillna',\n",
    "            'df.to_csv', 'df.value_counts', 'df.describe', 'df.info'\n",
    "        }\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path=\"input_tmp.csv\"):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        1. è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'\n",
    "        2. åªä½¿ç”¨pandasåŸºæœ¬æ“ä½œï¼Œé¿å…è¤‡é›œçš„è‡ªå®šç¾©å‡½æ•¸\n",
    "        3. ç¢ºä¿ä»£ç¢¼å®‰å…¨ï¼Œä¸åŒ…å«æ–‡ä»¶ç³»çµ±æ“ä½œï¼ˆé™¤äº†æŒ‡å®šçš„CSVè®€å¯«ï¼‰\n",
    "        4. æ’°å¯«å®Œæ•´python codeï¼ŒåŒ…å«éŒ¯èª¤è™•ç†\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=2):\n",
    "        \"\"\"å¸¶é‡è©¦çš„ç”Ÿæˆè«‹æ±‚\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    import time\n",
    "                    time.sleep(1)\n",
    "        return \"\"\n",
    "\n",
    "    def safe_execute(self, code: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"å®‰å…¨åŸ·è¡Œç”Ÿæˆçš„ä»£ç¢¼\"\"\"\n",
    "        try:\n",
    "            # æå–ä»£ç¢¼å¡Š\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            #print(f'python code: {code_block}')\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # ASTå®‰å…¨é©—è­‰\n",
    "            if not self._validate_code_safety(code):\n",
    "                logger.error(\"ä»£ç¢¼å®‰å…¨é©—è­‰å¤±æ•—\")\n",
    "                return df\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆ\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            allowed_builtin_names = [\n",
    "                'int', 'float', 'str', 'bool', 'list', 'dict', 'set', 'tuple',\n",
    "                'len', 'range', 'enumerate', 'zip', 'min', 'max', 'sum', 'abs',\n",
    "                'print',\n",
    "                'Exception', 'TypeError', 'ValueError', 'KeyError', 'IndexError',\n",
    "                'FileNotFoundError', 'ZeroDivisionError', 'AttributeError', 'ImportError',\n",
    "                '__import__'\n",
    "            ]\n",
    "\n",
    "            safe_globals = {\n",
    "                'pd': pd,\n",
    "                '__name__': '__main__',\n",
    "                '__builtins__': {name: getattr(builtins, name) for name in allowed_builtin_names}\n",
    "            }\n",
    "\n",
    "            safe_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œä»£ç¢¼\n",
    "            exec(code, safe_globals, safe_locals)\n",
    "\n",
    "            # è®€å–çµæœ\n",
    "            if os.path.exists(\"tmp.csv\"):\n",
    "                result_df = pd.read_csv(\"tmp.csv\")\n",
    "                logger.info(f\"æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                return result_df\n",
    "            else:\n",
    "                logger.warning(\"æœªç”Ÿæˆçµæœæ–‡ä»¶ï¼Œè¿”å›åŸå§‹DataFrame\")\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            print(\"éŒ¯èª¤ä»£ç¢¼å¦‚ä¸‹ï¼š\\n\" + \"-\" * 30)\n",
    "            print(code)  # âœ… è¼¸å‡ºé€ æˆéŒ¯èª¤çš„ç¨‹å¼ç¢¼\n",
    "            print(\"-\" * 30)\n",
    "            logger.error(error_msg)\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_code_safety(self, code: str) -> bool:\n",
    "        \"\"\"ä½¿ç”¨ASTé©—è­‰ä»£ç¢¼å®‰å…¨æ€§\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # æª¢æŸ¥å±éšªçš„å‡½æ•¸èª¿ç”¨\n",
    "                if isinstance(node, ast.Call):\n",
    "                    if isinstance(node.func, ast.Name):\n",
    "                        func_name = node.func.id\n",
    "                        if func_name in ['exec', 'eval', 'compile', '__import__', 'open']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°å±éšªå‡½æ•¸: {func_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥æ–‡ä»¶æ“ä½œï¼ˆé™¤äº†å…è¨±çš„CSVæ“ä½œï¼‰\n",
    "                if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
    "                    if hasattr(node.func, 'attr'):\n",
    "                        attr_name = node.func.attr\n",
    "                        if attr_name in ['system', 'popen', 'subprocess']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ç³»çµ±èª¿ç”¨: {attr_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥å°å…¥èªå¥\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for alias in node.names:\n",
    "                        if alias.name not in self.allowed_modules:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ä¸å…è¨±çš„æ¨¡çµ„å°å…¥: {alias.name}\")\n",
    "                            return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ASTé©—è­‰å¤±æ•—: {e}\")\n",
    "            return False\n",
    "\n",
    "# ===== æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "import time\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "\n",
    "    def extract_highlights_from_table(self, table: pd.DataFrame) -> str:\n",
    "        try:\n",
    "            if 'lose_reason' in table.columns:\n",
    "                top_reason = table['lose_reason'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_reason = \"ç„¡è³‡æ–™\"\n",
    "            if 'getpoint_player' in table.columns:\n",
    "                top_player = table['getpoint_player'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_player = \"æœªçŸ¥çƒå“¡\"\n",
    "            return f\"æœ€å¤šå¤±åˆ†åŸå› ç‚ºã€Œ{top_reason}ã€ï¼Œå¾—åˆ†æœ€å¤šçš„æ˜¯ {top_player}ã€‚\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_table_features(self, table: pd.DataFrame) -> str:\n",
    "        summary = []\n",
    "        for col in table.columns:\n",
    "            dtype = str(table[col].dtype)\n",
    "            line = f\"æ¬„ä½ã€Œ{col}ã€é¡å‹ï¼š{dtype}\"\n",
    "\n",
    "            # é¡¯ç¤ºå¸¸è¦‹å€¼åƒ…é™é¡åˆ¥å‹æ¬„ä½\n",
    "            if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
    "                top_values = table[col].value_counts().head(3).to_dict()\n",
    "                line += f\"ï¼Œå¸¸è¦‹å€¼ï¼š{list(top_values.keys())}\"\n",
    "            summary.append(line)\n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=3, delay_seconds=30):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                logger.error(f\"Gemini å›æ‡‰å¤±æ•—: {err}\")\n",
    "                if \"429\" in err:\n",
    "                    logger.info(f\"å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… {delay_seconds} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                else:\n",
    "                    break\n",
    "        return \"âš ï¸ å¯«ä½œè«‹æ±‚å¤±æ•—ï¼šAPI é™åˆ¶æˆ–å…¶ä»–éŒ¯èª¤\"\n",
    "\n",
    "    def generate_text_for_write_operation(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        table_str = table.to_string()\n",
    "        WRITE_TOKENS = 50\n",
    "        TABLE_FORMAT = \"Pandas DataFrame as plain text\"\n",
    "        highlight_summary = self.extract_highlights_from_table(table)\n",
    "        table_feature_summary = self.extract_table_features(table)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a professional content writer for the badminton game report .\n",
    "Please write the Report based on the input Table, just pick one or two lightspots.\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The Table format is {TABLE_FORMAT}.\n",
    "5. The Report can only describe the content included in the Tables and cannot describe anything not included in the Tables .\n",
    "6. The Report must consist of only one paragraph .\n",
    "7. The number of tokens in the Report must be within {WRITE_TOKENS}.\n",
    "8. è«‹å°ˆæ³¨æè¿°å¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼ã€é—œéµæ¬„ä½è¶¨å‹¢æˆ–çƒå“¡äº®é»ã€‚\n",
    "9. è«‹æ¨¡ä»¿æ¯”è³½è½‰æ’­å“¡æˆ–æ•™ç·´çš„èªæ°£æè¿°ï¼Œå¥å¼è‡ªç„¶ã€æœ‰ç¯€å¥æ„Ÿã€‚\n",
    "10. è«‹ç‰¹åˆ¥è§€å¯Ÿçƒç¨®ä¹‹é–“çš„é€£çºŒè½‰æ›ï¼Œä¾‹å¦‚ æ”¾å°çƒ æ¥ æ®ºçƒ ç­‰ï¼Œæ‰¾å‡ºå…¶ä¸­æœ‰æ•ˆå¾—åˆ†æˆ–ä¸å°‹å¸¸çš„çµ„åˆä¸¦æè¿°ã€‚\n",
    "\n",
    "# Highlights Summary\n",
    "{highlight_summary}\n",
    "\n",
    "# Table Features\n",
    "{table_feature_summary}\n",
    "\n",
    "# Table Description\n",
    "{self.table_description}\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Tables\n",
    "{table_str}\n",
    "## Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def merge_child_texts(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        GENERATING_TOKENS = 100\n",
    "        reports_str = \"\\n\".join([f\"- {txt}\" for txt in child_texts])\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a content generator for the badminton game report .\n",
    "Please merge and rewrite a New Report based on the input Reports .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The New Report must include all the content from the input Reports ; do not omit any information .\n",
    "5. The New Report must follow the order of the input Reports .\n",
    "6. The number of tokens in the New Report must be within {GENERATING_TOKENS}.\n",
    "7. è«‹ä¾åºæ•´åˆæ¯æ®µå…§å®¹ï¼Œå½¢æˆçµæ§‹æ¸…æ™°çš„æ®µè½ï¼ŒåŒ…æ‹¬äº®é»ã€å¤±èª¤æ¨¡å¼èˆ‡çƒå“¡è²¢ç»ã€‚\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Reports\n",
    "{reports_str}\n",
    "## New Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "# ===== OperationParser._validate_operation å¼·åŒ–åƒæ•¸é©—è­‰ï¼ˆè£œå…¥ df æ¬„ä½æ¯”å°ï¼‰ =====\n",
    "def validate_operation_with_columns(operation: Dict[str, Any], df_columns: List[str]) -> bool:\n",
    "    name = operation.get(\"name\", \"\").lower()\n",
    "    args = operation.get(\"args\", [])\n",
    "\n",
    "    # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "    if name not in {\n",
    "        'select_column', 'select_row', 'sort', 'calculate',\n",
    "        'group_by', 'value_counts', 'aggregate', 'crosstab', 'pivot_table', 'write'\n",
    "    }:\n",
    "        return False\n",
    "\n",
    "    # åƒ…é‡å°éœ€åƒæ•¸æ“ä½œæª¢æŸ¥æ¬„ä½\n",
    "    if name in ['select_column', 'sort', 'group_by']:\n",
    "        for arg in args:\n",
    "            if arg not in df_columns:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# ===== æ”¹é€²çš„TreeOfReporté¡åˆ¥ =====\n",
    "class TreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # åˆå§‹åŒ–æ”¹é€²çš„çµ„ä»¶\n",
    "        self.content_planner = ContentPlanner(api_key)\n",
    "        self.df_operator = SafeDataFrameOperator(api_key)  # ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬\n",
    "        self.text_generator = TextGenerator(api_key, table_description=self.table_description)\n",
    "        \n",
    "        # æ–°å¢è¿½è¹¤åŠŸèƒ½\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        self.node_registry: Dict[str, TreeNode] = {}\n",
    "\n",
    "    def load_configurations(self):\n",
    "        self.table_description = read_text_file(\"filtered_data _description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å„ç¨®æ¬„ä½ç”¨æ–¼åˆ†æ\"\n",
    "\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        if isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['name'] for op in self.operation_description]\n",
    "        else:\n",
    "            self.operation_pool = list(self.operation_description.keys())\n",
    "\n",
    "        logger.info(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "    def build_tree(self, root_table: pd.DataFrame) -> TreeNode:\n",
    "        \"\"\"æ”¹é€²çš„æ¨¹æ§‹å»ºï¼ŒåŠ å…¥å®Œæ•´çš„è¿½è¹¤å’Œé©—è­‰\"\"\"\n",
    "        root = TreeNode(level=0, text=\"è³‡æ–™åˆ†æå ±å‘Š\", table=root_table, operation=\"root(None)\")\n",
    "        root.operation_history = ['root(None)']\n",
    "        self.node_registry[root.node_id] = root\n",
    "        \n",
    "        queue = [root]\n",
    "        \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            \n",
    "            # è¨˜éŒ„è™•ç†æ—¥èªŒ\n",
    "            self._log_node_processing(current_node)\n",
    "\n",
    "            if current_node.operation.lower().startswith('write'):\n",
    "                continue\n",
    "\n",
    "            if current_node.level >= self.max_depth:\n",
    "                write_node = self.create_child_node(current_node, 'write()')\n",
    "                if write_node:\n",
    "                    current_node.add_child(write_node)\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"è™•ç†ç¯€é» - Level: {current_node.level}, Operation: {current_node.operation}\")\n",
    "\n",
    "            tables_str = current_node.table.to_string()\n",
    "            operations = self.content_planner.generate_operations(\n",
    "                tables=tables_str,\n",
    "                table_description=self.table_description,\n",
    "                operation_description=self.operation_description,\n",
    "                operation_history=current_node.operation_history,\n",
    "                operation_pool=self.operation_pool,\n",
    "                max_depth=self.max_depth,\n",
    "                max_degree=self.max_degree\n",
    "            )\n",
    "\n",
    "            logger.info(f\"ç”Ÿæˆæ“ä½œ: {operations}\")\n",
    "\n",
    "            for operation in operations[:self.max_degree]:\n",
    "                if operation.strip():\n",
    "                    child_node = self.create_child_node(current_node, operation)\n",
    "                    if child_node:\n",
    "                        current_node.add_child(child_node)\n",
    "                        queue.append(child_node)\n",
    "\n",
    "        self.generate_all_texts(root)\n",
    "        return root\n",
    "    \n",
    "    def _log_node_processing(self, node: TreeNode):\n",
    "        \"\"\"è¨˜éŒ„ç¯€é»è™•ç†æ—¥èªŒ\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"node_id\": node.node_id,\n",
    "            \"level\": node.level,\n",
    "            \"operation\": node.operation,\n",
    "            \"table_shape\": list(node.table.shape) if not node.table.empty else [0, 0],\n",
    "            \"validation_errors\": node.validation_errors\n",
    "        }\n",
    "        self.execution_log.append(log_entry)\n",
    "    \n",
    "    def create_child_node(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"æ”¹é€²çš„å­ç¯€é»å‰µå»ºï¼ŒåŠ å…¥å®Œæ•´é©—è­‰\"\"\"\n",
    "        try:\n",
    "            # å»ºç«‹æ–°çš„æ“ä½œæ­·å²\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚º write æ“ä½œ\n",
    "            if operation.lower().startswith('write'):\n",
    "                text = self.text_generator.generate_text_for_write_operation(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                self.node_registry[child.node_id] = child\n",
    "                logger.info(f\"å‰µå»º write ç¯€é»: {operation}\")\n",
    "                return child\n",
    "            else:\n",
    "                # å…¶ä»–æ“ä½œï¼šåŸ·è¡Œæ•¸æ“šæ“ä½œ\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    self.node_registry[child.node_id] = child\n",
    "                    logger.info(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_texts(self, node: TreeNode):\n",
    "        \"\"\"éæ­¸ç”Ÿæˆæ‰€æœ‰ç¯€é»çš„æ–‡æœ¬\"\"\"\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts(child)\n",
    "        \n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_text_for_write_operation(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "            print(f'node table: {node.table}')\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        logger.info(f'ç¯€é» {node.node_id} æ–‡æœ¬ç”Ÿæˆå®Œæˆ')\n",
    "        print(f'node.table: {node.table}')\n",
    "        print(f'ç¯€é»æ–‡æœ¬: {node.text}')\n",
    "        \n",
    "    def export_tree_structure(self, root: TreeNode, output_path: str = \"tree_structure.json\"):\n",
    "        \"\"\"å°å‡ºæ¨¹çµæ§‹ç‚ºJSONæ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–å’Œåˆ†æ\"\"\"\n",
    "        def node_to_dict(node: TreeNode) -> Dict[str, Any]:\n",
    "            result = node.to_dict()\n",
    "            result[\"children\"] = [node_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_time\": datetime.now().isoformat(),\n",
    "                \"total_nodes\": len(self.node_registry),\n",
    "                \"max_depth\": self.max_depth,\n",
    "                \"max_degree\": self.max_degree\n",
    "            },\n",
    "            \"execution_log\": self.execution_log,\n",
    "            \"tree\": node_to_dict(root)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(tree_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"æ¨¹çµæ§‹å·²å°å‡ºè‡³: {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å°å‡ºæ¨¹çµæ§‹å¤±æ•—: {e}\")\n",
    "    \n",
    "    def generate_execution_report(self) -> str:\n",
    "        \"\"\"ç”ŸæˆåŸ·è¡Œéç¨‹å ±å‘Š\"\"\"\n",
    "        total_nodes = len(self.node_registry)\n",
    "        error_nodes = sum(1 for node in self.node_registry.values() if node.validation_errors)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Tree-of-Report åŸ·è¡Œå ±å‘Š\n",
    "\n",
    "## çµ±è¨ˆä¿¡æ¯\n",
    "- ç¸½ç¯€é»æ•¸: {total_nodes}\n",
    "- éŒ¯èª¤ç¯€é»æ•¸: {error_nodes}\n",
    "- æ¨¹æœ€å¤§æ·±åº¦: {self.max_depth}\n",
    "- æœ€å¤§åˆ†æ”¯åº¦: {self.max_degree}\n",
    "\n",
    "## ç¯€é»åˆ†å¸ƒ\n",
    "\"\"\"\n",
    "        \n",
    "        # æŒ‰å±¤ç´šçµ±è¨ˆç¯€é»\n",
    "        level_counts = {}\n",
    "        for node in self.node_registry.values():\n",
    "            level = node.level\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "        \n",
    "        for level, count in sorted(level_counts.items()):\n",
    "            report += f\"- Level {level}: {count} å€‹ç¯€é»\\n\"\n",
    "        \n",
    "        # éŒ¯èª¤æ‘˜è¦\n",
    "        if error_nodes > 0:\n",
    "            report += \"\\n## é©—è­‰éŒ¯èª¤æ‘˜è¦\\n\"\n",
    "            for node in self.node_registry.values():\n",
    "                if node.validation_errors:\n",
    "                    report += f\"- ç¯€é» {node.node_id} ({node.operation}): {'; '.join(node.validation_errors)}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def generate_report(self, node: TreeNode, level: int = 0) -> str:\n",
    "        \"\"\"æ”¹é€²çš„å ±å‘Šç”Ÿæˆ\"\"\"\n",
    "        if node.level == 0:\n",
    "            prompt = f\"\"\"\n",
    "            ä½ æ˜¯ä¸€ä½æ–°èè¨˜è€…ï¼Œæ ¹æ“šä»¥ä¸‹åˆ†æç¸½çµï¼Œè«‹æ’°å¯«ä¸€ç¯‡è³½äº‹æ–°èå ±å°ï¼Œæä¾›å…¨é¢æ·±å…¥çš„åˆ†æï¼Œçµ±æ•´æˆæ–°èå ±å°ï¼Œæ–‡è¾­ä¸­éå¤šç›´æ¥ä½¿ç”¨æ¬„ä½åç¨±èˆ‡ç›´æ¥æ¬¡æ•¸çµ±è¨ˆï¼Œç”¨player_Aèˆ‡player_Bè¡¨ç¤ºå…©çƒå“¡ï¼Œç”¨ç”Ÿå‹•çš„æ–‡å¥æè¿°ï¼Œå‹¿å‡ºç¾ç´¯è´…çš„å¥å­ï¼Œè«‹å¾åˆ†æç¸½çµä¸­æå–è½‰æ›ï¼Œç¦æ­¢å‡ºç¾å¹»è¦ºã€‚\n",
    "            è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ï¼Œè³‡è¨Šæº–ç¢ºã€‚\n",
    "\n",
    "            åˆ†æç¸½çµ:\n",
    "            {node.text}\n",
    "            \"\"\"\n",
    "            final_text = self.text_generator._retry_generate(prompt)\n",
    "            \n",
    "            # ä¿å­˜å¤šç¨®æ ¼å¼çš„å ±å‘Š\n",
    "            with open(\"tree_of_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            \n",
    "            # å°å‡ºæ¨¹çµæ§‹\n",
    "            self.export_tree_structure(node)\n",
    "            \n",
    "            # ç”ŸæˆåŸ·è¡Œå ±å‘Š\n",
    "            exec_report = self.generate_execution_report()\n",
    "            with open(\"execution_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(exec_report)\n",
    "                print(\"finish generate report\")\n",
    "            \n",
    "            return final_text\n",
    "        else:\n",
    "            logger.info(f'generate report from not root')\n",
    "            indent = \"  \" * level\n",
    "            report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "            if node.text:\n",
    "                report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "            if node.table is not None and not node.table.empty and level < 2:\n",
    "                report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "                if len(node.table) <= 10:\n",
    "                    report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "                else:\n",
    "                    report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "            for child in node.children:\n",
    "                report += self.generate_report(child, level + 1)\n",
    "\n",
    "            return report\n",
    "\n",
    "\n",
    "# ===== ä¸»ç¨‹åº =====\n",
    "def main():\n",
    "    \"\"\"æ”¹é€²çš„ä¸»å‡½æ•¸\"\"\"\n",
    "    \n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    if not api_key:\n",
    "        logger.error(\"è«‹è¨­ç½® Gemini_AP ç’°å¢ƒè®Šæ•¸\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    logger.info(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "\n",
    "    TABLES = pd.read_csv('filtered_set1.csv')\n",
    "    logger.info(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "\n",
    "    \n",
    "    # è¨­ç½®åƒæ•¸\n",
    "    MAX_DEPTH = 3\n",
    "    MAX_DEGREE = 4\n",
    "    \n",
    "    logger.info(f\"æœ€å¤§æ·±åº¦: {MAX_DEPTH}\")\n",
    "    logger.info(f\"æœ€å¤§åˆ†æ”¯åº¦: {MAX_DEGREE}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ”¹é€²çš„ Tree-of-Report\n",
    "    tree_report = TreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "    \n",
    "    # å»ºæ§‹å ±å‘Šæ¨¹\n",
    "    logger.info(\"é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        root = tree_report.build_tree(TABLES)\n",
    "        \n",
    "        # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "        logger.info(\"ç”Ÿæˆæœ€çµ‚å ±å‘Š...\")\n",
    "        final_report = tree_report.generate_report(root)\n",
    "        \n",
    "        # è¼¸å‡ºå ±å‘Š\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(\"TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"=\"*50)\n",
    "        print(final_report)\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        with open('tree_of_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Tree-of-Report æ•¸æ“šåˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ)\\n\\n\")\n",
    "            f.write(final_report)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        logger.info(f\"å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’\")\n",
    "        logger.info(\"ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        logger.info(\"- tree_of_report.md: æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"- tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\")\n",
    "        logger.info(\"- tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\")\n",
    "        logger.info(\"- execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\")\n",
    "        logger.info(\"- tree_visualization.html: å¯è¦–åŒ–é é¢\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # æ¸…ç†æš«å­˜æª”æ¡ˆ\n",
    "        for temp_file in ['input_tmp.csv', 'tmp.csv']:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                    logger.info(f\"æ¸…ç†æš«å­˜æª”æ¡ˆ: {temp_file}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1db8ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ç¬¬ 1/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 2/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 3/3 æ¬¡ç”Ÿæˆ...\n",
      "\n",
      "âœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\n",
      "[1] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "[2] åˆ†æ•¸: 0.6 â†’ é€™å ´ç¾½çƒè³½äº‹æˆ°æ³è† è‘—ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚æ¯”è³½åˆæ®µï¼Œé›™æ–¹éƒ½ä»¥è©¦æ¢æ€§çš„ç™¼çƒé–‹å±€ï¼Œéš¨å¾Œçƒè·¯è®ŠåŒ–å¤šç«¯ï¼Œæœ‰æ™‚æ˜¯è¼•å·§çš„ç¶²å‰å°çƒï¼Œæœ‰æ™‚æ˜¯åŠ›é“åè¶³çš„å¾Œå ´é‡æ“Šï¼Œçœ‹å¾—å‡ºé›™æ–¹é¸æ‰‹éƒ½åœ¨ç©æ¥µå°‹æ‰¾å°æ–¹çš„ç ´ç¶»ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé¸æ‰‹Aä¸€åº¦å–å¾—é ˜å…ˆï¼Œä½†é¸æ‰‹BéŸŒæ€§åè¶³ï¼Œç·Šå’¬æ¯”åˆ†ã€‚åœ¨å¤šæ‹ä¾†å›ä¸­ï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µä½³çš„é˜²å®ˆèƒ½åŠ›ï¼Œå¤šæ¬¡å°‡çœ‹ä¼¼å¿…æ®ºçš„çƒè·¯åŒ–è§£ã€‚ç¶²å‰çš„ç´°è†©æ‰‹æ³•å’Œå¾Œå ´çš„å¼·åŠ›é€²æ”»äº¤ç¹”ï¼Œè®“è§€çœ¾çœ‹å¾—ç›®ä¸æš‡çµ¦ã€‚\n",
      "\n",
      "åœ¨é—œéµæ™‚åˆ»ï¼Œé¸æ‰‹Aåˆ©ç”¨ä¸€æ¬¡ç²¾æº–çš„åˆ¤æ–·ï¼Œè®“å°æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸå¾—åˆ†ã€‚ç„¶è€Œï¼Œé¸æ‰‹Bä¹Ÿæ¯«ä¸ç¤ºå¼±ï¼Œéš¨å³ä»¥ä¸€è¨˜æ¼‚äº®çš„è½åœ°å¾—åˆ†é‚„ä»¥é¡è‰²ã€‚æ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œæ¯”è³½æ°£æ°›ä¹Ÿè¶Šç™¼ç·Šå¼µã€‚\n",
      "\n",
      "æœ€çµ‚ï¼Œé¸æ‰‹Aç©©ä½é™£è…³ï¼Œæ†‘è—‰è‘—ç©©å®šçš„ç™¼æ®å’Œé—œéµæ™‚åˆ»çš„æœæ–·é€²æ”»ï¼ŒæˆåŠŸæ‹¿ä¸‹åˆ†æ•¸ã€‚ä½†é¸æ‰‹Bçš„è¡¨ç¾ä¹ŸåŒæ¨£ç²¾å½©ï¼Œé›–æ•—çŒ¶æ¦®ã€‚æ•´å ´æ¯”è³½é«˜æ½®è¿­èµ·ï¼Œå……åˆ†å±•ç¾äº†ç¾½çƒé‹å‹•çš„é­…åŠ›ã€‚è§€çœ¾å€‘ä¹Ÿç‚ºé€™å ´ç²¾å½©çš„å°æ±ºç»ä¸Šäº†ç†±çƒˆçš„æŒè²ã€‚\n",
      "[3] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œæ”»é˜²è½‰æ›ç¯€å¥å¿«é€Ÿã€‚é–‹å±€é›™æ–¹äº’æœ‰é ˜å…ˆï¼Œæ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œé¦–å±€å‰åŠæ®µAé¸æ‰‹ç¨ä½”å„ªå‹¢ï¼Œä¸€åº¦å°‡æ¯”åˆ†æ‹‰é–‹è‡³2:1ï¼Œä½†Bé¸æ‰‹éš¨å³å±•é–‹åæ“Šï¼Œåˆ©ç”¨ç²¾æº–çš„è½é»æ§åˆ¶å’Œå¼·å‹¢çš„é€²æ”»ï¼Œå°‡æ¯”åˆ†è¿½å¹³ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å¤šå›åˆçš„ç²¾é‡‡å°æ±ºã€‚ä¾‹å¦‚ç¬¬ä¸‰åˆ†ï¼Œé›™æ–¹é¸æ‰‹ç¶“éå¤šæ¬¡çš„çŸ­çƒã€æŒ‘çƒã€é•·çƒã€æŠ½çƒã€åˆ‡çƒç­‰æˆ°è¡“é‹ç”¨ï¼Œè¶³è¶³ä¾†å›äº†17æ‹æ‰ç”±Aé¸æ‰‹æŠ“ä½æ©Ÿæœƒï¼Œä¸€è¨˜å°æ‰‹ç„¡æ³•æ¥åˆ°çš„çƒæ‹¿ä¸‹åˆ†æ•¸ã€‚å„˜ç®¡å¦‚æ­¤ï¼ŒBé¸æ‰‹ä¹Ÿæ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œéš¨å¾Œä¹Ÿä»¥é€£çºŒçš„ç©æ¥µé€²æ”»ï¼ŒåŒ…æ‹¬å¤šæ¬¡çš„æ®ºçƒï¼Œçµ¦Aé¸æ‰‹å¸¶ä¾†äº†æ¥µå¤§çš„å£“åŠ›ã€‚\n",
      "\n",
      "æ¯”è³½é€²å…¥ä¸­æ®µå¾Œï¼ŒAé¸æ‰‹åœ¨ç™¼çƒç’°ç¯€ä¸Šæ›´æ³¨æ„ç­–ç•¥ï¼Œå¶çˆ¾æ¡ç”¨çŸ­ç™¼ï¼Œå¸Œæœ›æ“¾äº‚Bé¸æ‰‹çš„ç¯€å¥ã€‚ä½†Bé¸æ‰‹ä¹Ÿç©æ¥µèª¿æ•´ï¼Œä¸¦åˆ©ç”¨Aé¸æ‰‹å¹¾æ¬¡åˆ¤æ–·å¤±èª¤åŠå›çƒæ›ç¶²çš„æ©Ÿæœƒï¼ŒæˆåŠŸå°‡æ¯”åˆ†åè¶…ã€‚\n",
      "\n",
      "æ¯”è³½æœ«æ®µï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µå¼·çš„éŸŒæ€§ã€‚å„˜ç®¡é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œä½†ä¾èˆŠåŠªåŠ›åœ¨æ¯ä¸€æ¬¡æ“Šçƒä¸­å°‹æ‰¾æ©Ÿæœƒã€‚Aé¸æ‰‹æ›¾ä¾é ç²¾æº–çš„è½é»å’Œå¹¾æ¬¡æ¼‚äº®çš„é˜²å®ˆåæ“Šï¼Œå°‡æ¯”åˆ†è¿½è¿‘ï¼Œä½†Bé¸æ‰‹ç¸½èƒ½åœ¨é—œéµæ™‚åˆ»æŒºèº«è€Œå‡ºï¼Œå¤šæ¬¡åˆ©ç”¨å¼·åŠ›çš„æ‰£æ®ºä»¥åŠç²¾æº–çš„ç¶²å‰å°çƒï¼Œç©©ä½é™£è…³ã€‚æœ€çµ‚ï¼ŒBé¸æ‰‹ä»¥ä¸€è¨˜è§’åº¦åˆé‘½çš„æ’²çƒï¼Œè®“Aé¸æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸæ‹¿ä¸‹é€™å±€çš„ç¬¬15åˆ†ã€‚éš¨å¾ŒAé¸æ‰‹é›–å¥®åŠ›è¿½è¶•ï¼Œä½†æœ€çµ‚Bé¸æ‰‹æ†‘è—‰ä¸€è¨˜å¹¸é‹çš„é•·çƒå‡ºç•Œï¼Œä»¥21:15æ‹¿ä¸‹æ­¤å±€å‹åˆ©ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½å……æ»¿äº†å„å¼å„æ¨£çš„æˆ°è¡“é‹ç”¨ï¼ŒåŒ…å«é•·çƒã€çŸ­çƒã€åˆ‡çƒã€æŒ‘çƒã€æ®ºçƒã€æ¨çƒã€å‹¾çƒï¼Œä»¥åŠç¶²å‰å°çƒçš„çˆ­å¥ªï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„çƒæŠ€å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œç‚ºè§€çœ¾å¸¶ä¾†äº†ä¸€å ´ç²¾é‡‡çµ•å€«çš„ç¾½çƒé¥—å®´ã€‚ æ¯”è³½çµæœé›–æœ‰å‹è² ï¼Œä½†é›™æ–¹é‹å‹•å“¡çš„é‹å‹•å®¶ç²¾ç¥ï¼Œéƒ½ä»¤äººå°è±¡æ·±åˆ»ã€‚\n",
      "\n",
      "ğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ 1 æ¬¡ï¼šé€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "âœ”ï¸ å·²å„²å­˜è‡³ï¼šbest_of_three_report_20250928_195942.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === å¯«ä½œé¢¨æ ¼è©å½™ ===\n",
    "BADMINTON_TERMS = {\n",
    "    'net': 'ç¶²å‰å¤±èª¤', 'out': 'å‡ºç•Œ', 'long': 'éåº•ç·š', 'smash': 'æ®ºçƒ',\n",
    "    'clear': 'é«˜é çƒ', 'drop': 'åˆ‡çƒ', 'drive': 'å¹³æŠ½çƒ', 'serve': 'ç™¼çƒ', 'return': 'å›çƒ'\n",
    "}\n",
    "ACTION_VERBS = ['å±•ç¾', 'ç™¼æ®', 'æŒæ¡', 'é‹ç”¨', 'æ–½å±•', 'æ§åˆ¶', 'ä¸»å°', 'å£“åˆ¶', 'çªç ´', 'å‰µé€ ', 'ç· é€ ', 'å¥ å®š', 'ç¢ºç«‹', 'éå›º', 'æ‰­è½‰', 'é€†è½‰']\n",
    "TECHNICAL_TERMS = ['lose_reason', 'getpoint_player', 'type', 'column', 'row']\n",
    "\n",
    "# === Gemini æ¨¡å‹åˆå§‹åŒ– ===\n",
    "def init_model(api_key: str):\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# === å“è³ªè©•ä¼° ===\n",
    "def assess_text_quality(text: str) -> float:\n",
    "    score = 0.0\n",
    "    if 30 <= len(text) <= 120:\n",
    "        score += 0.2\n",
    "    score += min(0.2, sum(1 for t in BADMINTON_TERMS.values() if t in text) * 0.1)\n",
    "    score += min(0.2, sum(1 for v in ACTION_VERBS if v in text) * 0.05)\n",
    "    if not any(t in text for t in TECHNICAL_TERMS):\n",
    "        score += 0.2\n",
    "    if 'ï¼Œ' in text or 'ã€‚' in text:\n",
    "        score += 0.2\n",
    "    return round(min(score, 1.0), 2)\n",
    "\n",
    "# === ä¸»æµç¨‹ï¼šé‡è¤‡3æ¬¡ç”Ÿæˆä¸¦è©•ä¼° ===\n",
    "def generate_best_of_three(df: pd.DataFrame, api_key: str):\n",
    "    model = init_model(api_key)\n",
    "    table_str = df.to_string(index=False)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­é«”è‚²æ–°èè¨˜è€…ï¼Œæ“…é•·æ’°å¯«ç¾½çƒæ¯”è³½å ±å°ã€‚\n",
    "è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šè¡¨æ ¼æ’°å¯«è³½äº‹æè¿°ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…å‡ºç¾æŠ€è¡“æ¬„ä½åç¨±ã€‚\n",
    "\n",
    "# è³½äº‹æ•¸æ“šè¡¨æ ¼ï¼š\n",
    "{table_str}\n",
    "\n",
    "è«‹æ’°å¯«æè¿°ï¼š\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            print(f\"â³ ç¬¬ {i+1}/3 æ¬¡ç”Ÿæˆ...\")\n",
    "            response = model.generate_content(prompt_template)\n",
    "            time.sleep(1)\n",
    "            text = response.text.strip() if response.text else \"âš ï¸ ç„¡å…§å®¹\"\n",
    "        except Exception as e:\n",
    "            text = f\"âš ï¸ ç”ŸæˆéŒ¯èª¤: {e}\"\n",
    "        score = assess_text_quality(text)\n",
    "        results.append({'index': i+1, 'text': text, 'score': score})\n",
    "\n",
    "    # é¸å‡ºæœ€ä½³çµæœ\n",
    "    best = max(results, key=lambda x: x['score'])\n",
    "\n",
    "    # è¼¸å‡ºåˆ°æª”æ¡ˆ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"best_of_three_report_{timestamp}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(f\"[ç‰ˆæœ¬ {r['index']}] å“è³ªåˆ†æ•¸: {r['score']}\\n{r['text']}\\n\\n\")\n",
    "        f.write(f\"ğŸ† æœ€ä½³ç‰ˆæœ¬ç‚ºç¬¬ {best['index']} æ¬¡ï¼Œåˆ†æ•¸: {best['score']}\\n\")\n",
    "        f.write(best['text'])\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\")\n",
    "    for r in results:\n",
    "        print(f\"[{r['index']}] åˆ†æ•¸: {r['score']} â†’ {r['text']}\")\n",
    "    print(f\"\\nğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ {best['index']} æ¬¡ï¼š{best['text']}\")\n",
    "    print(f\"âœ”ï¸ å·²å„²å­˜è‡³ï¼š{file_name}\")\n",
    "    return best\n",
    "\n",
    "# === æ¸¬è©¦å…¥å£ ===\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"è«‹è¨­ç½® Gemini_API ç’°å¢ƒè®Šæ•¸\")\n",
    "\n",
    "    df = pd.read_csv(\"filtered_set1.csv\")\n",
    "    \n",
    "\n",
    "    generate_best_of_three(df, api_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
