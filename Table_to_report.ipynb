{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc75c21b",
   "metadata": {},
   "source": [
    "## ç°¡ä»‹ ##\n",
    "æ­¤ä»£ç¢¼ç”¨ä¾†è®“LLMæ ¹æ“šè¡¨æ ¼è³‡æ–™èˆ‡ä½¿ç”¨è€…çš„æå•è¦æ±‚ï¼Œé€épiplineèˆ‡tree stuctureï¼Œç”Ÿæˆå ±å°æˆ–åˆ†æè³‡æ–™\n",
    "\n",
    "æ­¤ç¯‡ç ”ç©¶åªéœ€æä¾›\n",
    "\"main.txt\"ç‚ºä½¿ç”¨è€…çš„å¤§ç¶±èˆ‡ç°¡çŸ­æƒ³æ³•\n",
    "\"data_description.txt\"ç‚ºè¦åˆ†æçš„table columnsæ‰€ä»£è¡¨çš„æ„ç¾©\n",
    "å°±å¯ç”¢ç”Ÿå®Œæ•´å ±å°\n",
    "(å¯ä½¿ç”¨åœ¨ç”¢ç”Ÿä»»ä½•å ±å°ä¸Šä¸é™æ–¼ç¾½çƒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0e02",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "\n",
    "åˆªæ¸›ä¸å¿…è¦çš„columns\n",
    "\n",
    "çµæœä¿ç•™['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a047de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"Gemini_API\")\n",
    "if not api_key:\n",
    "    print(\"âŒ Gemini_API ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca3c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danie\\anaconda3\\envs\\cotable\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#æ­£å¼\n",
    "import dspy\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "class GeminiOpenAI(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        # ä½¿ç”¨ Google çš„ OpenAI å…¼å®¹ç«¯é»\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "        super().__init__(model=model_name)\n",
    "     \n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "         \n",
    "        # Convert messages to OpenAI format\n",
    "        if isinstance(messages, list):\n",
    "            formatted_messages = []\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, dict) and 'content' in msg:\n",
    "                    role = msg.get('role', 'user')\n",
    "                    formatted_messages.append({\n",
    "                        'role': role,\n",
    "                        'content': msg['content']\n",
    "                    })\n",
    "                else:\n",
    "                    formatted_messages.append({\n",
    "                        'role': 'user',\n",
    "                        'content': str(msg)\n",
    "                    })\n",
    "        else:\n",
    "            formatted_messages = [{'role': 'user', 'content': str(messages)}]\n",
    "         \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=formatted_messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            if not response.choices or not response.choices[0].message.content:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            \n",
    "            return [{\n",
    "                'text': response.choices[0].message.content,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "     \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                **kwargs\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key, model_name=\"gemini-2.0-flash\"):\n",
    "    lm = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def parse_list_from_response(response_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a Python list from various response formats including markdown code blocks\n",
    "    \"\"\"\n",
    "    if not response_text or response_text.strip() == \"\":\n",
    "        print(\"âš ï¸ å›æ‡‰ç‚ºç©º\")\n",
    "        return []\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```(?:python|json)?\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    \n",
    "    # Remove any additional backticks\n",
    "    text = text.strip('`').strip()\n",
    "    \n",
    "    # Try to find a list pattern in the text\n",
    "    list_match = re.search(r'\\[.*?\\]', text, re.DOTALL)\n",
    "    \n",
    "    if list_match:\n",
    "        list_text = list_match.group(0)\n",
    "    else:\n",
    "        print(f\"âš ï¸ ç„¡æ³•åœ¨å›æ‡‰ä¸­æ‰¾åˆ°åˆ—è¡¨æ ¼å¼\")\n",
    "        print(f\"å®Œæ•´å›æ‡‰: {text[:200]}...\")\n",
    "        return []\n",
    "    \n",
    "    # Clean up the list text\n",
    "    list_text = list_text.strip()\n",
    "    \n",
    "    # Try multiple parsing strategies\n",
    "    try:\n",
    "        # Strategy 1: Parse as-is, change to python list\n",
    "        return json.loads(list_text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 2: Convert single quotes to double quotes\n",
    "        list_text_double = list_text.replace(\"'\", '\"')\n",
    "        return json.loads(list_text_double)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 3: Manual parsing for simple cases\n",
    "        # Remove brackets and split by comma\n",
    "        content = list_text.strip('[]').strip()\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        # Split by comma and clean each item\n",
    "        items = []\n",
    "        for item in content.split(','):\n",
    "            item = item.strip().strip('\"').strip(\"'\").strip()\n",
    "            if item:\n",
    "                items.append(item)\n",
    "        \n",
    "        if items:\n",
    "            print(f\"âœ“ ä½¿ç”¨æ‰‹å‹•è§£ææˆåŠŸ\")\n",
    "            return items\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ‰‹å‹•è§£æå¤±æ•—: {e}\")\n",
    "    \n",
    "    print(f\"âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±æ•—äº†\")\n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {list_text[:200]}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_news_relevant_fields(description_path: str, main_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    å¾æè¿°æ–‡ä»¶å’Œå¤§ç¶±æ–‡ä»¶ä¸­æå–ç›¸é—œæ¬„ä½\n",
    "    \n",
    "    Args:\n",
    "        description_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: ç¯©é¸å‡ºçš„æ¬„ä½åˆ—è¡¨\n",
    "    \"\"\"\n",
    "     \n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(description_path)\n",
    "    \n",
    "    prompt = f\"\"\"Using the following outline and list of data column descriptions, select only the columns that are useful for the outline.\n",
    "\n",
    "## outline\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Please return only a Python list of column names, like this:\n",
    "['player_name', 'match_score', 'duration', ...]\n",
    "\n",
    "Do not include explanations or any other text. Return only the list.\"\"\"\n",
    "     \n",
    "    result = lm.basic_request(prompt)\n",
    "    \n",
    "    print(f\"ğŸ” åŸå§‹å›æ‡‰:\\n{result}\\n\")\n",
    "    \n",
    "    selected_fields = parse_list_from_response(result)\n",
    "    \n",
    "    if selected_fields:\n",
    "        print(\"âœ… ç¯©é¸å‡ºçš„æ¬„ä½:\", selected_fields)\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè§£ææ¬„ä½åˆ—è¡¨\")\n",
    "    \n",
    "    return selected_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692ab381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åŸå§‹å›æ‡‰:\n",
      "```python\n",
      "['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n",
      "```\n",
      "\n",
      "âœ… ç¯©é¸å‡ºçš„æ¬„ä½: ['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n",
      "æœ€çµ‚æ¬„ä½æ¸…å–®: ['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n"
     ]
    }
   ],
   "source": [
    "# ç›´æ¥èª¿ç”¨å‡½å¼\n",
    "fields = extract_news_relevant_fields(\"data_description.txt\", \"main.txt\")\n",
    "print(\"æœ€çµ‚æ¬„ä½æ¸…å–®:\", fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c560a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"set1.csv\")\n",
    "filtered_df = df[fields]\n",
    "filtered_df.to_csv(\"filtered_set1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7b2b4",
   "metadata": {},
   "source": [
    "å°‡æŒ‘é¸å‡ºçš„æ¬„ä½åŠèªªæ˜å¯«å…¥filtered_data_description.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc7c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ filtered_data_description.txt\n"
     ]
    }
   ],
   "source": [
    "def extract_descriptions_for_fields(fields: List[str], desc_path: str, output_path: str):\n",
    "    description_text = read_text_file(desc_path)\n",
    "\n",
    "    field_desc = {}\n",
    "    for line in description_text.splitlines():\n",
    "        for field in fields:\n",
    "            if line.lower().startswith(field.lower() + \":\"):\n",
    "                field_desc[field] = line.strip()\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for field in fields:\n",
    "                f.write(field_desc.get(field, f\"{field}: [Description not found]\") + \"\\n\")\n",
    "        print(f\"âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯«å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "\n",
    "extract_descriptions_for_fields(fields, 'data_description.txt', \"filtered_data_description.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c668469",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "\n",
    "è—‰ç”±äººç‚ºè¼¸å…¥å•é¡Œèˆ‡æ–¹å‘æç¤ºï¼Œçµ¦LLMåšå®Œæ•´åˆ†æå•é¡Œèˆ‡æ–¹å‘ä¹‹è¦åŠƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e639240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain_of_thought_response(main_path: str, desc_path: str, output_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆ Chain-of-Thought åˆ†æå›æ‡‰\n",
    "    \n",
    "    Args:\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        desc_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        str: ç”Ÿæˆçš„å›æ‡‰å…§å®¹,å¦‚æœå¤±æ•—å‰‡è¿”å› None\n",
    "    \"\"\"\n",
    "\n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(desc_path)\n",
    "\n",
    "    chain_prompt = f\"\"\"\n",
    "You are a planning assistant.\n",
    "Analyze the following outline and column descriptions.\n",
    "\n",
    "## Outline & Ideas:\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Step-by-step:\n",
    "1. Reflect on the structure and meaning of the content.\n",
    "2. Formulate relevant and meaningful questions or planning strategies.\n",
    "3. Be explicit and detailed, use Chain-of-Thought reasoning.\n",
    "4. Output all thoughts and questions in English only.\n",
    "\"\"\"\n",
    "\n",
    "    result = lm.basic_request(chain_prompt)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "        print(f\"âœ… Response saved to: {output_path}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to write output: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbf0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response saved to: analyze_response.txt\n"
     ]
    }
   ],
   "source": [
    "response = generate_chain_of_thought_response(\n",
    "    main_path=\"main.txt\",\n",
    "    desc_path=\"filtered_data_description.txt\",\n",
    "    output_path=\"analyze_response.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1496f5f",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "\n",
    "è«‹LLMæ ¹æ“š\"analyze_response.txt\"æ€è€ƒå¯ä»¥ä½¿ç”¨çš„operationä¸¦å°‡çµæœå­˜æ–¼ \"operations_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6399f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ operations_info.json\n",
      "\n",
      "âœ… æ“ä½œåç¨±é™£åˆ—:\n",
      "['write', 'select_row', 'select_column', '**group_by', '**aggregate', '**sort', '**join', '**calculate', '**pivot_table', '**window_function', '**value_counts', '**crosstab', '**shift', '**correlation', '**query']\n"
     ]
    }
   ],
   "source": [
    "def analyze_operations(analyze_path: str, output_json: str) -> List[str]:\n",
    "    lm = setup_gemini_api(api_key)\n",
    "    analysis = read_text_file(analyze_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a news journalist want to analyze data not forecaster.\n",
    "Based on the following text analysis, identify multiple useful table operations\n",
    "and describe the direct meaning of each operation.\n",
    "\n",
    "## Text Analysis:\n",
    "{analysis}\n",
    "\n",
    "---\n",
    "\n",
    "Please output a numbered list in this format:\n",
    "1. write: If the table is clear or small enough, generates text based on the tables using the LLM.\n",
    "2. select_row: Description\n",
    "3. select_column: Description\n",
    "4. operation_name: Description\n",
    "5. operation_name: Description\n",
    "...\n",
    "\n",
    "IMPORTANT: operation must contain select_row, select_column, and write in the first three operation.\n",
    "\n",
    "Give important operations and at most 15 operations.\n",
    "operation_name should be different and each operation can not be similar.\n",
    "operation can be apply on many columns is better.\n",
    "Description just give the original definition of the operation name and give some useful functions name in pandas.\n",
    "Only include operations and their descriptions. Be concise and clear.\n",
    "\"\"\"\n",
    "\n",
    "    response = lm.basic_request(prompt)\n",
    "\n",
    "    operations = []\n",
    "    operations_dict = {}\n",
    "\n",
    "    try:\n",
    "        for line in response.strip().split('\\n'):\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            if \".\" in line:\n",
    "                num, rest = line.split(\".\", 1)\n",
    "                if \":\" in rest:\n",
    "                    name, desc = rest.strip().split(\":\", 1)\n",
    "                    name = name.strip()\n",
    "                    desc = desc.strip()\n",
    "                    operations.append(name)\n",
    "                    operations_dict[num.strip()] = {\"operation\": name, \"description\": desc}\n",
    "\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(operations_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ {output_json}\")\n",
    "        return operations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å›æ‡‰è™•ç†å¤±æ•—: {e}\\nåŸå§‹å›æ‡‰:\\n{response}\")\n",
    "        return []\n",
    "\n",
    "ops = analyze_operations(\"analyze_response.txt\", \"operations_info.json\")\n",
    "print(\"\\nâœ… æ“ä½œåç¨±é™£åˆ—:\")\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cebfb8",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "\n",
    "ä½¿LLMè‡ªå‹•åˆ†ætableé¸å‡ºåˆé©çš„operationæ”¾å…¥æ“ä½œæ± (operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14dcba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperationSignature(dspy.Signature):\n",
    "    \"\"\"Identify suitable operations for analyzing badminton match data.\"\"\"\n",
    "    data_description = dspy.InputField(desc=\"Overview and sample of the dataset\")\n",
    "    column_descriptions = dspy.InputField(desc=\"Descriptions of each column in the dataset\")\n",
    "    rules = dspy.InputField(desc=\"Rules for selecting operations\")\n",
    "    operations_list = dspy.OutputField(desc=\"A list of suitable operations number (e.g., [1, 2, 3, 4])\")\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: CSV æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: è®€å–çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    è®€å– JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: JSON æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "def parse_column_descriptions(description_text):\n",
    "    \"\"\"\n",
    "    è§£ææ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        description_text: æ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Returns:\n",
    "        dict: æ¬„ä½åç¨±åˆ°æè¿°çš„æ˜ å°„\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    pattern = r'''\n",
    "        ^                # Line start\n",
    "        (\\w+)            # Column name\n",
    "        :\\s+             # Colon and space\n",
    "        (.+?)            # Description text\n",
    "        (?=\\n\\w+:\\s+|\\Z) # Lookahead for next column or end of file\n",
    "    '''\n",
    "    matches = re.findall(pattern, description_text, flags=re.M | re.X)\n",
    "    for col_name, desc in matches:\n",
    "        clean_desc = ' '.join(desc.split()).strip()\n",
    "        descriptions[col_name] = clean_desc\n",
    "    return descriptions\n",
    "\n",
    "class BadmintonOperationSelector(dspy.Module):\n",
    "    def __init__(self, required_operations=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ“ä½œé¸æ“‡å™¨\n",
    "        \n",
    "        Args:\n",
    "            required_operations: å¿…é ˆåŒ…å«çš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (ä¾‹å¦‚: [1, 2, 3])\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.chain_of_thought = dspy.ChainOfThought(OperationSignature)\n",
    "        self.required_operations = required_operations or []\n",
    "\n",
    "    def forward(self, data_description, column_descriptions, rules):\n",
    "        result = self.chain_of_thought(\n",
    "            data_description=data_description,\n",
    "            column_descriptions=str(column_descriptions),\n",
    "            rules=str(rules)\n",
    "        )\n",
    "        operations = self.extract_operations_from_result(result.operations_list)\n",
    "        \n",
    "        # ç¢ºä¿å¿…éœ€çš„æ“ä½œè¢«åŒ…å«\n",
    "        operations = self.ensure_required_operations(operations)\n",
    "        \n",
    "        return operations\n",
    "\n",
    "    def extract_operations_from_result(self, operations_text):\n",
    "        \"\"\"\n",
    "        å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        æ”¯æ´å¤šç¨®æ ¼å¼:\n",
    "        - [1, 2, 3, 4]\n",
    "        - 1, 2, 3, 4\n",
    "        - 1 2 3 4\n",
    "        - Operation 1, Operation 2, etc.\n",
    "        \"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç§»é™¤ markdown ä»£ç¢¼å¡Šæ¨™è¨˜\n",
    "        operations_text = re.sub(r'```(?:python|json)?\\s*', '', operations_text)\n",
    "        operations_text = operations_text.strip('`').strip()\n",
    "        \n",
    "        # å˜—è©¦è§£æ JSON æ ¼å¼ [1, 2, 3]\n",
    "        try:\n",
    "            # å°‹æ‰¾æ–¹æ‹¬è™Ÿä¸­çš„å…§å®¹\n",
    "            list_match = re.search(r'\\[([^\\]]+)\\]', operations_text)\n",
    "            if list_match:\n",
    "                list_content = list_match.group(1)\n",
    "                # æå–æ‰€æœ‰æ•¸å­—\n",
    "                numbers = re.findall(r'\\d+', list_content)\n",
    "                operations = [int(num) for num in numbers]\n",
    "                if operations:\n",
    "                    return operations\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # å¦‚æœæ²’æœ‰æ–¹æ‹¬è™Ÿ,å˜—è©¦ç›´æ¥æå–æ‰€æœ‰æ•¸å­—\n",
    "        numbers = re.findall(r'\\d+', operations_text)\n",
    "        if numbers:\n",
    "            operations = [int(num) for num in numbers]\n",
    "            return operations\n",
    "        \n",
    "        # å¦‚æœä»¥ä¸Šéƒ½å¤±æ•—,å˜—è©¦é€è¡Œè™•ç†\n",
    "        lines = operations_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # æå–è©²è¡Œä¸­çš„æ‰€æœ‰æ•¸å­—\n",
    "            line_numbers = re.findall(r'\\d+', line)\n",
    "            operations.extend([int(num) for num in line_numbers])\n",
    "        \n",
    "        # å»é‡ä¸¦æ’åº\n",
    "        if operations:\n",
    "            operations = sorted(list(set(operations)))\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def ensure_required_operations(self, operations):\n",
    "        \"\"\"\n",
    "        ç¢ºä¿å¿…éœ€çš„æ“ä½œè¢«åŒ…å«åœ¨æ“ä½œåˆ—è¡¨ä¸­\n",
    "        \n",
    "        Args:\n",
    "            operations: ç•¶å‰çš„æ“ä½œåˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "            list: åŒ…å«å¿…éœ€æ“ä½œçš„å®Œæ•´åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # è½‰æ›ç‚ºé›†åˆä»¥é¿å…é‡è¤‡\n",
    "        operations_set = set(operations)\n",
    "        \n",
    "        # æ·»åŠ å¿…éœ€çš„æ“ä½œ\n",
    "        for required_op in self.required_operations:\n",
    "            operations_set.add(required_op)\n",
    "        \n",
    "        # è½‰æ›å›åˆ—è¡¨ä¸¦æ’åº\n",
    "        return sorted(list(operations_set))\n",
    "\n",
    "\n",
    "def analyze_badminton_match(data_path, column_desc_path, rules_path, \n",
    "                           model_name=\"gemini-2.0-flash-exp\", \n",
    "                           required_operations=None):\n",
    "    \"\"\"\n",
    "    åˆ†æç¾½çƒæ¯”è³½æ•¸æ“šä¸¦è­˜åˆ¥é©åˆçš„æ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        data_path: æ¯”è³½æ•¸æ“š CSV æ–‡ä»¶è·¯å¾‘\n",
    "        column_desc_path: æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        rules_path: æ“ä½œè¦å‰‡ JSON æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "        required_operations: å¿…é ˆåŒ…å«çš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (ä¾‹å¦‚: [1, 2, 3])\n",
    "    \n",
    "    Returns:\n",
    "        list: è­˜åˆ¥å‡ºçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (æ•´æ•¸)\n",
    "    \"\"\"\n",
    "    \n",
    "    # è¨­ç½®é»˜èªçš„å¿…éœ€æ“ä½œç‚º [1, 2, 3]\n",
    "    if required_operations is None:\n",
    "        required_operations = [1, 2, 3]\n",
    "    \n",
    "    print(\"Reading badminton match data...\")\n",
    "    try:\n",
    "        match_data = read_badminton_data(data_path)\n",
    "        columns_desc_content = read_text_file(column_desc_path)\n",
    "        rules = read_json_file(rules_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading files: {e}\")\n",
    "        return []\n",
    "\n",
    "    column_descriptions = parse_column_descriptions(columns_desc_content)\n",
    "    setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    data_sample = match_data.head().to_string()\n",
    "    data_description = f\"\"\"\n",
    "    one match data:\n",
    "    {data_sample}\n",
    "\n",
    "    Data shape: {match_data.shape[0]} rows, {match_data.shape[1]} columns\n",
    "    Columns: {', '.join(match_data.columns)}\n",
    "    \"\"\"\n",
    "\n",
    "    selector = BadmintonOperationSelector(required_operations=required_operations)\n",
    "    operations = selector.forward(data_description, column_descriptions, rules)\n",
    "\n",
    "    print(f\"âœ… Identified {len(operations)} suitable operations:\")\n",
    "    print(f\"   Required operations: {required_operations}\")\n",
    "    for i, op in enumerate(operations, 1):\n",
    "        required_marker = \" (Required)\" if op in required_operations else \"\"\n",
    "        print(f\"{i}. Operation {op}{required_marker}\")\n",
    "\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e586506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading badminton match data...\n",
      "âœ… Identified 8 suitable operations:\n",
      "   Required operations: [1, 2, 3]\n",
      "1. Operation 1 (Required)\n",
      "2. Operation 2 (Required)\n",
      "3. Operation 3 (Required)\n",
      "4. Operation 4\n",
      "5. Operation 5\n",
      "6. Operation 8\n",
      "7. Operation 11\n",
      "8. Operation 12\n",
      "\n",
      "Final operations array: [1, 2, 3, 4, 5, 8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "operations = analyze_badminton_match(\n",
    "    data_path=\"filtered_set1.csv\",\n",
    "    column_desc_path=\"filtered_data_description.txt\",\n",
    "    rules_path=\"operations_info.json\"\n",
    ")\n",
    "print(\"\\nFinal operations array:\", operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d09a",
   "metadata": {},
   "source": [
    "å°‡æ‰€æŒ‘é¸å‡ºä¾†çš„æ“ä½œå¯«å…¥\"operations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "689bc29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… operations.json has been created with 8 operations.\n",
      "Selected operations: [1, 2, 3, 4, 5, 8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# å¾ JSON æª”æ¡ˆè®€å– operations\n",
    "original_operations_dict = read_json_file(\"operations_info.json\")\n",
    "\n",
    "# ä½ æƒ³è¦æŒ‘é¸çš„ operation ç·¨è™Ÿï¼ˆæ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹é€™å€‹ listï¼‰\n",
    "selected_numbers = operations\n",
    "\n",
    "def clean_operation_name(operation_text):\n",
    "    \"\"\"\n",
    "    æ¸…ç†æ“ä½œåç¨±ï¼Œåªä¿ç•™è‹±æ–‡å­—æ¯ã€æ•¸å­—å’Œåº•ç·š\n",
    "    ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å¦‚ **, -, ç­‰\n",
    "    \n",
    "    Args:\n",
    "        operation_text: åŸå§‹æ“ä½œåç¨±\n",
    "    \n",
    "    Returns:\n",
    "        str: æ¸…ç†å¾Œçš„æ“ä½œåç¨±\n",
    "    \"\"\"\n",
    "    # ç§»é™¤æ‰€æœ‰éå­—æ¯ã€æ•¸å­—ã€åº•ç·šçš„å­—ç¬¦\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '', operation_text)\n",
    "    return cleaned\n",
    "\n",
    "filtered_operations = []\n",
    "for new_number, original_number in enumerate(selected_numbers, start=1):\n",
    "    # å°‡æ•¸å­—è½‰æ›ç‚ºå­—ä¸²éµä¾†æŸ¥æ‰¾\n",
    "    key = str(original_number)\n",
    "    if key in original_operations_dict:\n",
    "        op_data = original_operations_dict[key]\n",
    "        \n",
    "        # æ¸…ç† operation åç¨±\n",
    "        cleaned_operation = clean_operation_name(op_data[\"operation\"])\n",
    "        \n",
    "        filtered_operations.append({\n",
    "            \"number\": new_number,\n",
    "            \"operation\": cleaned_operation,\n",
    "            \"description\": op_data[\"description\"]\n",
    "        })\n",
    "\n",
    "# æ–°çš„ JSON çµæ§‹\n",
    "output_json = {\n",
    "    \"description\": \"Selected operations for badminton data analysis.\",\n",
    "    \"requirements\": [\n",
    "        \"The output must be based on the input data; do not hallucinate.\",\n",
    "        \"Give me the list of numbers.\"\n",
    "    ],\n",
    "    \"operations\": filtered_operations\n",
    "}\n",
    "\n",
    "# å¯«å…¥ JSON æª”æ¡ˆ\n",
    "with open(\"operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… operations.json has been created with {len(filtered_operations)} operations.\")\n",
    "print(f\"Selected operations: {selected_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743109f4",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "\n",
    "æ ¹æ“šçœŸå¯¦tableå°‡æ“ä½œé‡è¦æ€§æ’åºï¼Œè‹¥ç‚ºæ’åºå¾Œ30%ä¸”éä¸‰ç¨®é‡è¦æ“ä½œï¼Œå‰‡æ›¿é™¤ï¼Œä¿ç•™'write' 'select_col' 'select_row'ä¸‰å€‹é‡è¦æ“ä½œï¼Œåˆ°'selected_operations.json'\n",
    "\n",
    "æ“ä½œæå–å·²å®Œæˆ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7229122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations.json æˆåŠŸè¼‰å…¥ 8 å€‹æ“ä½œ\n",
      "æ“ä½œæ•¸é‡: 8\n",
      "å˜—è©¦ API è«‹æ±‚ (ç¬¬ 1/3 æ¬¡)...\n",
      "âœ… æˆåŠŸç²å¾— API å›æ‡‰\n",
      "æå–åˆ°æ“ä½œåˆ—è¡¨: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "\n",
      "============================================================\n",
      "å®Œæ•´å›æ‡‰:\n",
      "å¥½çš„ï¼Œæˆ‘å€‘ä¾†åˆ†æé€™äº›æ“ä½œå°æ–¼ç¾½çƒæ–°èå¯«ä½œçš„é‡è¦æ€§ï¼Œä¸¦çµ¦å‡ºæ’åºã€‚\n",
      "\n",
      "**Chain-of-Thought:**\n",
      "\n",
      "ä½œç‚ºä¸€ä½è³‡æ·±çš„ç¾½çƒæ–°èè¨˜è€…ï¼Œæˆ‘çš„ç›®æ¨™æ˜¯åˆ©ç”¨æ¯”è³½æ•¸æ“šï¼Œç”¢å‡ºæœ‰æ·±åº¦ã€æœ‰åƒ¹å€¼çš„å ±å°ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦äº†è§£æ¯”è³½çš„åŸºæœ¬æƒ…æ³ï¼Œç„¶å¾Œæ·±å…¥æŒ–æ˜æ•¸æ“šä¸­çš„æ¨¡å¼å’Œæ´è¦‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘å°å„å€‹æ“ä½œçš„è©•ä¼°ï¼š\n",
      "\n",
      "*   **`value_counts` (7):**  é€™å€‹æ“ä½œå¯ä»¥å¿«é€Ÿäº†è§£å„ç¨®é¡å‹çš„çƒ (type) å‡ºç¾çš„é »ç‡ï¼Œæˆ–è€…çƒå“¡å¾—åˆ† (getpoint\\_player) çš„æ¬¡æ•¸ã€‚é€™å°æ–¼åˆ†æçƒå“¡çš„æ‰“æ³•åå¥½ã€æˆ°è¡“é¸æ“‡ï¼Œä»¥åŠæ‰¾å‡ºé—œéµå¾—åˆ†æ‰‹æ®µéå¸¸æœ‰å¹«åŠ©ã€‚èƒ½å¿«é€ŸæŒæ¡çƒå“¡æˆ–è³½äº‹çš„åˆæ­¥å°è±¡ã€‚é€™æ˜¯åŸºç¤åˆ†æçš„é—œéµã€‚\n",
      "*   **`crosstab` (8):** é€™å€‹æ“ä½œå¯ä»¥å¹«åŠ©æˆ‘å€‘å»ºç«‹å…©å€‹æˆ–å¤šå€‹å› ç´ ä¹‹é–“çš„é—œè¯æ€§ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥æ¯”è¼ƒä¸åŒçƒå“¡åœ¨ä¸åŒæƒ…æ³ä¸‹ä½¿ç”¨çš„çƒç¨®ï¼Œæˆ–è€…åˆ†æç™¼çƒå¾Œæ¥ç™¼çƒæ–¹å¾—åˆ†çš„æ©Ÿç‡ã€‚é€™ç¨®é—œè¯æ€§åˆ†æå¯ä»¥æ­ç¤ºæ›´æ·±å±¤æ¬¡çš„æˆ°è¡“ç­–ç•¥ã€‚\n",
      "*   **`group_by` (4) and `aggregate` (5):**  é€™å…©å€‹æ“ä½œé€šå¸¸ä¸€èµ·ä½¿ç”¨ï¼Œå¯ä»¥å°‡æ•¸æ“šæŒ‰ç…§ç‰¹å®šæ¢ä»¶åˆ†çµ„ï¼Œç„¶å¾Œè¨ˆç®—å„çµ„çš„çµ±è¨ˆæ•¸æ“šã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥æŒ‰ç…§çƒå“¡åˆ†çµ„ï¼Œè¨ˆç®—ä»–å€‘çš„å¹³å‡å¾—åˆ†ã€å¹³å‡å›åˆæ•¸ç­‰ã€‚æˆ–è€…æˆ‘å€‘å¯ä»¥æŒ‰ç…§ä¸åŒçš„å›åˆæ•¸åˆ†çµ„ï¼Œè¨ˆç®—ä¸åŒå›åˆçš„å¾—åˆ†ç‡ç­‰ã€‚é€™å€‹æ“ä½œå¯ä»¥å¹«åŠ©æˆ‘å€‘æ¯”è¼ƒä¸åŒçƒå“¡æˆ–ä¸åŒå›åˆä¹‹é–“çš„å·®ç•°ï¼Œæ‰¾å‡ºé—œéµå› ç´ ã€‚åœ¨è³½äº‹åˆ†æä¸­ï¼Œé€šå¸¸éœ€è¦åˆ†çµ„æ¯”è¼ƒæ•¸æ“šï¼Œä¾‹å¦‚æ¯”è¼ƒå‹è² æ–¹çš„å„é …æ•¸æ“šã€‚\n",
      "*   **`calculate` (6):** é€™å€‹æ“ä½œå¯ä»¥ç”¨ä¾†è¨ˆç®—æ–°çš„æ•¸æ“šæŒ‡æ¨™ï¼Œä¾‹å¦‚å¾—åˆ†å·®ã€å›åˆæŒçºŒæ™‚é–“ç­‰ç­‰ã€‚é€™äº›æŒ‡æ¨™å¯ä»¥å¹«åŠ©æˆ‘å€‘æ›´æ·±å…¥åœ°äº†è§£æ¯”è³½çš„é€²ç¨‹å’Œçƒå“¡çš„è¡¨ç¾ã€‚ä¾‹å¦‚ï¼Œè¨ˆç®—ã€Œä¾µç•¥æ€§æ¯”ç‡ã€ï¼ˆæ®ºçƒæ¬¡æ•¸/ç¸½æ“Šçƒæ¬¡æ•¸ï¼‰å¯ä»¥åæ˜ çƒå“¡çš„é€²æ”»é¢¨æ ¼ã€‚\n",
      "*   **`select_column` (3):** é¸æ“‡ç‰¹å®šçš„æ•¸æ“šåˆ—æ˜¯é€²è¡Œä»»ä½•åˆ†æçš„åŸºç¤ã€‚å¦‚æœæˆ‘å€‘æƒ³åˆ†æçƒå“¡çš„å¾—åˆ†æƒ…æ³ï¼Œå°±éœ€è¦é¸æ“‡`getpoint_player`é€™ä¸€åˆ—ã€‚å…¶ä»–æ“ä½œåŸºæœ¬ä¸Šéƒ½è¦å…ˆé€²è¡Œæ¬„ä½çš„é¸æ“‡ã€‚\n",
      "*   **`select_row` (2):** æ ¹æ“šæ¢ä»¶ç¯©é¸æ•¸æ“šå¯ä»¥å¹«åŠ©æˆ‘å€‘èšç„¦åˆ°ç‰¹å®šçš„æ¯”è³½éšæ®µæˆ–æƒ…å¢ƒã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥ç¯©é¸å‡ºæ¯”åˆ†æ¥è¿‘çš„éšæ®µé€²è¡Œåˆ†æï¼Œæˆ–è€…åªé—œæ³¨æŸå€‹ç‰¹å®šçƒå“¡çš„è¡¨ç¾ã€‚\n",
      "*   **`write` (1):** åœ¨åˆ†æå®Œæˆå¾Œï¼Œæ’°å¯«æ–°èå ±å°æ˜¯æœ€çµ‚ç›®çš„ã€‚æ ¹æ“šåˆ†æçµæœï¼Œå°‡æ•¸æ“šè½‰åŒ–ç‚ºæ˜“æ–¼ç†è§£çš„æ–‡å­—æè¿°ã€‚é€™å€‹æ“ä½œé›–ç„¶é‡è¦ï¼Œä½†ä¾è³´æ–¼å‰é¢æ‰€æœ‰åˆ†ææ“ä½œçš„çµæœã€‚\n",
      "\n",
      "**æ’åºçµæœ:**\n",
      "\n",
      "```\n",
      "[7, 8, 4, 5, 6, 3, 2, 1]\n",
      "```\n",
      "\n",
      "============================================================\n",
      "\n",
      "æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "é¸æ“‡äº† 8 å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: 70%)\n",
      "é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "âœ… selected_operations.json has been created with 8 operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_operations_from_json(json_file_path):\n",
    "    \"\"\"\n",
    "    Load operations from JSON file\n",
    "    æ”¯æ´å…©ç¨®æ ¼å¼:\n",
    "    1. èˆŠæ ¼å¼: {\"1\": {\"operation\": \"...\", \"description\": \"...\"}, ...}\n",
    "    2. æ–°æ ¼å¼: {\"operations\": [{\"number\": 1, \"operation\": \"...\", \"description\": \"...\"}, ...]}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = read_json_file(json_file_path)\n",
    "        \n",
    "        operations_data = data['operations']\n",
    "        \n",
    "        # Create formatted operation strings for LLM processing\n",
    "        operation_strings = []\n",
    "        operation_details = []\n",
    "        \n",
    "        for op in operations_data:\n",
    "            number = op.get('number', '')\n",
    "            name = op.get('operation', '')\n",
    "            description = op.get('description', '')\n",
    "            \n",
    "            # Format as: \"number. name: description\"\n",
    "            if number and name and description:\n",
    "                formatted_op = f\"{number}. {name}: {description}\"\n",
    "                operation_strings.append(formatted_op)\n",
    "                operation_details.append({\n",
    "                    'number': number,\n",
    "                    'operation': name,  # çµ±ä¸€ä½¿ç”¨ 'operation' éµ\n",
    "                    'description': description,\n",
    "                    'formatted': formatted_op\n",
    "                })\n",
    "        \n",
    "        print(f\"å¾ {json_file_path} æˆåŠŸè¼‰å…¥ {len(operation_strings)} å€‹æ“ä½œ\")\n",
    "        return operation_details, operation_strings\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_file_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"éŒ¯èª¤: {json_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataset\n",
    "    \"\"\"\n",
    "    summary = f\"è³‡æ–™é›†æ¦‚è¦:\\n- ç¸½è¡Œæ•¸: {dataframe.shape[0]}\\n- ç¸½åˆ—æ•¸: {dataframe.shape[1]}\\n- æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\\n\\nå„æ¬„ä½è³‡è¨Š:\\n\"\n",
    "    for col in dataframe.columns:\n",
    "        summary += f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            unique_vals = dataframe[col].unique()[:10]\n",
    "            summary += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, unique_vals))}\\n\"\n",
    "        else:\n",
    "            summary += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\\n\"\n",
    "    return summary\n",
    "\n",
    "\n",
    "def extract_operation_numbers_from_response(response):\n",
    "    \"\"\"\n",
    "    å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "    æ”¯æ´å¤šç¨®æ ¼å¼\n",
    "    \"\"\"\n",
    "    # æ–¹æ³•1: åŒ¹é…ä»£ç¢¼å¡Šä¸­çš„æ•¸çµ„\n",
    "    pattern1 = r'```\\s*\\[([\\d,\\s]+)\\]\\s*```'\n",
    "    match = re.search(pattern1, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',') if num]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•2: åŒ¹é…æ™®é€šæ–¹æ‹¬è™Ÿä¸­çš„æ•¸çµ„\n",
    "    pattern2 = r'\\[([\\d,\\s]+)\\]'\n",
    "    match = re.search(pattern2, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',') if num]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•3: æå–æ‰€æœ‰æ•¸å­—\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        operation_list = [int(num) for num in numbers]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ’åºæ•¸çµ„\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def filter_badminton_operations(operation_details, operation_strings, df, api_key, \n",
    "                                outline_path='outline.txt', model_name=\"gemini-2.0-flash\", \n",
    "                                max_retries=3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Gemini æ ¹æ“šé‡è¦æ€§æ’åºæ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        operation_strings: æ“ä½œæ ¼å¼åŒ–å­—ä¸²åˆ—è¡¨\n",
    "        df: æ•¸æ“šæ¡†\n",
    "        api_key: API é‡‘é‘°\n",
    "        outline_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: æ¨¡å‹åç¨±\n",
    "        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨, å®Œæ•´å›æ‡‰)\n",
    "    \"\"\"\n",
    "    gemini = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    data_summary = get_data_summary(df)\n",
    "    \n",
    "    # é™åˆ¶è³‡æ–™æ¨£æœ¬å¤§å°\n",
    "    data_sample = df.head(10).to_string()\n",
    "    if len(data_sample) > 3000:\n",
    "        data_sample = data_sample[:3000] + \"...\\n[è³‡æ–™å·²æˆªæ–·]\"\n",
    "    \n",
    "    outline = read_text_file(outline_path)\n",
    "    \n",
    "    print(f\"æ“ä½œæ•¸é‡: {len(operation_strings)}\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "æˆ‘æœ‰ä¸€å€‹æ’°å¯«æ–°èçš„å¤§ç¶±èˆ‡æ¯”è³½çš„è³‡æ–™é›†å’Œ {len(operation_strings)} å€‹åˆ†ææ“ä½œï¼Œè«‹ä¾æ“šæ“ä½œé‡è¦æ€§æ’åº(ç”±é«˜åˆ°ä½)ã€‚\n",
    "\n",
    "å¤§ç¶±:\n",
    "{outline}\n",
    "\n",
    "è³‡æ–™æ¨£æœ¬:\n",
    "{data_sample}\n",
    "\n",
    "è³‡æ–™é›†è³‡è¨Š:\n",
    "{data_summary}\n",
    "\n",
    "æ“ä½œæ¸…å–®:\n",
    "{chr(10).join(operation_strings)}\n",
    "\n",
    "è«‹å…ˆæ ¹æ“š chain-of-thought åˆ†æï¼Œç„¶å¾Œå°‡æ“ä½œç·¨è™Ÿæ ¹æ“šé‡è¦æ€§æ’åºï¼Œæ¯å€‹ç·¨è™Ÿåƒ…åœ¨é™£åˆ—ä¸­å‡ºç¾ä¸€æ¬¡ï¼Œé™£åˆ—é•·åº¦æ‡‰ç‚º {len(operation_strings)}ã€‚\n",
    "\n",
    "æœ€å¾Œè«‹ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºæ’åºçµæœ:\n",
    "[1, 2, 3, ...]\"\"\"\n",
    "    \n",
    "    # ä½¿ç”¨é‡è©¦é‚è¼¯\n",
    "    response = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"å˜—è©¦ API è«‹æ±‚ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...\")\n",
    "            response = gemini.basic_request(prompt)\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤å›æ‡‰\n",
    "            if \"âš ï¸\" in response or not response:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"âš ï¸ è«‹æ±‚å¤±æ•—ï¼Œ{3}ç§’å¾Œé‡è©¦...\")\n",
    "                    import time\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"âŒ API å›æ‡‰éŒ¯èª¤ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸\")\n",
    "                    return [], response if response else \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "            \n",
    "            # æˆåŠŸç²å¾—å›æ‡‰\n",
    "            print(\"âœ… æˆåŠŸç²å¾— API å›æ‡‰\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è«‹æ±‚ç™¼ç”Ÿç•°å¸¸: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"3ç§’å¾Œé‡è©¦...\")\n",
    "                import time\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(f\"âŒ å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸\")\n",
    "                return [], f\"âš ï¸ API è«‹æ±‚å¤±æ•—: {e}\"\n",
    "    \n",
    "    if not response:\n",
    "        return [], \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "    \n",
    "    return extract_operation_numbers_from_response(response), response\n",
    "\n",
    "\n",
    "def create_selected_operations_json(operation_details, sorted_numbers, keep_percentage=0.7, \n",
    "                                    force_include=[1, 2, 3], output_path=\"selected_operations.json\"):\n",
    "    \"\"\"\n",
    "    å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        sorted_numbers: æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        keep_percentage: ä¿ç•™æ¯”ä¾‹\n",
    "        force_include: å¼·åˆ¶åŒ…å«çš„æ“ä½œç·¨è™Ÿ\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        list: é¸æ“‡çš„æ“ä½œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not sorted_numbers:\n",
    "        print(\"âš ï¸ è­¦å‘Š: sorted_numbers ç‚ºç©ºï¼Œç„¡æ³•å‰µå»ºæ“ä½œåˆ—è¡¨\")\n",
    "        return []\n",
    "    \n",
    "    # è¨ˆç®—è¦ä¿ç•™çš„æ“ä½œæ•¸é‡\n",
    "    keep_count = max(len(force_include), int(keep_percentage * len(sorted_numbers)))\n",
    "    \n",
    "    # é¸æ“‡å‰ N å€‹æ“ä½œ\n",
    "    selected_numbers = sorted_numbers[:keep_count]\n",
    "    \n",
    "    # ç¢ºä¿å¼·åˆ¶åŒ…å«çš„æ“ä½œåœ¨åˆ—è¡¨ä¸­\n",
    "    selected_numbers = list(set(selected_numbers) | set(force_include))\n",
    "    \n",
    "    # é‡æ–°æ’åº: å…ˆæŒ‰ç…§ sorted_numbers çš„é †åºï¼Œç„¶å¾ŒåŠ ä¸Š force_include ä¸­æœªå‡ºç¾çš„\n",
    "    final_selected = []\n",
    "    for num in sorted_numbers:\n",
    "        if num in selected_numbers and num not in final_selected:\n",
    "            final_selected.append(num)\n",
    "    \n",
    "    for num in force_include:\n",
    "        if num not in final_selected:\n",
    "            final_selected.append(num)\n",
    "    \n",
    "    print(f\"é¸æ“‡äº† {len(final_selected)} å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: {keep_percentage*100:.0f}%)\")\n",
    "    print(f\"é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: {final_selected}\")\n",
    "    \n",
    "    # å‰µå»ºæ“ä½œç·¨è™Ÿåˆ°è©³ç´°è³‡è¨Šçš„æ˜ å°„\n",
    "    operation_map = {int(detail['number']): detail for detail in operation_details}\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„æ“ä½œåˆ—è¡¨\n",
    "    new_operations = []\n",
    "    missing_operations = []\n",
    "    \n",
    "    for new_id, num in enumerate(final_selected, 1):\n",
    "        if num in operation_map:\n",
    "            detail = operation_map[num]\n",
    "            new_operations.append({\n",
    "                'number': new_id,\n",
    "                'operation': detail['operation'],\n",
    "                'description': detail['description']\n",
    "            })\n",
    "        else:\n",
    "            missing_operations.append(num)\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ“ä½œç·¨è™Ÿ {num}\")\n",
    "    \n",
    "    if missing_operations:\n",
    "        print(f\"âš ï¸ ç¼ºå¤±çš„æ“ä½œç·¨è™Ÿ: {missing_operations}\")\n",
    "    \n",
    "    output_json = {\n",
    "        \"description\": \"Selected operations for badminton data analysis.\",\n",
    "        \"requirements\": [\n",
    "            \"The output must be based on the input data; do not hallucinate.\",\n",
    "            \"Give me the list of numbers.\"\n",
    "        ],\n",
    "        \"operations\": new_operations\n",
    "    }\n",
    "\n",
    "    # å¯«å…¥ JSON æ–‡ä»¶\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… {output_path} has been created with {len(new_operations)} operations.\")\n",
    "    return new_operations\n",
    "\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "\n",
    "# ==================== ä¸»ç¨‹å¼ ====================\n",
    "\n",
    "# è¼‰å…¥æ“ä½œ\n",
    "json_file_path = \"operations.json\"\n",
    "operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "\n",
    "if not operation_details:\n",
    "    print(\"âŒ ç„¡æ³•è¼‰å…¥æ“ä½œï¼Œç¨‹å¼çµ‚æ­¢\")\n",
    "else:\n",
    "    # è¼‰å…¥æ•¸æ“š\n",
    "    df = read_badminton_data(\"filtered_set1.csv\")\n",
    "    \n",
    "    # ç²å– API é‡‘é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\") \n",
    "    \n",
    "    # æ’åºæ“ä½œ\n",
    "    sorted_numbers, response = filter_badminton_operations(\n",
    "        operation_details, \n",
    "        operation_strings, \n",
    "        df, \n",
    "        api_key, \n",
    "        outline_path='main.txt'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\nå®Œæ•´å›æ‡‰:\\n{response}\\n{'='*60}\\n\")\n",
    "    print(f\"æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: {sorted_numbers}\")\n",
    "    \n",
    "    # å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON\n",
    "    if sorted_numbers:\n",
    "        selected_ops = create_selected_operations_json(\n",
    "            operation_details,\n",
    "            sorted_numbers,\n",
    "            keep_percentage=0.7,\n",
    "            force_include=[1, 2, 3],\n",
    "            output_path=\"selected_operations.json\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"âŒ ç„¡æ³•æå–æ’åºçµæœï¼Œè·³éå‰µå»º selected_operations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a7919",
   "metadata": {},
   "source": [
    "# STEP final\n",
    "\n",
    "æ“ä½œç”Ÿæˆ (ContentPlanner)ã€å®‰å…¨åŸ·è¡Œ DataFrame æ“ä½œ (SafeDataFrameOperator)ã€æ¨¹çµæ§‹è¿½è¹¤ (TreeNode / TreeOfReport)ã€ä»¥åŠ æ–‡æœ¬ç”Ÿæˆ (TextGenerator)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d366becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:10,531 - INFO - Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\n",
      "2025-09-29 15:50:10,532 - INFO - ==================================================\n",
      "2025-09-29 15:50:10,532 - INFO - æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "2025-09-29 15:50:10,535 - INFO - æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 8 åˆ—\n",
      "2025-09-29 15:50:10,535 - INFO - æœ€å¤§æ·±åº¦: 3\n",
      "2025-09-29 15:50:10,536 - INFO - æœ€å¤§åˆ†æ”¯åº¦: 4\n",
      "2025-09-29 15:50:10,537 - INFO - è¼‰å…¥æ“ä½œæ± : ['value_counts', 'crosstab', 'group_by', 'aggregate', 'calculate', 'select_column', 'select_row', 'write']\n",
      "2025-09-29 15:50:10,538 - INFO - å¾é…ç½®æå–çš„æœ‰æ•ˆæ“ä½œ: {'crosstab', 'select_row', 'aggregate', 'write', 'calculate', 'group_by', 'select_column', 'value_counts'}\n",
      "2025-09-29 15:50:10,538 - INFO - OperationParser åˆå§‹åŒ–ï¼Œæœ‰æ•ˆæ“ä½œ: {'crosstab', 'select_row', 'aggregate', 'write', 'calculate', 'group_by', 'select_column', 'value_counts'}\n",
      "2025-09-29 15:50:10,539 - INFO - é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\n",
      "2025-09-29 15:50:10,539 - INFO - è™•ç†ç¯€é» - Level: 0, Operation: root(None)\n",
      "2025-09-29 15:50:10,554 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:50:12,113 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:50:12,114 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(type, getpoint_player, player)', 'value_counts(type)', 'value_counts(getpoint_player)', 'value_counts(player)']\n",
      "2025-09-29 15:50:17,226 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (0, 8)\n",
      "2025-09-29 15:50:17,227 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(type, getpoint_player, player), çµæœå½¢ç‹€: (0, 8)\n",
      "2025-09-29 15:50:17,228 - INFO - æ·»åŠ å­ç¯€é»: 94f1f6c1 to c659592a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame å·²æˆåŠŸä¿å­˜åˆ° 'tmp.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:19,100 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:50:19,100 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:50:19,101 - INFO - æ·»åŠ å­ç¯€é»: e67d99b0 to c659592a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type') æ“ä½œå®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:21,082 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-29 15:50:21,083 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(getpoint_player), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-29 15:50:21,083 - INFO - æ·»åŠ å­ç¯€é»: 5c4a956c to c659592a\n",
      "2025-09-29 15:50:23,243 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-29 15:50:23,243 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(player), çµæœå½¢ç‹€: (2, 2)\n",
      "2025-09-29 15:50:23,245 - INFO - æ·»åŠ å­ç¯€é»: fd387980 to c659592a\n",
      "2025-09-29 15:50:23,245 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: select_column(type, getpoint_player, player)\n",
      "2025-09-29 15:50:23,249 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('player') æ“ä½œå®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:23,770 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:50:23,772 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'value_counts(getpoint_player)']\n",
      "2025-09-29 15:50:26,337 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (0, 2)\n",
      "2025-09-29 15:50:26,338 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (0, 2)\n",
      "2025-09-29 15:50:26,339 - INFO - æ·»åŠ å­ç¯€é»: e4f7540c to 94f1f6c1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type') æ“ä½œå®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:28,691 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (0, 2)\n",
      "2025-09-29 15:50:28,692 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(getpoint_player), çµæœå½¢ç‹€: (0, 2)\n",
      "2025-09-29 15:50:28,692 - INFO - æ·»åŠ å­ç¯€é»: 2539f1c4 to 94f1f6c1\n",
      "2025-09-29 15:50:28,693 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(type)\n",
      "2025-09-29 15:50:28,695 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:50:29,270 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:50:29,271 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(type)', 'value_counts(type)']\n",
      "2025-09-29 15:50:31,871 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 1)\n",
      "2025-09-29 15:50:31,872 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(type), çµæœå½¢ç‹€: (18, 1)\n",
      "2025-09-29 15:50:31,872 - INFO - æ·»åŠ å­ç¯€é»: 8e8199c9 to e67d99b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æˆåŠŸå°‡çµæœå¯«å…¥ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:33,746 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:50:33,747 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:50:33,748 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: value_counts(type)']\n",
      "2025-09-29 15:50:33,749 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(getpoint_player)\n",
      "2025-09-29 15:50:33,751 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts() æ“ä½œå·²æˆåŠŸåŸ·è¡Œï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:34,401 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:50:34,402 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(count)', 'write()']\n",
      "2025-09-29 15:50:37,482 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 1)\n",
      "2025-09-29 15:50:37,483 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(count), çµæœå½¢ç‹€: (2, 1)\n",
      "2025-09-29 15:50:37,484 - INFO - æ·»åŠ å­ç¯€é»: 3770f29b to 5c4a956c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully processed and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:50:38,631 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:50:38,632 - INFO - æ·»åŠ å­ç¯€é»: 3b4e895a to 5c4a956c\n",
      "2025-09-29 15:50:38,633 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(player)\n",
      "2025-09-29 15:50:38,636 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:50:39,218 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:50:39,219 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(count)', 'write()']\n",
      "2025-09-29 15:50:41,965 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (2, 1)\n",
      "2025-09-29 15:50:41,966 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(count), çµæœå½¢ç‹€: (2, 1)\n",
      "2025-09-29 15:50:41,967 - INFO - æ·»åŠ å­ç¯€é»: 4b781957 to fd387980\n",
      "2025-09-29 15:50:42,034 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 18.42660804s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "2025-09-29 15:50:42,035 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully selected column 'count' and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:51:13,250 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:13,253 - INFO - æ·»åŠ å­ç¯€é»: 6dcccdbd to fd387980\n",
      "2025-09-29 15:51:13,255 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-09-29 15:51:13,261 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:51:13,961 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:13,962 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 15:51:15,235 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:15,236 - INFO - æ·»åŠ å­ç¯€é»: ed4afb4a to e4f7540c\n",
      "2025-09-29 15:51:15,237 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(getpoint_player)\n",
      "2025-09-29 15:51:15,239 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:51:15,806 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:15,808 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 15:51:16,623 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:16,624 - INFO - æ·»åŠ å­ç¯€é»: 792193fb to 2539f1c4\n",
      "2025-09-29 15:51:16,624 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: select_column(type)\n",
      "2025-09-29 15:51:16,626 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:51:17,253 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:17,256 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 15:51:18,886 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:18,887 - INFO - æ·»åŠ å­ç¯€é»: 39dcc001 to 8e8199c9\n",
      "2025-09-29 15:51:18,888 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-09-29 15:51:18,890 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:51:19,440 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:19,441 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)']\n",
      "2025-09-29 15:51:21,524 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:51:21,525 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 15:51:21,526 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: value_counts(type)']\n",
      "2025-09-29 15:51:21,526 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: select_column(count)\n",
      "2025-09-29 15:51:21,528 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:51:22,236 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:22,238 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 15:51:23,564 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:23,565 - INFO - æ·»åŠ å­ç¯€é»: c88576a2 to 3770f29b\n",
      "2025-09-29 15:51:23,566 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: select_column(count)\n",
      "2025-09-29 15:51:23,567 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 15:51:24,060 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 15:51:24,061 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 15:51:25,223 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:25,226 - INFO - æ·»åŠ å­ç¯€é»: 7879c583 to 4b781957\n",
      "2025-09-29 15:51:26,810 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 15:51:26,812 - INFO - æ·»åŠ å­ç¯€é»: 4fd755a1 to f32472b3\n",
      "2025-09-29 15:51:26,813 - INFO - ç¯€é» ed4afb4a æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table: Empty DataFrame\n",
      "Columns: [type, count]\n",
      "Index: []\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±è¼ƒå¤šï¼Œå¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼å‡ä¸æ˜é¡¯ï¼Œçƒå“¡è¡¨ç¾ä¹Ÿé›£ä»¥è©•ä¼°ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:51:27,532 - INFO - ç¯€é» e4f7540c æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:51:27,533 - INFO - ç¯€é» 792193fb æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table: Empty DataFrame\n",
      "Columns: [type, count]\n",
      "Index: []\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±è¼ƒå¤šï¼Œå¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼å‡ä¸æ˜é¡¯ï¼Œçƒå“¡è¡¨ç¾ä¹Ÿé›£ä»¥è©•ä¼°ã€‚\n",
      "node.table: Empty DataFrame\n",
      "Columns: [getpoint_player, count]\n",
      "Index: []\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±ï¼Œæš«ç„¡å¾—åˆ†æ¨¡å¼æˆ–çƒå“¡äº®é»å¯ä¾›åˆ†æã€‚æœŸå¾…å¾ŒçºŒæ¯”è³½èƒ½æœ‰æ›´å¤šç²¾å½©æ•¸æ“šå‘ˆç¾ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:51:28,258 - INFO - ç¯€é» 2539f1c4 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:51:28,328 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 32.137442598s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "2025-09-29 15:51:28,329 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table: Empty DataFrame\n",
      "Columns: [getpoint_player, count]\n",
      "Index: []\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±ï¼Œæš«ç„¡å¾—åˆ†æ¨¡å¼æˆ–çƒå“¡äº®é»å¯ä¾›åˆ†æã€‚æœŸå¾…å¾ŒçºŒæ¯”è³½èƒ½æœ‰æ›´å¤šç²¾å½©æ•¸æ“šå‘ˆç¾ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:51:59,920 - INFO - ç¯€é» 94f1f6c1 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:51:59,922 - INFO - ç¯€é» 39dcc001 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table: Empty DataFrame\n",
      "Columns: [Unnamed: 0, roundscore_A, roundscore_B, player, getpoint_player, type, rally, time]\n",
      "Index: []\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±è¼ƒå¤šï¼Œå¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼å‡ä¸æ˜é¡¯ï¼Œçƒå“¡è¡¨ç¾ä¹Ÿé›£ä»¥è©•ä¼°ï¼Œæš«ç„¡å¾—åˆ†æ¨¡å¼æˆ–çƒå“¡äº®é»å¯ä¾›åˆ†æã€‚æœŸå¾…å¾ŒçºŒæ¯”è³½èƒ½æœ‰æ›´å¤šç²¾å½©æ•¸æ“šå‘ˆç¾ã€‚\n",
      "node.table:      type\n",
      "0      é•·çƒ\n",
      "1      æ®ºçƒ\n",
      "2      æŒ‘çƒ\n",
      "3      åˆ‡çƒ\n",
      "4      æ¨çƒ\n",
      "5     æ”¾å°çƒ\n",
      "6     æ“‹å°çƒ\n",
      "7    æœªçŸ¥çƒç¨®\n",
      "8      å‹¾çƒ\n",
      "9     ç™¼é•·çƒ\n",
      "10    ç™¼çŸ­çƒ\n",
      "11  å¾Œå ´æŠ½å¹³çƒ\n",
      "12   éåº¦åˆ‡çƒ\n",
      "13   é˜²å®ˆå›æŠ½\n",
      "14     æ’²çƒ\n",
      "15     é»æ‰£\n",
      "16   é˜²å®ˆå›æŒ‘\n",
      "17     å¹³çƒ\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­ï¼Œå¸¸è¦‹çƒè·¯åŒ…æ‹¬é•·çƒã€æ®ºçƒèˆ‡æŒ‘çƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±æ–¼ã€Œç„¡è³‡æ–™ã€å°è‡´çš„å¤±åˆ†æƒ…æ³è¼ƒå¤šï¼Œè€ŒæœªçŸ¥çƒå“¡å»æ˜¯å¾—åˆ†çš„ä¸»è¦ä¾†æºï¼Œé€™æˆ–è¨±æš—ç¤ºè‘—æ¯”è³½ä¸­å­˜åœ¨ä¸€äº›é›£ä»¥é æ¸¬çš„è®Šæ•¸ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:01,368 - INFO - ç¯€é» 8e8199c9 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type\n",
      "0      é•·çƒ\n",
      "1      æ®ºçƒ\n",
      "2      æŒ‘çƒ\n",
      "3      åˆ‡çƒ\n",
      "4      æ¨çƒ\n",
      "5     æ”¾å°çƒ\n",
      "6     æ“‹å°çƒ\n",
      "7    æœªçŸ¥çƒç¨®\n",
      "8      å‹¾çƒ\n",
      "9     ç™¼é•·çƒ\n",
      "10    ç™¼çŸ­çƒ\n",
      "11  å¾Œå ´æŠ½å¹³çƒ\n",
      "12   éåº¦åˆ‡çƒ\n",
      "13   é˜²å®ˆå›æŠ½\n",
      "14     æ’²çƒ\n",
      "15     é»æ‰£\n",
      "16   é˜²å®ˆå›æŒ‘\n",
      "17     å¹³çƒ\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­å¸¸è¦‹é•·çƒã€æ®ºçƒèˆ‡æŒ‘çƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ã€Œç„¡è³‡æ–™ã€å°è‡´å¤±åˆ†è¼ƒå¤šï¼ŒæœªçŸ¥çƒå“¡å»æ˜¯ä¸»è¦å¾—åˆ†ä¾†æºï¼Œæš—ç¤ºæ¯”è³½ä¸­å­˜åœ¨é›£ä»¥é æ¸¬çš„è®Šæ•¸ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:02,525 - INFO - ç¯€é» e67d99b0 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:52:02,527 - INFO - ç¯€é» c88576a2 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­å¸¸è¦‹é•·çƒã€æ®ºçƒèˆ‡æŒ‘çƒç­‰æŠ€è¡“é‹ç”¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ã€Œç„¡è³‡æ–™ã€å°è‡´å¤±åˆ†è¼ƒå¤šï¼Œè€ŒæœªçŸ¥çƒå“¡å»æ˜¯ä¸»è¦å¾—åˆ†ä¾†æºï¼Œæš—ç¤ºæ¯”è³½ä¸­å­˜åœ¨é›£ä»¥é æ¸¬çš„è®Šæ•¸ã€‚\n",
      "node.table:    count\n",
      "0     21\n",
      "1     15\n",
      "ç¯€é»æ–‡æœ¬: ç›®å‰æ¯”åˆ†ï¼ŒAéšŠ21åˆ†ï¼ŒBéšŠ15åˆ†ã€‚AéšŠæŒçºŒé ˜å…ˆï¼Œä½†ä»éœ€å°å¿ƒæ‡‰å°ï¼ŒæŠŠæ¡æ¯ä¸€æ¬¡å¾—åˆ†æ©Ÿæœƒã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:03,282 - INFO - ç¯€é» 3770f29b æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:52:03,285 - INFO - ç¯€é» 3b4e895a æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:    count\n",
      "0     21\n",
      "1     15\n",
      "ç¯€é»æ–‡æœ¬: ç›®å‰æ¯”åˆ†AéšŠ21åˆ†ï¼ŒBéšŠ15åˆ†ã€‚AéšŠæŒçºŒé ˜å…ˆï¼Œä»éœ€å°å¿ƒæ‡‰å°ï¼ŒæŠŠæ¡æ¯ä¸€æ¬¡å¾—åˆ†æ©Ÿæœƒï¼Œç©©ä½å„ªå‹¢ã€‚\n",
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: Aé¸æ‰‹åœ¨æ¯”è³½ä¸­å¾—åˆ†æ¬¡æ•¸é«˜é”21æ¬¡ï¼Œç›¸è¼ƒä¹‹ä¸‹ï¼ŒBé¸æ‰‹åƒ…æœ‰15æ¬¡å¾—åˆ†ã€‚Aé¸æ‰‹çš„é€²æ”»æ›´å…·å¨è„…æ€§ï¼Œæ˜¯æœ¬å ´æ¯”è³½çš„äº®é»ä¹‹ä¸€ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:04,390 - INFO - ç¯€é» 5c4a956c æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:52:04,393 - INFO - ç¯€é» 7879c583 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   getpoint_player  count\n",
      "0               A     21\n",
      "1               B     15\n",
      "ç¯€é»æ–‡æœ¬: AéšŠç›®å‰ä»¥21:15é ˜å…ˆBéšŠï¼Œä½†ä»éœ€è¬¹æ…ï¼ŒæŠŠæ¡å¾—åˆ†æ©Ÿæœƒä»¥ç©©å›ºå„ªå‹¢ã€‚Aé¸æ‰‹è¡¨ç¾äº®çœ¼ï¼Œå¾—åˆ†é«˜é”21æ¬¡ï¼Œé€²æ”»æ›´å…·å¨è„…æ€§ï¼Œè€ŒBé¸æ‰‹åƒ…å¾—15åˆ†ã€‚\n",
      "node.table:    count\n",
      "0    158\n",
      "1    157\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹äº’æœ‰æ”»å®ˆï¼Œå„æœ‰150é¤˜æ¬¡æ”»é˜²å›åˆï¼Œä½†å…·é«”å¾—åˆ†æ‰‹æ®µå°šä¸æ˜ç¢ºï¼Œä»éœ€æ›´å¤šæ•¸æ“šåˆ†ææ‰èƒ½é–å®šå‹è² é—œéµã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:05,163 - INFO - ç¯€é» 4b781957 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:52:05,165 - INFO - ç¯€é» 6dcccdbd æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:    count\n",
      "0    158\n",
      "1    157\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹æ”»é˜²æ¿€çƒˆï¼Œå…±æœ‰è¶…é150æ¬¡æ”»é˜²å›åˆã€‚ç„¶è€Œï¼Œå…·é«”çš„å¾—åˆ†æ–¹å¼å°šä¸æ˜ç¢ºï¼Œéœ€è¦é€²ä¸€æ­¥çš„æ•¸æ“šåˆ†æä¾†ç¢ºå®šå‹è² çš„é—œéµå› ç´ ã€‚\n",
      "node.table:   player  count\n",
      "0      B    158\n",
      "1      A    157\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½ï¼ŒBé¸æ‰‹çš„æ“Šçƒæ¬¡æ•¸ç¨å¤šï¼Œå…±158æ¬¡ï¼Œè€ŒAé¸æ‰‹ä¹Ÿä¸é‘å¤šè®“ï¼Œæœ‰157æ¬¡æ“Šçƒã€‚é›™æ–¹åœ¨å ´ä¸Šä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:06,239 - INFO - ç¯€é» fd387980 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   player  count\n",
      "0      B    158\n",
      "1      A    157\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½é›™æ–¹æ”»é˜²æ¿€çƒˆï¼Œå…±æœ‰è¶…é150æ¬¡æ”»é˜²å›åˆã€‚Bé¸æ‰‹æ“Šçƒ158æ¬¡ï¼ŒAé¸æ‰‹157æ¬¡ï¼Œå¯è¦‹é›™æ–¹äº’ä¸ç›¸è®“ã€‚å…·é«”å¾—åˆ†æ–¹å¼å°šä¸æ˜ç¢ºï¼Œéœ€é€²ä¸€æ­¥æ•¸æ“šåˆ†æä»¥ç¢ºå®šå‹è² é—œéµã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:07,808 - INFO - ç¯€é» c659592a æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 15:52:07,814 - INFO - ç”Ÿæˆæœ€çµ‚å ±å‘Š...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      Unnamed: 0  roundscore_A  roundscore_B player getpoint_player  type  \\\n",
      "0             0             1             0      B             NaN   ç™¼é•·çƒ   \n",
      "1             1             1             0      A             NaN    åˆ‡çƒ   \n",
      "2             2             1             0      B             NaN    æŒ‘çƒ   \n",
      "3             3             1             0      A             NaN    é•·çƒ   \n",
      "4             4             1             0      B             NaN    æ®ºçƒ   \n",
      "..          ...           ...           ...    ...             ...   ...   \n",
      "310         310            21            15      B             NaN  æœªçŸ¥çƒç¨®   \n",
      "311         311            21            15      A             NaN    åˆ‡çƒ   \n",
      "312         312            21            15      B             NaN    æŒ‘çƒ   \n",
      "313         313            21            15      A             NaN    é•·çƒ   \n",
      "314         314            21            15      B               A    é•·çƒ   \n",
      "\n",
      "     rally      time  \n",
      "0        1  00:05:47  \n",
      "1        1  00:05:49  \n",
      "2        1  00:05:50  \n",
      "3        1  00:05:51  \n",
      "4        1  00:05:52  \n",
      "..     ...       ...  \n",
      "310     36  00:24:44  \n",
      "311     36  00:24:58  \n",
      "312     36  00:25:00  \n",
      "313     36  00:25:01  \n",
      "314     36  00:25:02  \n",
      "\n",
      "[315 rows x 8 columns]\n",
      "ç¯€é»æ–‡æœ¬: è³‡æ–™åˆ†æå ±å‘Š\n",
      "\n",
      "æœ¬å ´æ¯”è³½æ•¸æ“šç¼ºå¤±è¼ƒå¤šï¼Œå¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼ä¸æ˜é¡¯ï¼Œçƒå“¡è¡¨ç¾ä¹Ÿé›£ä»¥è©•ä¼°ï¼Œæš«ç„¡å¾—åˆ†æ¨¡å¼æˆ–çƒå“¡äº®é»å¯ä¾›åˆ†æã€‚æ¯”è³½ä¸­å¸¸è¦‹é•·çƒã€æ®ºçƒèˆ‡æŒ‘çƒç­‰æŠ€è¡“é‹ç”¨ï¼Œä½†å› ã€Œç„¡è³‡æ–™ã€å°è‡´å¤±åˆ†è¼ƒå¤šï¼ŒæœªçŸ¥çƒå“¡å»æ˜¯ä¸»è¦å¾—åˆ†ä¾†æºï¼Œæš—ç¤ºæ¯”è³½ä¸­å­˜åœ¨é›£ä»¥é æ¸¬çš„è®Šæ•¸ã€‚AéšŠç›®å‰ä»¥21:15é ˜å…ˆBéšŠï¼Œä½†ä»éœ€è¬¹æ…ï¼ŒæŠŠæ¡å¾—åˆ†æ©Ÿæœƒä»¥ç©©å›ºå„ªå‹¢ã€‚Aé¸æ‰‹è¡¨ç¾äº®çœ¼ï¼Œå¾—åˆ†é«˜é”21æ¬¡ï¼Œé€²æ”»æ›´å…·å¨è„…æ€§ï¼Œè€ŒBé¸æ‰‹åƒ…å¾—15åˆ†ã€‚é›™æ–¹æ”»é˜²æ¿€çƒˆï¼Œå…±æœ‰è¶…é150æ¬¡æ”»é˜²å›åˆã€‚Bé¸æ‰‹æ“Šçƒ158æ¬¡ï¼ŒAé¸æ‰‹157æ¬¡ï¼Œå¯è¦‹é›™æ–¹äº’ä¸ç›¸è®“ã€‚å…·é«”å¾—åˆ†æ–¹å¼å°šä¸æ˜ç¢ºï¼Œéœ€é€²ä¸€æ­¥æ•¸æ“šåˆ†æä»¥ç¢ºå®šå‹è² é—œéµã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 15:52:11,708 - INFO - æ¨¹çµæ§‹å·²å°å‡ºè‡³: tree_structure.json\n",
      "2025-09-29 15:52:11,710 - INFO - \n",
      "==================================================\n",
      "2025-09-29 15:52:11,710 - INFO - TREE-OF-REPORT æœ€çµ‚å ±å‘Š\n",
      "2025-09-29 15:52:11,712 - INFO - ==================================================\n",
      "2025-09-29 15:52:11,713 - INFO - å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: 121.17 ç§’\n",
      "2025-09-29 15:52:11,714 - INFO - ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "2025-09-29 15:52:11,715 - INFO - - tree_of_report.md: æœ€çµ‚å ±å‘Š\n",
      "2025-09-29 15:52:11,715 - INFO - - tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\n",
      "2025-09-29 15:52:11,716 - INFO - - tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\n",
      "2025-09-29 15:52:11,717 - INFO - - execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\n",
      "2025-09-29 15:52:11,718 - INFO - - tree_visualization.html: å¯è¦–åŒ–é é¢\n",
      "2025-09-29 15:52:11,720 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: input_tmp.csv\n",
      "2025-09-29 15:52:11,721 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: tmp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish generate report\n",
      "## ç¾½çƒæ¿€æˆ°æ­£é…£ï¼AéšŠæš«é ˜å…ˆï¼Œå‹è² ä»å­˜è®Šæ•¸\n",
      "\n",
      "**ï¼ˆæœ¬å ±è¨Šï¼‰** ä¸€å ´æ¿€çƒˆçš„ç¾½çƒå°æ±ºæ­£åœ¨ä¸Šæ¼”ï¼ŒAéšŠç›®å‰ä»¥21:15æš«æ™‚é ˜å…ˆBéšŠã€‚å„˜ç®¡æ¯”è³½æ•¸æ“šç•¥æœ‰ç¼ºå¤±ï¼Œé›£ä»¥å®Œå…¨æŒæ¡å ´ä¸Šç¬æ¯è¬è®Šçš„å±€å‹¢ï¼Œä½†é›™æ–¹ä½ ä¾†æˆ‘å¾€çš„æ”»é˜²ï¼Œå·²è®“è§€çœ¾çœ‹å¾—ç›®ä¸æš‡çµ¦ã€‚\n",
      "\n",
      "å¾æœ‰é™çš„æ•¸æ“šä¾†çœ‹ï¼Œé€™å ´æ¯”è³½å……æ»¿äº†é•·çƒã€æ®ºçƒå’ŒæŒ‘çƒç­‰ç¾½çƒåŸºæœ¬æŠ€è¡“çš„é‹ç”¨ã€‚ç„¶è€Œï¼Œã€Œç„¡è³‡æ–™ã€å¤±åˆ†çš„æƒ…æ³é »ç¹å‡ºç¾ï¼Œæš—ç¤ºå ´ä¸Šå­˜åœ¨è‘—è¨±å¤šé›£ä»¥é æ¸¬çš„è®Šæ•¸ï¼Œä¹Ÿè®“æ¯”è³½æ›´æ·»æ‡¸å¿µã€‚ä»¤äººæ„å¤–çš„æ˜¯ï¼Œæ•¸æ“šä¸­ã€ŒæœªçŸ¥çƒå“¡ã€å»æ˜¯ä¸»è¦å¾—åˆ†ä¾†æºï¼Œæ›´å‡¸é¡¯äº†æ¯”è³½çš„è¤‡é›œæ€§ã€‚\n",
      "\n",
      "AéšŠé¸æ‰‹è¡¨ç¾çªå‡ºï¼Œä»¥ç²¾æ¹›çš„çƒæŠ€å’Œæ›´å…·å¨è„…æ€§çš„é€²æ”»ï¼Œç¨æ”¬21åˆ†ï¼Œæˆç‚ºçƒéšŠé ˜å…ˆçš„é—œéµäººç‰©ã€‚åè§€BéšŠé¸æ‰‹ï¼Œé›–å¥®åŠ›è¿½è¶•ï¼Œä½†ç›®å‰åƒ…æ‹¿ä¸‹15åˆ†ã€‚\n",
      "\n",
      "å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œé€™å ´æ¯”è³½çš„æ”»é˜²æ¥µç‚ºæ¿€çƒˆï¼Œé›™æ–¹å…±è¨ˆé€²è¡Œäº†è¶…é150æ¬¡æ”»é˜²å›åˆï¼Œå¯è¦‹æˆ°æ³ä¹‹è† è‘—ã€‚BéšŠé¸æ‰‹å…¨å ´å…±æ“Šçƒ158æ¬¡ï¼ŒAéšŠé¸æ‰‹ä¹Ÿä¸ç”˜ç¤ºå¼±ï¼Œæ“Šçƒ157æ¬¡ï¼Œå……åˆ†å±•ç¾äº†é›™æ–¹äº’ä¸ç›¸è®“çš„æ±ºå¿ƒã€‚\n",
      "\n",
      "é›–ç„¶AéšŠç›®å‰æ¡æœ‰é ˜å…ˆå„ªå‹¢ï¼Œä½†æ¯”è³½å°šæœªçµæŸï¼Œå‹è² ä»å……æ»¿è®Šæ•¸ã€‚AéšŠå¿…é ˆä¿æŒè¬¹æ…ï¼Œç‰¢ç‰¢æŠŠæ¡æ¯ä¸€å€‹å¾—åˆ†æ©Ÿæœƒï¼Œæ‰èƒ½ç©©å›ºå„ªå‹¢ï¼Œæœ€çµ‚è´å¾—å‹åˆ©ã€‚è€ŒBéšŠè‹¥èƒ½æ‰¾å‡ºçªç ´é»ï¼Œå°‡æœ‰æ©Ÿæœƒé€†è½‰å±€å‹¢ã€‚ç©¶ç«Ÿé¹¿æ­»èª°æ‰‹ï¼Œè®“æˆ‘å€‘æ‹­ç›®ä»¥å¾…ï¼\n",
      "\n",
      "**ï¼ˆå¾ŒçºŒå ±å°å°‡æŒçºŒé—œæ³¨æ¯”è³½é€²å±•ï¼Œä¸¦å˜—è©¦å–å¾—æ›´è©³ç›¡çš„æ•¸æ“šï¼Œä»¥æ·±å…¥åˆ†æé›™æ–¹çš„å¾—åˆ†æ¨¡å¼èˆ‡å‹è² é—œéµã€‚ï¼‰**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import builtins\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„å‡½æ•¸ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # è¿”å›é»˜èªæ“ä½œé›†åˆ\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# ===== æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ =====\n",
    "class TreeNode:\n",
    "    \"\"\"æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ï¼Œå¢åŠ èªæ„é©—è­‰å’Œè¿½è¹¤åŠŸèƒ½\"\"\"\n",
    "    def __init__(self, level: int = 0, text: str = \"\", table: pd.DataFrame = None, operation: str = None):\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.level: int = level\n",
    "        self.text: str = text\n",
    "        self.table: pd.DataFrame = table if table is not None else pd.DataFrame()\n",
    "        self.operation: str = operation\n",
    "        self.parent: Optional['TreeNode'] = None\n",
    "        self.operation_history: List[str] = []\n",
    "        \n",
    "        # æ–°å¢å±¬æ€§ç”¨æ–¼æ”¹é€²åŠŸèƒ½\n",
    "        self.node_id: str = self._generate_node_id()\n",
    "        self.created_at: datetime = datetime.now()\n",
    "        self.validation_errors: List[str] = []\n",
    "        self.table_hash: str = self._calculate_table_hash()\n",
    "        self.semantic_score: float = 0.0\n",
    "        \n",
    "    def _generate_node_id(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆå”¯ä¸€ç¯€é»ID\"\"\"\n",
    "        content = f\"{self.level}_{self.operation}_{datetime.now().isoformat()}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        \n",
    "    def _calculate_table_hash(self) -> str:\n",
    "        \"\"\"è¨ˆç®—è¡¨æ ¼å…§å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨æ–¼æª¢æ¸¬é‡è¤‡\"\"\"\n",
    "        if self.table.empty:\n",
    "            return \"\"\n",
    "        try:\n",
    "            return hashlib.md5(str(self.table.values.tobytes()).encode()).hexdigest()[:8]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def add_child(self, child: 'TreeNode'):\n",
    "        \"\"\"æ·»åŠ å­ç¯€é»ä¸¦é€²è¡Œé©—è­‰\"\"\"\n",
    "        if self._validate_child(child):\n",
    "            child.parent = self\n",
    "            self.children.append(child)\n",
    "            logger.info(f\"æ·»åŠ å­ç¯€é»: {child.node_id} to {self.node_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"å­ç¯€é»é©—è­‰å¤±æ•—: {child.validation_errors}\")\n",
    "    \n",
    "    def _validate_child(self, child: 'TreeNode') -> bool:\n",
    "        \"\"\"é©—è­‰å­ç¯€é»çš„åˆç†æ€§\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„è¡¨æ ¼ç‹€æ…‹\n",
    "        if child.table_hash and child.table_hash == self.table_hash:\n",
    "            if not child.operation.lower().startswith('write'):\n",
    "                errors.append(\"è¡¨æ ¼å…§å®¹æœªç™¼ç”Ÿè®ŠåŒ–ä½†éå¯«ä½œæ“ä½œ\")\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œæ˜¯å¦é‚è¼¯åˆç†\n",
    "        if self._is_redundant_operation(child.operation):\n",
    "            errors.append(f\"æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: {child.operation}\")\n",
    "        \n",
    "        child.validation_errors = errors\n",
    "        return len(errors) == 0\n",
    "    \n",
    "    def _is_redundant_operation(self, operation: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æ“ä½œæ˜¯å¦å†—é¤˜\"\"\"\n",
    "        if len(self.operation_history) < 2:\n",
    "            return False\n",
    "            \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ“ä½œåœ¨è¿‘æœŸæ­·å²ä¸­\n",
    "        recent_ops = self.operation_history[-3:]  # æª¢æŸ¥æœ€è¿‘3å€‹æ“ä½œ\n",
    "        op_name = operation.split('(')[0].lower()\n",
    "        \n",
    "        for hist_op in recent_ops:\n",
    "            if hist_op.split('(')[0].lower() == op_name:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºè‘‰ç¯€é»\"\"\"\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"å°‡ç¯€é»è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self.node_id,\n",
    "            \"level\": self.level,\n",
    "            \"operation\": self.operation,\n",
    "            \"text_preview\": self.text[:100] + \"...\" if len(self.text) > 100 else self.text,\n",
    "            \"table_shape\": list(self.table.shape) if not self.table.empty else [0, 0],\n",
    "            \"table_columns\": list(self.table.columns) if not self.table.empty else [],\n",
    "            \"children_count\": len(self.children),\n",
    "            \"validation_errors\": self.validation_errors,\n",
    "            \"semantic_score\": self.semantic_score,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"table_hash\": self.table_hash\n",
    "        }\n",
    "\n",
    "# ===== æ”¹é€²çš„æ“ä½œè§£æå™¨ =====\n",
    "class OperationParser:\n",
    "    \"\"\"å°ˆé–€è² è²¬è§£æå’Œé©—è­‰æ“ä½œçš„é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self, valid_operations: Set[str]):\n",
    "        \"\"\"åˆå§‹åŒ–ï¼Œæ¥å—å¾ JSON è®€å–çš„æœ‰æ•ˆæ“ä½œé›†åˆ\"\"\"\n",
    "        self.valid_operations = valid_operations\n",
    "        logger.info(f\"OperationParser åˆå§‹åŒ–ï¼Œæœ‰æ•ˆæ“ä½œ: {self.valid_operations}\")\n",
    "        \n",
    "    def parse_operations(self, response_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"æ”¹é€²çš„æ“ä½œè§£æï¼Œè¿”å›çµæ§‹åŒ–çµæœ\"\"\"\n",
    "        try:\n",
    "            parsed_operations = []\n",
    "            \n",
    "            # å¤šç¨®è§£æç­–ç•¥\n",
    "            operations = self._extract_operations_multiple_strategies(response_text)\n",
    "            \n",
    "            for op_str in operations:\n",
    "                parsed_op = self._parse_single_operation(op_str)\n",
    "                if parsed_op and self._validate_operation(parsed_op):\n",
    "                    parsed_operations.append(parsed_op)\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: {op_str}\")\n",
    "            \n",
    "            return parsed_operations[:5]  # é™åˆ¶æœ€å¤š5å€‹æ“ä½œ\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œå¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_operations_multiple_strategies(self, text: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨å¤šç¨®ç­–ç•¥æå–æ“ä½œ\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç­–ç•¥1: å°‹æ‰¾æ–¹æ‹¬è™Ÿå…§å®¹\n",
    "        bracket_match = re.search(r'\\[(.*?)\\]', text, re.DOTALL)\n",
    "        if bracket_match:\n",
    "            content = bracket_match.group(1)\n",
    "            # ä½¿ç”¨æ­£å‰‡æå–å‡½æ•¸èª¿ç”¨æ ¼å¼\n",
    "            pattern = r'([a-zA-Z_]+\\([^)]*\\))'\n",
    "            ops = re.findall(pattern, content)\n",
    "            operations.extend(ops)\n",
    "        \n",
    "        # ç­–ç•¥2: é€è¡Œè§£æ\n",
    "        if not operations:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '(' in line and ')' in line:\n",
    "                    operations.append(line)\n",
    "        \n",
    "        # ç­–ç•¥3: é€—è™Ÿåˆ†å‰²\n",
    "        if not operations:\n",
    "            parts = text.replace('[', '').replace(']', '').split(',')\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part and '(' in part:\n",
    "                    operations.append(part)\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def _parse_single_operation(self, op_str: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"è§£æå–®å€‹æ“ä½œå­—ç¬¦ä¸²\"\"\"\n",
    "        try:\n",
    "            # ç§»é™¤å¤šé¤˜çš„å­—ç¬¦\n",
    "            op_str = op_str.strip().rstrip(',').strip()\n",
    "            \n",
    "            # æå–æ“ä½œåç¨±å’Œåƒæ•¸\n",
    "            if '(' not in op_str:\n",
    "                return {\"name\": op_str, \"args\": [], \"raw\": op_str}\n",
    "            \n",
    "            name_part = op_str.split('(')[0].strip()\n",
    "            args_part = op_str[op_str.find('(')+1:op_str.rfind(')')].strip()\n",
    "            \n",
    "            # è§£æåƒæ•¸\n",
    "            args = []\n",
    "            if args_part:\n",
    "                # ç°¡å–®çš„åƒæ•¸åˆ†å‰²ï¼ˆå¯ä»¥é€²ä¸€æ­¥æ”¹é€²ï¼‰\n",
    "                for arg in args_part.split(','):\n",
    "                    arg = arg.strip().strip('\\'\"')\n",
    "                    if arg:\n",
    "                        args.append(arg)\n",
    "            \n",
    "            return {\n",
    "                \"name\": name_part.lower(),\n",
    "                \"args\": args,\n",
    "                \"raw\": op_str\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œ '{op_str}' å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_operation(self, operation: Dict[str, Any]) -> bool:\n",
    "        \"\"\"é©—è­‰æ“ä½œçš„æœ‰æ•ˆæ€§\"\"\"\n",
    "        name = operation.get(\"name\", \"\").lower()\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆï¼ˆå¾ JSON è¼‰å…¥çš„æ“ä½œï¼‰\n",
    "        if name not in self.valid_operations:\n",
    "            logger.warning(f\"æœªçŸ¥æ“ä½œ: {name}ï¼Œä¸åœ¨æ“ä½œæ± ä¸­\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥ç‰¹å®šæ“ä½œçš„åƒæ•¸\n",
    "        args = operation.get(\"args\", [])\n",
    "        \n",
    "        if name in ['select_column', 'sort_values', 'groupby'] and not args:\n",
    "            logger.warning(f\"{name} æ“ä½œéœ€è¦åƒæ•¸\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ===== æ”¹é€²çš„å…§å®¹è¦åŠƒå™¨ =====\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key, operations_config: Dict[str, Any]):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        \n",
    "        # å¾é…ç½®ä¸­æå–æ“ä½œä¿¡æ¯\n",
    "        self.operations_config = operations_config\n",
    "        valid_ops = self._extract_valid_operations(operations_config)\n",
    "        self.parser = OperationParser(valid_ops)\n",
    "        \n",
    "    def _extract_valid_operations(self, config: Dict[str, Any]) -> Set[str]:\n",
    "        \"\"\"å¾é…ç½®ä¸­æå–æœ‰æ•ˆæ“ä½œåç¨±\"\"\"\n",
    "        valid_ops = set()\n",
    "        if isinstance(config, dict) and \"operations\" in config:\n",
    "            for op in config[\"operations\"]:\n",
    "                if \"operation\" in op:\n",
    "                    valid_ops.add(op[\"operation\"].lower())\n",
    "        elif isinstance(config, list):\n",
    "            for op in config:\n",
    "                if isinstance(op, dict) and \"operation\" in op:\n",
    "                    valid_ops.add(op[\"operation\"].lower())\n",
    "        \n",
    "        logger.info(f\"å¾é…ç½®æå–çš„æœ‰æ•ˆæ“ä½œ: {valid_ops}\")\n",
    "        return valid_ops\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        æ”¹é€²çš„æ“ä½œç”Ÿæˆï¼ŒåŠ å…¥é‡è¤‡æª¢æ¸¬å’Œèªæ„é©—è­‰\n",
    "        \"\"\"\n",
    "        \n",
    "        # æª¢æ¸¬è¿‘æœŸæ“ä½œï¼Œé¿å…é‡è¤‡\n",
    "        recent_operations = self._extract_recent_operations(operation_history)\n",
    "        \n",
    "        # æ ¼å¼åŒ–æ“ä½œæè¿°ï¼ˆä½¿ç”¨ selected_operations.json çš„æ ¼å¼ï¼‰\n",
    "        formatted_operations = self._format_operations_for_prompt(operation_description)\n",
    "        \n",
    "        # æ§‹å»ºæ”¹é€²çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the badminton game report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History.\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements.\n",
    "2. The output must be in English.\n",
    "3. The output must be based on the input data; do not hallucinate.\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree} and more than zero.\n",
    "6. Only select Operations from the Operation Pool.\n",
    "7. Arguments must match the format required by the corresponding Operations.\n",
    "8. Operations & Arguments must follow this format: [operation_1(argument_1, ...), operation_2(argument_2, ...), operation_3(argument_3, ...), ...]\n",
    "9. Only output Operations & Arguments!\n",
    "10. If Table is big or Level is low, it should be more Operations include select_column or groupby not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "13. AVOID repeating recent operations: {recent_operations}\n",
    "14. Prioritize operations that will meaningfully transform the data.\n",
    "15. Arguments must be valid column names from the table.\n",
    "\n",
    "# Outline\n",
    "{read_text_file(outline_path) if os.path.exists(outline_path) else \"Generate comprehensive badminton data analysis\"}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Available Operations\n",
    "{formatted_operations}\n",
    "\n",
    "User: # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                logger.info(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                parsed_ops = self.parser.parse_operations(response.text.strip())\n",
    "                return [op[\"raw\"] for op in parsed_ops]  # è¿”å›åŸå§‹å­—ç¬¦ä¸²æ ¼å¼\n",
    "            else:\n",
    "                logger.warning(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _format_operations_for_prompt(self, operation_description) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–æ“ä½œæè¿°ä¾› LLM ä½¿ç”¨\"\"\"\n",
    "        if isinstance(operation_description, dict) and \"operations\" in operation_description:\n",
    "            ops = operation_description[\"operations\"]\n",
    "        elif isinstance(operation_description, list):\n",
    "            ops = operation_description\n",
    "        else:\n",
    "            return str(operation_description)\n",
    "        \n",
    "        formatted = []\n",
    "        for op in ops:\n",
    "            if isinstance(op, dict):\n",
    "                num = op.get(\"number\", \"\")\n",
    "                name = op.get(\"operation\", \"\")\n",
    "                desc = op.get(\"description\", \"\")\n",
    "                formatted.append(f\"{num}. {name}: {desc}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    def _extract_recent_operations(self, operation_history: List[str]) -> List[str]:\n",
    "        \"\"\"æå–æœ€è¿‘çš„æ“ä½œåç¨±\"\"\"\n",
    "        recent = []\n",
    "        for op in operation_history[-3:]:  # æœ€è¿‘3å€‹æ“ä½œ\n",
    "            if '(' in op:\n",
    "                name = op.split('(')[0].strip()\n",
    "                recent.append(name)\n",
    "        return recent\n",
    "\n",
    "# ===== å®‰å…¨çš„DataFrameæ“ä½œå™¨ =====\n",
    "class SafeDataFrameOperator:\n",
    "    \"\"\"å®‰å…¨çš„DataFrameæ“ä½œå™¨ï¼Œä½¿ç”¨ASTé©—è­‰è€Œéç›´æ¥exec\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.allowed_modules = {'pandas', 'numpy', 're'}\n",
    "        self.allowed_functions = {\n",
    "            'pd.read_csv', 'pd.DataFrame', 'df.head', 'df.tail', 'df.sort_values',\n",
    "            'df.groupby', 'df.filter', 'df.select', 'df.drop', 'df.fillna',\n",
    "            'df.to_csv', 'df.value_counts', 'df.describe', 'df.info'\n",
    "        }\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path=\"input_tmp.csv\"):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        1. è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'\n",
    "        2. åªä½¿ç”¨pandasåŸºæœ¬æ“ä½œï¼Œé¿å…è¤‡é›œçš„è‡ªå®šç¾©å‡½æ•¸\n",
    "        3. ç¢ºä¿ä»£ç¢¼å®‰å…¨ï¼Œä¸åŒ…å«æ–‡ä»¶ç³»çµ±æ“ä½œï¼ˆé™¤äº†æŒ‡å®šçš„CSVè®€å¯«ï¼‰\n",
    "        4. æ’°å¯«å®Œæ•´python codeï¼ŒåŒ…å«éŒ¯èª¤è™•ç†\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=2):\n",
    "        \"\"\"å¸¶é‡è©¦çš„ç”Ÿæˆè«‹æ±‚\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    import time\n",
    "                    time.sleep(1)\n",
    "        return \"\"\n",
    "\n",
    "    def safe_execute(self, code: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"å®‰å…¨åŸ·è¡Œç”Ÿæˆçš„ä»£ç¢¼\"\"\"\n",
    "        try:\n",
    "            # æå–ä»£ç¢¼å¡Š\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            #print(f'python code: {code_block}')\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # ASTå®‰å…¨é©—è­‰\n",
    "            if not self._validate_code_safety(code):\n",
    "                logger.error(\"ä»£ç¢¼å®‰å…¨é©—è­‰å¤±æ•—\")\n",
    "                return df\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆ\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            allowed_builtin_names = [\n",
    "                'int', 'float', 'str', 'bool', 'list', 'dict', 'set', 'tuple',\n",
    "                'len', 'range', 'enumerate', 'zip', 'min', 'max', 'sum', 'abs',\n",
    "                'print',\n",
    "                'Exception', 'TypeError', 'ValueError', 'KeyError', 'IndexError',\n",
    "                'FileNotFoundError', 'ZeroDivisionError', 'AttributeError', 'ImportError','__import__'\n",
    "            ]       \n",
    "\n",
    "            safe_globals = {\n",
    "                'pd': pd,\n",
    "                '__name__': '__main__',\n",
    "                '__builtins__': {name: getattr(builtins, name) for name in allowed_builtin_names}\n",
    "            }\n",
    "\n",
    "            safe_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œä»£ç¢¼\n",
    "            exec(code, safe_globals, safe_locals)\n",
    "\n",
    "            # è®€å–çµæœ\n",
    "            if os.path.exists(\"tmp.csv\"):\n",
    "                result_df = pd.read_csv(\"tmp.csv\")\n",
    "                logger.info(f\"æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                return result_df\n",
    "            else:\n",
    "                logger.warning(\"æœªç”Ÿæˆçµæœæ–‡ä»¶ï¼Œè¿”å›åŸå§‹DataFrame\")\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            print(\"éŒ¯èª¤ä»£ç¢¼å¦‚ä¸‹ï¼š\\n\" + \"-\" * 30)\n",
    "            print(code)  # âœ… è¼¸å‡ºé€ æˆéŒ¯èª¤çš„ç¨‹å¼ç¢¼\n",
    "            print(\"-\" * 30)\n",
    "            logger.error(error_msg)\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_code_safety(self, code: str) -> bool:\n",
    "        \"\"\"ä½¿ç”¨ASTé©—è­‰ä»£ç¢¼å®‰å…¨æ€§\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # æª¢æŸ¥å±éšªçš„å‡½æ•¸èª¿ç”¨\n",
    "                if isinstance(node, ast.Call):\n",
    "                    if isinstance(node.func, ast.Name):\n",
    "                        func_name = node.func.id\n",
    "                        if func_name in ['exec', 'eval', 'compile', '__import__', 'open']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°å±éšªå‡½æ•¸: {func_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥æ–‡ä»¶æ“ä½œï¼ˆé™¤äº†å…è¨±çš„CSVæ“ä½œï¼‰\n",
    "                if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
    "                    if hasattr(node.func, 'attr'):\n",
    "                        attr_name = node.func.attr\n",
    "                        if attr_name in ['system', 'popen', 'subprocess']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ç³»çµ±èª¿ç”¨: {attr_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥å°å…¥èªå¥\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for alias in node.names:\n",
    "                        if alias.name not in self.allowed_modules:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ä¸å…è¨±çš„æ¨¡çµ„å°å…¥: {alias.name}\")\n",
    "                            return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ASTé©—è­‰å¤±æ•—: {e}\")\n",
    "            return False\n",
    "\n",
    "# ===== æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "import time\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "\n",
    "    def extract_highlights_from_table(self, table: pd.DataFrame) -> str:\n",
    "        try:\n",
    "            if 'lose_reason' in table.columns:\n",
    "                top_reason = table['lose_reason'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_reason = \"ç„¡è³‡æ–™\"\n",
    "            if 'getpoint_player' in table.columns:\n",
    "                top_player = table['getpoint_player'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_player = \"æœªçŸ¥çƒå“¡\"\n",
    "            return f\"æœ€å¤šå¤±åˆ†åŸå› ç‚ºã€Œ{top_reason}ã€ï¼Œå¾—åˆ†æœ€å¤šçš„æ˜¯ {top_player}ã€‚\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_table_features(self, table: pd.DataFrame) -> str:\n",
    "        summary = []\n",
    "        for col in table.columns:\n",
    "            dtype = str(table[col].dtype)\n",
    "            line = f\"æ¬„ä½ã€Œ{col}ã€é¡å‹ï¼š{dtype}\"\n",
    "\n",
    "            # é¡¯ç¤ºå¸¸è¦‹å€¼åƒ…é™é¡åˆ¥å‹æ¬„ä½\n",
    "            if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
    "                top_values = table[col].value_counts().head(3).to_dict()\n",
    "                line += f\"ï¼Œå¸¸è¦‹å€¼ï¼š{list(top_values.keys())}\"\n",
    "            summary.append(line)\n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=3, delay_seconds=30):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                logger.error(f\"Gemini å›æ‡‰å¤±æ•—: {err}\")\n",
    "                if \"429\" in err:\n",
    "                    logger.info(f\"å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… {delay_seconds} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                else:\n",
    "                    break\n",
    "        return \"âš ï¸ å¯«ä½œè«‹æ±‚å¤±æ•—ï¼šAPI é™åˆ¶æˆ–å…¶ä»–éŒ¯èª¤\"\n",
    "\n",
    "    def generate_text_for_write_operation(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        table_str = table.to_string()\n",
    "        WRITE_TOKENS = 50\n",
    "        TABLE_FORMAT = \"Pandas DataFrame as plain text\"\n",
    "        highlight_summary = self.extract_highlights_from_table(table)\n",
    "        table_feature_summary = self.extract_table_features(table)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a professional content writer for the badminton game report .\n",
    "Please write the Report based on the input Table, just pick one or two lightspots.\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The Table format is {TABLE_FORMAT}.\n",
    "5. The Report can only describe the content included in the Tables and cannot describe anything not included in the Tables .\n",
    "6. The Report must consist of only one paragraph .\n",
    "7. The number of tokens in the Report must be within {WRITE_TOKENS}.\n",
    "8. è«‹å°ˆæ³¨æè¿°å¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼ã€é—œéµæ¬„ä½è¶¨å‹¢æˆ–çƒå“¡äº®é»ã€‚\n",
    "9. è«‹æ¨¡ä»¿æ¯”è³½è½‰æ’­å“¡æˆ–æ•™ç·´çš„èªæ°£æè¿°ï¼Œå¥å¼è‡ªç„¶ã€æœ‰ç¯€å¥æ„Ÿã€‚\n",
    "10. è«‹ç‰¹åˆ¥è§€å¯Ÿçƒç¨®ä¹‹é–“çš„é€£çºŒè½‰æ›ï¼Œä¾‹å¦‚ æ”¾å°çƒ æ¥ æ®ºçƒ ç­‰ï¼Œæ‰¾å‡ºå…¶ä¸­æœ‰æ•ˆå¾—åˆ†æˆ–ä¸å°‹å¸¸çš„çµ„åˆä¸¦æè¿°ã€‚\n",
    "\n",
    "# Highlights Summary\n",
    "{highlight_summary}\n",
    "\n",
    "# Table Features\n",
    "{table_feature_summary}\n",
    "\n",
    "# Table Description\n",
    "{self.table_description}\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Tables\n",
    "{table_str}\n",
    "## Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def merge_child_texts(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        GENERATING_TOKENS = 100\n",
    "        reports_str = \"\\n\".join([f\"- {txt}\" for txt in child_texts])\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a content generator for the badminton game report .\n",
    "Please merge and rewrite a New Report based on the input Reports .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The New Report must include all the content from the input Reports ; do not omit any information .\n",
    "5. The New Report must follow the order of the input Reports .\n",
    "6. The number of tokens in the New Report must be within {GENERATING_TOKENS}.\n",
    "7. è«‹ä¾åºæ•´åˆæ¯æ®µå…§å®¹ï¼Œå½¢æˆçµæ§‹æ¸…æ™°çš„æ®µè½ï¼ŒåŒ…æ‹¬äº®é»ã€å¤±èª¤æ¨¡å¼èˆ‡çƒå“¡è²¢ç»ã€‚\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Reports\n",
    "{reports_str}\n",
    "## New Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "# ===== OperationParser._validate_operation å¼·åŒ–åƒæ•¸é©—è­‰ï¼ˆè£œå…¥ df æ¬„ä½æ¯”å°ï¼‰ =====\n",
    "def validate_operation_with_columns(operation: Dict[str, Any], df_columns: List[str]) -> bool:\n",
    "    name = operation.get(\"name\", \"\").lower()\n",
    "    args = operation.get(\"args\", [])\n",
    "\n",
    "    # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "    if name not in {\n",
    "        'select_column', 'select_row', 'sort', 'calculate',\n",
    "        'group_by', 'value_counts', 'aggregate', 'crosstab', 'pivot_table', 'write'\n",
    "    }:\n",
    "        return False\n",
    "\n",
    "    # åƒ…é‡å°éœ€åƒæ•¸æ“ä½œæª¢æŸ¥æ¬„ä½\n",
    "    if name in ['select_column', 'sort', 'group_by']:\n",
    "        for arg in args:\n",
    "            if arg not in df_columns:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# ===== æ”¹é€²çš„TreeOfReporté¡åˆ¥ =====\n",
    "class TreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # åˆå§‹åŒ–æ”¹é€²çš„çµ„ä»¶ï¼ˆå‚³å…¥ operations_configï¼‰\n",
    "        self.content_planner = ContentPlanner(api_key, self.operation_description)\n",
    "        self.df_operator = SafeDataFrameOperator(api_key)\n",
    "        self.text_generator = TextGenerator(api_key, table_description=self.table_description)\n",
    "        \n",
    "        # æ–°å¢è¿½è¹¤åŠŸèƒ½\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        self.node_registry: Dict[str, TreeNode] = {}\n",
    "\n",
    "    def load_configurations(self):\n",
    "        \"\"\"è¼‰å…¥é…ç½®æ–‡ä»¶\"\"\"\n",
    "        self.table_description = read_text_file(\"filtered_data_description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å„ç¨®æ¬„ä½ç”¨æ–¼åˆ†æ\"\n",
    "\n",
    "        # è®€å– selected_operations.json\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        \n",
    "        # æ ¹æ“š JSON çµæ§‹æå–æ“ä½œæ± \n",
    "        if isinstance(self.operation_description, dict) and \"operations\" in self.operation_description:\n",
    "            self.operation_pool = [op['operation'] for op in self.operation_description['operations']]\n",
    "        elif isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['operation'] for op in self.operation_description if 'operation' in op]\n",
    "        else:\n",
    "            self.operation_pool = ['value_counts', 'crosstab', 'pivot_table', 'groupby', 'write']\n",
    "\n",
    "        logger.info(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "\n",
    "    def build_tree(self, root_table: pd.DataFrame) -> TreeNode:\n",
    "        \"\"\"æ”¹é€²çš„æ¨¹æ§‹å»ºï¼ŒåŠ å…¥å®Œæ•´çš„è¿½è¹¤å’Œé©—è­‰\"\"\"\n",
    "        root = TreeNode(level=0, text=\"è³‡æ–™åˆ†æå ±å‘Š\", table=root_table, operation=\"root(None)\")\n",
    "        root.operation_history = ['root(None)']\n",
    "        self.node_registry[root.node_id] = root\n",
    "        \n",
    "        queue = [root]\n",
    "        \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            \n",
    "            # è¨˜éŒ„è™•ç†æ—¥èªŒ\n",
    "            self._log_node_processing(current_node)\n",
    "\n",
    "            if current_node.operation.lower().startswith('write'):\n",
    "                continue\n",
    "\n",
    "            if current_node.level >= self.max_depth:\n",
    "                write_node = self.create_child_node(current_node, 'write()')\n",
    "                if write_node:\n",
    "                    current_node.add_child(write_node)\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"è™•ç†ç¯€é» - Level: {current_node.level}, Operation: {current_node.operation}\")\n",
    "\n",
    "            tables_str = current_node.table.to_string()\n",
    "            operations = self.content_planner.generate_operations(\n",
    "                tables=tables_str,\n",
    "                table_description=self.table_description,\n",
    "                operation_description=self.operation_description,\n",
    "                operation_history=current_node.operation_history,\n",
    "                operation_pool=self.operation_pool,\n",
    "                max_depth=self.max_depth,\n",
    "                max_degree=self.max_degree\n",
    "            )\n",
    "\n",
    "            logger.info(f\"ç”Ÿæˆæ“ä½œ: {operations}\")\n",
    "\n",
    "            for operation in operations[:self.max_degree]:\n",
    "                if operation.strip():\n",
    "                    child_node = self.create_child_node(current_node, operation)\n",
    "                    if child_node:\n",
    "                        current_node.add_child(child_node)\n",
    "                        queue.append(child_node)\n",
    "\n",
    "        self.generate_all_texts(root)\n",
    "        return root\n",
    "    \n",
    "    def _log_node_processing(self, node: TreeNode):\n",
    "        \"\"\"è¨˜éŒ„ç¯€é»è™•ç†æ—¥èªŒ\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"node_id\": node.node_id,\n",
    "            \"level\": node.level,\n",
    "            \"operation\": node.operation,\n",
    "            \"table_shape\": list(node.table.shape) if not node.table.empty else [0, 0],\n",
    "            \"validation_errors\": node.validation_errors\n",
    "        }\n",
    "        self.execution_log.append(log_entry)\n",
    "    \n",
    "    def create_child_node(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"æ”¹é€²çš„å­ç¯€é»å‰µå»ºï¼ŒåŠ å…¥å®Œæ•´é©—è­‰\"\"\"\n",
    "        try:\n",
    "            # å»ºç«‹æ–°çš„æ“ä½œæ­·å²\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚º write æ“ä½œ\n",
    "            if operation.lower().startswith('write'):\n",
    "                text = self.text_generator.generate_text_for_write_operation(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                self.node_registry[child.node_id] = child\n",
    "                logger.info(f\"å‰µå»º write ç¯€é»: {operation}\")\n",
    "                return child\n",
    "            else:\n",
    "                # å…¶ä»–æ“ä½œï¼šåŸ·è¡Œæ•¸æ“šæ“ä½œ\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    self.node_registry[child.node_id] = child\n",
    "                    logger.info(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_texts(self, node: TreeNode):\n",
    "        \"\"\"éæ­¸ç”Ÿæˆæ‰€æœ‰ç¯€é»çš„æ–‡æœ¬\"\"\"\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts(child)\n",
    "        \n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_text_for_write_operation(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "            print(f'node table: {node.table}')\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        logger.info(f'ç¯€é» {node.node_id} æ–‡æœ¬ç”Ÿæˆå®Œæˆ')\n",
    "        print(f'node.table: {node.table}')\n",
    "        print(f'ç¯€é»æ–‡æœ¬: {node.text}')\n",
    "        \n",
    "    def export_tree_structure(self, root: TreeNode, output_path: str = \"tree_structure.json\"):\n",
    "        \"\"\"å°å‡ºæ¨¹çµæ§‹ç‚ºJSONæ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–å’Œåˆ†æ\"\"\"\n",
    "        def node_to_dict(node: TreeNode) -> Dict[str, Any]:\n",
    "            result = node.to_dict()\n",
    "            result[\"children\"] = [node_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_time\": datetime.now().isoformat(),\n",
    "                \"total_nodes\": len(self.node_registry),\n",
    "                \"max_depth\": self.max_depth,\n",
    "                \"max_degree\": self.max_degree\n",
    "            },\n",
    "            \"execution_log\": self.execution_log,\n",
    "            \"tree\": node_to_dict(root)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(tree_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"æ¨¹çµæ§‹å·²å°å‡ºè‡³: {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å°å‡ºæ¨¹çµæ§‹å¤±æ•—: {e}\")\n",
    "    \n",
    "    def generate_execution_report(self) -> str:\n",
    "        \"\"\"ç”ŸæˆåŸ·è¡Œéç¨‹å ±å‘Š\"\"\"\n",
    "        total_nodes = len(self.node_registry)\n",
    "        error_nodes = sum(1 for node in self.node_registry.values() if node.validation_errors)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Tree-of-Report åŸ·è¡Œå ±å‘Š\n",
    "\n",
    "## çµ±è¨ˆä¿¡æ¯\n",
    "- ç¸½ç¯€é»æ•¸: {total_nodes}\n",
    "- éŒ¯èª¤ç¯€é»æ•¸: {error_nodes}\n",
    "- æ¨¹æœ€å¤§æ·±åº¦: {self.max_depth}\n",
    "- æœ€å¤§åˆ†æ”¯åº¦: {self.max_degree}\n",
    "\n",
    "## ç¯€é»åˆ†å¸ƒ\n",
    "\"\"\"\n",
    "        \n",
    "        # æŒ‰å±¤ç´šçµ±è¨ˆç¯€é»\n",
    "        level_counts = {}\n",
    "        for node in self.node_registry.values():\n",
    "            level = node.level\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "        \n",
    "        for level, count in sorted(level_counts.items()):\n",
    "            report += f\"- Level {level}: {count} å€‹ç¯€é»\\n\"\n",
    "        \n",
    "        # éŒ¯èª¤æ‘˜è¦\n",
    "        if error_nodes > 0:\n",
    "            report += \"\\n## é©—è­‰éŒ¯èª¤æ‘˜è¦\\n\"\n",
    "            for node in self.node_registry.values():\n",
    "                if node.validation_errors:\n",
    "                    report += f\"- ç¯€é» {node.node_id} ({node.operation}): {'; '.join(node.validation_errors)}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def generate_report(self, node: TreeNode, level: int = 0) -> str:\n",
    "        \"\"\"æ”¹é€²çš„å ±å‘Šç”Ÿæˆ\"\"\"\n",
    "        if node.level == 0:\n",
    "            prompt = f\"\"\"\n",
    "            ä½ æ˜¯ä¸€ä½æ–°èè¨˜è€…ï¼Œæ ¹æ“šä»¥ä¸‹åˆ†æç¸½çµï¼Œè«‹æ’°å¯«ä¸€ç¯‡è³½äº‹æ–°èå ±å°ï¼Œæä¾›å…¨é¢æ·±å…¥çš„åˆ†æï¼Œçµ±æ•´æˆæ–°èå ±å°ï¼Œæ–‡è¾­ä¸­éå¤šç›´æ¥ä½¿ç”¨æ¬„ä½åç¨±èˆ‡ç›´æ¥æ¬¡æ•¸çµ±è¨ˆï¼Œç”¨player_Aèˆ‡player_Bè¡¨ç¤ºå…©çƒå“¡ï¼Œç”¨ç”Ÿå‹•çš„æ–‡å¥æè¿°ï¼Œå‹¿å‡ºç¾ç´¯è´…çš„å¥å­ï¼Œè«‹å¾åˆ†æç¸½çµä¸­æå–è½‰æ›ï¼Œç¦æ­¢å‡ºç¾å¹»è¦ºã€‚\n",
    "            è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ï¼Œè³‡è¨Šæº–ç¢ºã€‚\n",
    "\n",
    "            åˆ†æç¸½çµ:\n",
    "            {node.text}\n",
    "            \"\"\"\n",
    "            final_text = self.text_generator._retry_generate(prompt)\n",
    "            \n",
    "            # ä¿å­˜å¤šç¨®æ ¼å¼çš„å ±å‘Š\n",
    "            with open(\"tree_of_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            \n",
    "            # å°å‡ºæ¨¹çµæ§‹\n",
    "            self.export_tree_structure(node)\n",
    "            \n",
    "            # ç”ŸæˆåŸ·è¡Œå ±å‘Š\n",
    "            exec_report = self.generate_execution_report()\n",
    "            with open(\"execution_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(exec_report)\n",
    "                print(\"finish generate report\")\n",
    "            \n",
    "            return final_text\n",
    "        else:\n",
    "            logger.info(f'generate report from not root')\n",
    "            indent = \"  \" * level\n",
    "            report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "            if node.text:\n",
    "                report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "            if node.table is not None and not node.table.empty and level < 2:\n",
    "                report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "                if len(node.table) <= 10:\n",
    "                    report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "                else:\n",
    "                    report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "            for child in node.children:\n",
    "                report += self.generate_report(child, level + 1)\n",
    "\n",
    "            return report\n",
    "\n",
    "\n",
    "# ===== ä¸»ç¨‹åº =====\n",
    "def main():\n",
    "    \"\"\"æ”¹é€²çš„ä¸»å‡½æ•¸\"\"\"\n",
    "    \n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "\n",
    "    logger.info(\"Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    logger.info(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "\n",
    "    TABLES = pd.read_csv('filtered_set1.csv')\n",
    "    logger.info(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "\n",
    "    \n",
    "    # è¨­ç½®åƒæ•¸\n",
    "    MAX_DEPTH = 3\n",
    "    MAX_DEGREE = 4\n",
    "    \n",
    "    logger.info(f\"æœ€å¤§æ·±åº¦: {MAX_DEPTH}\")\n",
    "    logger.info(f\"æœ€å¤§åˆ†æ”¯åº¦: {MAX_DEGREE}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ”¹é€²çš„ Tree-of-Report\n",
    "    tree_report = TreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "    \n",
    "    # å»ºæ§‹å ±å‘Šæ¨¹\n",
    "    logger.info(\"é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        root = tree_report.build_tree(TABLES)\n",
    "        \n",
    "        # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "        logger.info(\"ç”Ÿæˆæœ€çµ‚å ±å‘Š...\")\n",
    "        final_report = tree_report.generate_report(root)\n",
    "        \n",
    "        # è¼¸å‡ºå ±å‘Š\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(\"TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"=\"*50)\n",
    "        print(final_report)\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        with open('tree_of_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Tree-of-Report æ•¸æ“šåˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ)\\n\\n\")\n",
    "            f.write(final_report)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        logger.info(f\"å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’\")\n",
    "        logger.info(\"ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        logger.info(\"- tree_of_report.md: æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"- tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\")\n",
    "        logger.info(\"- tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\")\n",
    "        logger.info(\"- execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\")\n",
    "        logger.info(\"- tree_visualization.html: å¯è¦–åŒ–é é¢\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # æ¸…ç†æš«å­˜æª”æ¡ˆ\n",
    "        for temp_file in ['input_tmp.csv', 'tmp.csv']:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                    logger.info(f\"æ¸…ç†æš«å­˜æª”æ¡ˆ: {temp_file}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1db8ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ç¬¬ 1/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 2/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 3/3 æ¬¡ç”Ÿæˆ...\n",
      "\n",
      "âœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\n",
      "[1] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "[2] åˆ†æ•¸: 0.6 â†’ é€™å ´ç¾½çƒè³½äº‹æˆ°æ³è† è‘—ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚æ¯”è³½åˆæ®µï¼Œé›™æ–¹éƒ½ä»¥è©¦æ¢æ€§çš„ç™¼çƒé–‹å±€ï¼Œéš¨å¾Œçƒè·¯è®ŠåŒ–å¤šç«¯ï¼Œæœ‰æ™‚æ˜¯è¼•å·§çš„ç¶²å‰å°çƒï¼Œæœ‰æ™‚æ˜¯åŠ›é“åè¶³çš„å¾Œå ´é‡æ“Šï¼Œçœ‹å¾—å‡ºé›™æ–¹é¸æ‰‹éƒ½åœ¨ç©æ¥µå°‹æ‰¾å°æ–¹çš„ç ´ç¶»ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé¸æ‰‹Aä¸€åº¦å–å¾—é ˜å…ˆï¼Œä½†é¸æ‰‹BéŸŒæ€§åè¶³ï¼Œç·Šå’¬æ¯”åˆ†ã€‚åœ¨å¤šæ‹ä¾†å›ä¸­ï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µä½³çš„é˜²å®ˆèƒ½åŠ›ï¼Œå¤šæ¬¡å°‡çœ‹ä¼¼å¿…æ®ºçš„çƒè·¯åŒ–è§£ã€‚ç¶²å‰çš„ç´°è†©æ‰‹æ³•å’Œå¾Œå ´çš„å¼·åŠ›é€²æ”»äº¤ç¹”ï¼Œè®“è§€çœ¾çœ‹å¾—ç›®ä¸æš‡çµ¦ã€‚\n",
      "\n",
      "åœ¨é—œéµæ™‚åˆ»ï¼Œé¸æ‰‹Aåˆ©ç”¨ä¸€æ¬¡ç²¾æº–çš„åˆ¤æ–·ï¼Œè®“å°æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸå¾—åˆ†ã€‚ç„¶è€Œï¼Œé¸æ‰‹Bä¹Ÿæ¯«ä¸ç¤ºå¼±ï¼Œéš¨å³ä»¥ä¸€è¨˜æ¼‚äº®çš„è½åœ°å¾—åˆ†é‚„ä»¥é¡è‰²ã€‚æ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œæ¯”è³½æ°£æ°›ä¹Ÿè¶Šç™¼ç·Šå¼µã€‚\n",
      "\n",
      "æœ€çµ‚ï¼Œé¸æ‰‹Aç©©ä½é™£è…³ï¼Œæ†‘è—‰è‘—ç©©å®šçš„ç™¼æ®å’Œé—œéµæ™‚åˆ»çš„æœæ–·é€²æ”»ï¼ŒæˆåŠŸæ‹¿ä¸‹åˆ†æ•¸ã€‚ä½†é¸æ‰‹Bçš„è¡¨ç¾ä¹ŸåŒæ¨£ç²¾å½©ï¼Œé›–æ•—çŒ¶æ¦®ã€‚æ•´å ´æ¯”è³½é«˜æ½®è¿­èµ·ï¼Œå……åˆ†å±•ç¾äº†ç¾½çƒé‹å‹•çš„é­…åŠ›ã€‚è§€çœ¾å€‘ä¹Ÿç‚ºé€™å ´ç²¾å½©çš„å°æ±ºç»ä¸Šäº†ç†±çƒˆçš„æŒè²ã€‚\n",
      "[3] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œæ”»é˜²è½‰æ›ç¯€å¥å¿«é€Ÿã€‚é–‹å±€é›™æ–¹äº’æœ‰é ˜å…ˆï¼Œæ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œé¦–å±€å‰åŠæ®µAé¸æ‰‹ç¨ä½”å„ªå‹¢ï¼Œä¸€åº¦å°‡æ¯”åˆ†æ‹‰é–‹è‡³2:1ï¼Œä½†Bé¸æ‰‹éš¨å³å±•é–‹åæ“Šï¼Œåˆ©ç”¨ç²¾æº–çš„è½é»æ§åˆ¶å’Œå¼·å‹¢çš„é€²æ”»ï¼Œå°‡æ¯”åˆ†è¿½å¹³ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å¤šå›åˆçš„ç²¾é‡‡å°æ±ºã€‚ä¾‹å¦‚ç¬¬ä¸‰åˆ†ï¼Œé›™æ–¹é¸æ‰‹ç¶“éå¤šæ¬¡çš„çŸ­çƒã€æŒ‘çƒã€é•·çƒã€æŠ½çƒã€åˆ‡çƒç­‰æˆ°è¡“é‹ç”¨ï¼Œè¶³è¶³ä¾†å›äº†17æ‹æ‰ç”±Aé¸æ‰‹æŠ“ä½æ©Ÿæœƒï¼Œä¸€è¨˜å°æ‰‹ç„¡æ³•æ¥åˆ°çš„çƒæ‹¿ä¸‹åˆ†æ•¸ã€‚å„˜ç®¡å¦‚æ­¤ï¼ŒBé¸æ‰‹ä¹Ÿæ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œéš¨å¾Œä¹Ÿä»¥é€£çºŒçš„ç©æ¥µé€²æ”»ï¼ŒåŒ…æ‹¬å¤šæ¬¡çš„æ®ºçƒï¼Œçµ¦Aé¸æ‰‹å¸¶ä¾†äº†æ¥µå¤§çš„å£“åŠ›ã€‚\n",
      "\n",
      "æ¯”è³½é€²å…¥ä¸­æ®µå¾Œï¼ŒAé¸æ‰‹åœ¨ç™¼çƒç’°ç¯€ä¸Šæ›´æ³¨æ„ç­–ç•¥ï¼Œå¶çˆ¾æ¡ç”¨çŸ­ç™¼ï¼Œå¸Œæœ›æ“¾äº‚Bé¸æ‰‹çš„ç¯€å¥ã€‚ä½†Bé¸æ‰‹ä¹Ÿç©æ¥µèª¿æ•´ï¼Œä¸¦åˆ©ç”¨Aé¸æ‰‹å¹¾æ¬¡åˆ¤æ–·å¤±èª¤åŠå›çƒæ›ç¶²çš„æ©Ÿæœƒï¼ŒæˆåŠŸå°‡æ¯”åˆ†åè¶…ã€‚\n",
      "\n",
      "æ¯”è³½æœ«æ®µï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µå¼·çš„éŸŒæ€§ã€‚å„˜ç®¡é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œä½†ä¾èˆŠåŠªåŠ›åœ¨æ¯ä¸€æ¬¡æ“Šçƒä¸­å°‹æ‰¾æ©Ÿæœƒã€‚Aé¸æ‰‹æ›¾ä¾é ç²¾æº–çš„è½é»å’Œå¹¾æ¬¡æ¼‚äº®çš„é˜²å®ˆåæ“Šï¼Œå°‡æ¯”åˆ†è¿½è¿‘ï¼Œä½†Bé¸æ‰‹ç¸½èƒ½åœ¨é—œéµæ™‚åˆ»æŒºèº«è€Œå‡ºï¼Œå¤šæ¬¡åˆ©ç”¨å¼·åŠ›çš„æ‰£æ®ºä»¥åŠç²¾æº–çš„ç¶²å‰å°çƒï¼Œç©©ä½é™£è…³ã€‚æœ€çµ‚ï¼ŒBé¸æ‰‹ä»¥ä¸€è¨˜è§’åº¦åˆé‘½çš„æ’²çƒï¼Œè®“Aé¸æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸæ‹¿ä¸‹é€™å±€çš„ç¬¬15åˆ†ã€‚éš¨å¾ŒAé¸æ‰‹é›–å¥®åŠ›è¿½è¶•ï¼Œä½†æœ€çµ‚Bé¸æ‰‹æ†‘è—‰ä¸€è¨˜å¹¸é‹çš„é•·çƒå‡ºç•Œï¼Œä»¥21:15æ‹¿ä¸‹æ­¤å±€å‹åˆ©ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½å……æ»¿äº†å„å¼å„æ¨£çš„æˆ°è¡“é‹ç”¨ï¼ŒåŒ…å«é•·çƒã€çŸ­çƒã€åˆ‡çƒã€æŒ‘çƒã€æ®ºçƒã€æ¨çƒã€å‹¾çƒï¼Œä»¥åŠç¶²å‰å°çƒçš„çˆ­å¥ªï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„çƒæŠ€å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œç‚ºè§€çœ¾å¸¶ä¾†äº†ä¸€å ´ç²¾é‡‡çµ•å€«çš„ç¾½çƒé¥—å®´ã€‚ æ¯”è³½çµæœé›–æœ‰å‹è² ï¼Œä½†é›™æ–¹é‹å‹•å“¡çš„é‹å‹•å®¶ç²¾ç¥ï¼Œéƒ½ä»¤äººå°è±¡æ·±åˆ»ã€‚\n",
      "\n",
      "ğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ 1 æ¬¡ï¼šé€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "âœ”ï¸ å·²å„²å­˜è‡³ï¼šbest_of_three_report_20250928_195942.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === å¯«ä½œé¢¨æ ¼è©å½™ ===\n",
    "BADMINTON_TERMS = {\n",
    "    'net': 'ç¶²å‰å¤±èª¤', 'out': 'å‡ºç•Œ', 'long': 'éåº•ç·š', 'smash': 'æ®ºçƒ',\n",
    "    'clear': 'é«˜é çƒ', 'drop': 'åˆ‡çƒ', 'drive': 'å¹³æŠ½çƒ', 'serve': 'ç™¼çƒ', 'return': 'å›çƒ'\n",
    "}\n",
    "ACTION_VERBS = ['å±•ç¾', 'ç™¼æ®', 'æŒæ¡', 'é‹ç”¨', 'æ–½å±•', 'æ§åˆ¶', 'ä¸»å°', 'å£“åˆ¶', 'çªç ´', 'å‰µé€ ', 'ç· é€ ', 'å¥ å®š', 'ç¢ºç«‹', 'éå›º', 'æ‰­è½‰', 'é€†è½‰']\n",
    "TECHNICAL_TERMS = ['lose_reason', 'getpoint_player', 'type', 'column', 'row']\n",
    "\n",
    "# === Gemini æ¨¡å‹åˆå§‹åŒ– ===\n",
    "def init_model(api_key: str):\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# === å“è³ªè©•ä¼° ===\n",
    "def assess_text_quality(text: str) -> float:\n",
    "    score = 0.0\n",
    "    if 30 <= len(text) <= 120:\n",
    "        score += 0.2\n",
    "    score += min(0.2, sum(1 for t in BADMINTON_TERMS.values() if t in text) * 0.1)\n",
    "    score += min(0.2, sum(1 for v in ACTION_VERBS if v in text) * 0.05)\n",
    "    if not any(t in text for t in TECHNICAL_TERMS):\n",
    "        score += 0.2\n",
    "    if 'ï¼Œ' in text or 'ã€‚' in text:\n",
    "        score += 0.2\n",
    "    return round(min(score, 1.0), 2)\n",
    "\n",
    "# === ä¸»æµç¨‹ï¼šé‡è¤‡3æ¬¡ç”Ÿæˆä¸¦è©•ä¼° ===\n",
    "def generate_best_of_three(df: pd.DataFrame, api_key: str):\n",
    "    model = init_model(api_key)\n",
    "    table_str = df.to_string(index=False)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­é«”è‚²æ–°èè¨˜è€…ï¼Œæ“…é•·æ’°å¯«ç¾½çƒæ¯”è³½å ±å°ã€‚\n",
    "è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šè¡¨æ ¼æ’°å¯«è³½äº‹æè¿°ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…å‡ºç¾æŠ€è¡“æ¬„ä½åç¨±ã€‚\n",
    "\n",
    "# è³½äº‹æ•¸æ“šè¡¨æ ¼ï¼š\n",
    "{table_str}\n",
    "\n",
    "è«‹æ’°å¯«æè¿°ï¼š\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            print(f\"â³ ç¬¬ {i+1}/3 æ¬¡ç”Ÿæˆ...\")\n",
    "            response = model.generate_content(prompt_template)\n",
    "            time.sleep(1)\n",
    "            text = response.text.strip() if response.text else \"âš ï¸ ç„¡å…§å®¹\"\n",
    "        except Exception as e:\n",
    "            text = f\"âš ï¸ ç”ŸæˆéŒ¯èª¤: {e}\"\n",
    "        score = assess_text_quality(text)\n",
    "        results.append({'index': i+1, 'text': text, 'score': score})\n",
    "\n",
    "    # é¸å‡ºæœ€ä½³çµæœ\n",
    "    best = max(results, key=lambda x: x['score'])\n",
    "\n",
    "    # è¼¸å‡ºåˆ°æª”æ¡ˆ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"best_of_three_report_{timestamp}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(f\"[ç‰ˆæœ¬ {r['index']}] å“è³ªåˆ†æ•¸: {r['score']}\\n{r['text']}\\n\\n\")\n",
    "        f.write(f\"ğŸ† æœ€ä½³ç‰ˆæœ¬ç‚ºç¬¬ {best['index']} æ¬¡ï¼Œåˆ†æ•¸: {best['score']}\\n\")\n",
    "        f.write(best['text'])\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\")\n",
    "    for r in results:\n",
    "        print(f\"[{r['index']}] åˆ†æ•¸: {r['score']} â†’ {r['text']}\")\n",
    "    print(f\"\\nğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ {best['index']} æ¬¡ï¼š{best['text']}\")\n",
    "    print(f\"âœ”ï¸ å·²å„²å­˜è‡³ï¼š{file_name}\")\n",
    "    return best\n",
    "\n",
    "# === æ¸¬è©¦å…¥å£ ===\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"è«‹è¨­ç½® Gemini_API ç’°å¢ƒè®Šæ•¸\")\n",
    "\n",
    "    df = pd.read_csv(\"filtered_set1.csv\")\n",
    "    \n",
    "\n",
    "    generate_best_of_three(df, api_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
