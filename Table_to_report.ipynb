{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc75c21b",
   "metadata": {},
   "source": [
    "## ç°¡ä»‹ ##\n",
    "æ­¤ä»£ç¢¼ç”¨ä¾†è®“LLMæ ¹æ“šè¡¨æ ¼è³‡æ–™èˆ‡ä½¿ç”¨è€…çš„æå•è¦æ±‚ï¼Œé€épiplineèˆ‡tree stuctureï¼Œç”Ÿæˆå ±å°æˆ–åˆ†æè³‡æ–™\n",
    "\n",
    "æ­¤ç¯‡ç ”ç©¶åªéœ€æä¾›\n",
    "\"main.txt\"ç‚ºä½¿ç”¨è€…çš„å¤§ç¶±èˆ‡ç°¡çŸ­æƒ³æ³•\n",
    "\"data_description.txt\"ç‚ºè¦åˆ†æçš„table columnsæ‰€ä»£è¡¨çš„æ„ç¾©\n",
    "å°±å¯ç”¢ç”Ÿå®Œæ•´å ±å°\n",
    "(å¯ä½¿ç”¨åœ¨ç”¢ç”Ÿä»»ä½•å ±å°ä¸Šä¸é™æ–¼ç¾½çƒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0e02",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "\n",
    "åˆªæ¸›ä¸å¿…è¦çš„columns\n",
    "\n",
    "çµæœä¿ç•™['rally', 'time', 'roundscore_A', 'roundscore_B', 'player', 'type', 'lose_reason', 'getpoint_player']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a047de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"Gemini_API\")\n",
    "if not api_key:\n",
    "    print(\"âŒ Gemini_API ç’°å¢ƒè®Šæ•¸æœªè¨­å®š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca3c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danie\\anaconda3\\envs\\cotable\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#æ­£å¼\n",
    "import dspy\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, ClassVar\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "class GeminiOpenAI(dspy.LM):\n",
    "    def __init__(self, api_key, model_name=\"gemini-2.0-flash\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        # ä½¿ç”¨ Google çš„ OpenAI å…¼å®¹ç«¯é»\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "        super().__init__(model=model_name)\n",
    "     \n",
    "    def __call__(self, messages=None, **kwargs):\n",
    "        if messages is None:\n",
    "            raise ValueError(\"Missing 'messages' argument\")\n",
    "         \n",
    "        # Convert messages to OpenAI format\n",
    "        if isinstance(messages, list):\n",
    "            formatted_messages = []\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, dict) and 'content' in msg:\n",
    "                    role = msg.get('role', 'user')\n",
    "                    formatted_messages.append({\n",
    "                        'role': role,\n",
    "                        'content': msg['content']\n",
    "                    })\n",
    "                else:\n",
    "                    formatted_messages.append({\n",
    "                        'role': 'user',\n",
    "                        'content': str(msg)\n",
    "                    })\n",
    "        else:\n",
    "            formatted_messages = [{'role': 'user', 'content': str(messages)}]\n",
    "         \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=formatted_messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            if not response.choices or not response.choices[0].message.content:\n",
    "                raise ValueError(\"Empty response from Gemini\")\n",
    "            \n",
    "            return [{\n",
    "                'text': response.choices[0].message.content,\n",
    "                'logprobs': None\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return [{\n",
    "                'text': \"âš ï¸ Gemini API å›æ‡‰å¤±æ•—,å¯èƒ½å·²é”é™é¡æˆ–å‡ºç¾éŒ¯èª¤ã€‚\",\n",
    "                'logprobs': None\n",
    "            }]\n",
    "     \n",
    "    def basic_request(self, prompt, **kwargs):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                **kwargs\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Gemini model: {e}\")\n",
    "            return \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "\n",
    "def setup_gemini_api(api_key, model_name=\"gemini-2.0-flash\"):\n",
    "    lm = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    dspy.settings.configure(lm=lm)\n",
    "    return lm\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return file.read()\n",
    "\n",
    "def parse_list_from_response(response_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a Python list from various response formats including markdown code blocks\n",
    "    \"\"\"\n",
    "    if not response_text or response_text.strip() == \"\":\n",
    "        print(\"âš ï¸ å›æ‡‰ç‚ºç©º\")\n",
    "        return []\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```(?:python|json)?\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    \n",
    "    # Remove any additional backticks\n",
    "    text = text.strip('`').strip()\n",
    "    \n",
    "    # Try to find a list pattern in the text\n",
    "    list_match = re.search(r'\\[.*?\\]', text, re.DOTALL)\n",
    "    \n",
    "    if list_match:\n",
    "        list_text = list_match.group(0)\n",
    "    else:\n",
    "        print(f\"âš ï¸ ç„¡æ³•åœ¨å›æ‡‰ä¸­æ‰¾åˆ°åˆ—è¡¨æ ¼å¼\")\n",
    "        print(f\"å®Œæ•´å›æ‡‰: {text[:200]}...\")\n",
    "        return []\n",
    "    \n",
    "    # Clean up the list text\n",
    "    list_text = list_text.strip()\n",
    "    \n",
    "    # Try multiple parsing strategies\n",
    "    try:\n",
    "        # Strategy 1: Parse as-is, change to python list\n",
    "        return json.loads(list_text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 2: Convert single quotes to double quotes\n",
    "        list_text_double = list_text.replace(\"'\", '\"')\n",
    "        return json.loads(list_text_double)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Strategy 3: Manual parsing for simple cases\n",
    "        # Remove brackets and split by comma\n",
    "        content = list_text.strip('[]').strip()\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        # Split by comma and clean each item\n",
    "        items = []\n",
    "        for item in content.split(','):\n",
    "            item = item.strip().strip('\"').strip(\"'\").strip()\n",
    "            if item:\n",
    "                items.append(item)\n",
    "        \n",
    "        if items:\n",
    "            print(f\"âœ“ ä½¿ç”¨æ‰‹å‹•è§£ææˆåŠŸ\")\n",
    "            return items\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ‰‹å‹•è§£æå¤±æ•—: {e}\")\n",
    "    \n",
    "    print(f\"âŒ æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±æ•—äº†\")\n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {list_text[:200]}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_news_relevant_fields(description_path: str, main_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    å¾æè¿°æ–‡ä»¶å’Œå¤§ç¶±æ–‡ä»¶ä¸­æå–ç›¸é—œæ¬„ä½\n",
    "    \n",
    "    Args:\n",
    "        description_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: ç¯©é¸å‡ºçš„æ¬„ä½åˆ—è¡¨\n",
    "    \"\"\"\n",
    "     \n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(description_path)\n",
    "    \n",
    "    prompt = f\"\"\"Using the following outline and list of data column descriptions, select only the columns that are useful for the outline.\n",
    "\n",
    "## outline\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Please return only a Python list of column names, like this:\n",
    "['player_name', 'match_score', 'duration', ...]\n",
    "\n",
    "Do not include explanations or any other text. Return only the list.\"\"\"\n",
    "     \n",
    "    result = lm.basic_request(prompt)\n",
    "    \n",
    "    print(f\"ğŸ” åŸå§‹å›æ‡‰:\\n{result}\\n\")\n",
    "    \n",
    "    selected_fields = parse_list_from_response(result)\n",
    "    \n",
    "    if selected_fields:\n",
    "        print(\"âœ… ç¯©é¸å‡ºçš„æ¬„ä½:\", selected_fields)\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè§£ææ¬„ä½åˆ—è¡¨\")\n",
    "    \n",
    "    return selected_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692ab381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åŸå§‹å›æ‡‰:\n",
      "```python\n",
      "['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n",
      "```\n",
      "\n",
      "âœ… ç¯©é¸å‡ºçš„æ¬„ä½: ['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n",
      "æœ€çµ‚æ¬„ä½æ¸…å–®: ['roundscore_A', 'roundscore_B', 'player', 'getpoint_player', 'type', 'rally', 'time']\n"
     ]
    }
   ],
   "source": [
    "# ç›´æ¥èª¿ç”¨å‡½å¼\n",
    "fields = extract_news_relevant_fields(\"data_description.txt\", \"main.txt\")\n",
    "print(\"æœ€çµ‚æ¬„ä½æ¸…å–®:\", fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c560a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"set1.csv\")\n",
    "filtered_df = df[fields]\n",
    "filtered_df.to_csv(\"filtered_set1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7b2b4",
   "metadata": {},
   "source": [
    "å°‡æŒ‘é¸å‡ºçš„æ¬„ä½åŠèªªæ˜å¯«å…¥filtered_data_description.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc7c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ filtered_data_description.txt\n"
     ]
    }
   ],
   "source": [
    "def extract_descriptions_for_fields(fields: List[str], desc_path: str, output_path: str):\n",
    "    description_text = read_text_file(desc_path)\n",
    "\n",
    "    field_desc = {}\n",
    "    for line in description_text.splitlines():\n",
    "        for field in fields:\n",
    "            if line.lower().startswith(field.lower() + \":\"):\n",
    "                field_desc[field] = line.strip()\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for field in fields:\n",
    "                f.write(field_desc.get(field, f\"{field}: [Description not found]\") + \"\\n\")\n",
    "        print(f\"âœ… å·²å°‡æ¬„ä½æè¿°å¯«å…¥ {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯«å…¥å¤±æ•—: {e}\")\n",
    "\n",
    "\n",
    "extract_descriptions_for_fields(fields, 'data_description.txt', \"filtered_data_description.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c668469",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "\n",
    "è—‰ç”±äººç‚ºè¼¸å…¥å•é¡Œèˆ‡æ–¹å‘æç¤ºï¼Œçµ¦LLMåšå®Œæ•´åˆ†æå•é¡Œèˆ‡æ–¹å‘ä¹‹è¦åŠƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e639240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain_of_thought_response(main_path: str, desc_path: str, output_path: str, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆ Chain-of-Thought åˆ†æå›æ‡‰\n",
    "    \n",
    "    Args:\n",
    "        main_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        desc_path: è³‡æ–™æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    \n",
    "    Returns:\n",
    "        str: ç”Ÿæˆçš„å›æ‡‰å…§å®¹,å¦‚æœå¤±æ•—å‰‡è¿”å› None\n",
    "    \"\"\"\n",
    "\n",
    "    lm = setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    main_content = read_text_file(main_path)\n",
    "    description = read_text_file(desc_path)\n",
    "\n",
    "    chain_prompt = f\"\"\"\n",
    "You are a planning assistant.\n",
    "Analyze the following outline and column descriptions.\n",
    "\n",
    "## Outline & Ideas:\n",
    "{main_content}\n",
    "\n",
    "## Data Column Descriptions:\n",
    "{description}\n",
    "\n",
    "---\n",
    "\n",
    "Step-by-step:\n",
    "1. Reflect on the structure and meaning of the content.\n",
    "2. Formulate relevant and meaningful questions or planning strategies.\n",
    "3. Be explicit and detailed, use Chain-of-Thought reasoning.\n",
    "4. Output all thoughts and questions in English only.\n",
    "\"\"\"\n",
    "\n",
    "    result = lm.basic_request(chain_prompt)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result)\n",
    "        print(f\"âœ… Response saved to: {output_path}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to write output: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbf0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response saved to: analyze_response.txt\n"
     ]
    }
   ],
   "source": [
    "response = generate_chain_of_thought_response(\n",
    "    main_path=\"main.txt\",\n",
    "    desc_path=\"filtered_data_description.txt\",\n",
    "    output_path=\"analyze_response.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1496f5f",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "\n",
    "è«‹LLMæ ¹æ“š\"analyze_response.txt\"æ€è€ƒå¯ä»¥ä½¿ç”¨çš„operationä¸¦å°‡çµæœå­˜æ–¼ \"operations_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6399f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ operations_info.json\n",
      "\n",
      "âœ… æ“ä½œåç¨±é™£åˆ—:\n",
      "['write', 'select_row', 'select_column', '**group_by', '**aggregate', '**sort', '**join', '**calculate', '**pivot_table', '**window_function', '**value_counts', '**crosstab', '**shift', '**correlation', '**query']\n"
     ]
    }
   ],
   "source": [
    "def analyze_operations(analyze_path: str, output_json: str) -> List[str]:\n",
    "    lm = setup_gemini_api(api_key)\n",
    "    analysis = read_text_file(analyze_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a news journalist want to analyze data not forecaster.\n",
    "Based on the following text analysis, identify multiple useful table operations\n",
    "and describe the direct meaning of each operation.\n",
    "\n",
    "## Text Analysis:\n",
    "{analysis}\n",
    "\n",
    "---\n",
    "\n",
    "Please output a numbered list in this format:\n",
    "1. write: If the table is clear or small enough, generates text based on the tables using the LLM.\n",
    "2. select_row: Description\n",
    "3. select_column: Description\n",
    "4. operation_name: Description\n",
    "5. operation_name: Description\n",
    "...\n",
    "\n",
    "IMPORTANT: operation must contain select_row, select_column, and write in the first three operation.\n",
    "\n",
    "Give important operations and at most 15 operations.\n",
    "operation_name should be different and each operation can not be similar.\n",
    "operation can be apply on many columns is better.\n",
    "Description just give the original definition of the operation name and give some useful functions name in pandas.\n",
    "Only include operations and their descriptions. Be concise and clear.\n",
    "\"\"\"\n",
    "\n",
    "    response = lm.basic_request(prompt)\n",
    "\n",
    "    operations = []\n",
    "    operations_dict = {}\n",
    "\n",
    "    try:\n",
    "        for line in response.strip().split('\\n'):\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            if \".\" in line:\n",
    "                num, rest = line.split(\".\", 1)\n",
    "                if \":\" in rest:\n",
    "                    name, desc = rest.strip().split(\":\", 1)\n",
    "                    name = name.strip()\n",
    "                    desc = desc.strip()\n",
    "                    operations.append(name)\n",
    "                    operations_dict[num.strip()] = {\"operation\": name, \"description\": desc}\n",
    "\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(operations_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… æ“ä½œæ¸…å–®èˆ‡æè¿°å·²å„²å­˜è‡³ {output_json}\")\n",
    "        return operations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å›æ‡‰è™•ç†å¤±æ•—: {e}\\nåŸå§‹å›æ‡‰:\\n{response}\")\n",
    "        return []\n",
    "\n",
    "ops = analyze_operations(\"analyze_response.txt\", \"operations_info.json\")\n",
    "print(\"\\nâœ… æ“ä½œåç¨±é™£åˆ—:\")\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cebfb8",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "\n",
    "ä½¿LLMè‡ªå‹•åˆ†ætableé¸å‡ºåˆé©çš„operationæ”¾å…¥æ“ä½œæ± (operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14dcba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperationSignature(dspy.Signature):\n",
    "    \"\"\"Identify suitable operations for analyzing badminton match data.\"\"\"\n",
    "    data_description = dspy.InputField(desc=\"Overview and sample of the dataset\")\n",
    "    column_descriptions = dspy.InputField(desc=\"Descriptions of each column in the dataset\")\n",
    "    rules = dspy.InputField(desc=\"Rules for selecting operations\")\n",
    "    operations_list = dspy.OutputField(desc=\"A list of suitable operations number (e.g., [1, 2, 3, 4])\")\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: CSV æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: è®€å–çš„æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    è®€å– JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: JSON æ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON æ•¸æ“š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "def parse_column_descriptions(description_text):\n",
    "    \"\"\"\n",
    "    è§£ææ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        description_text: æ¬„ä½æè¿°æ–‡æœ¬\n",
    "    \n",
    "    Returns:\n",
    "        dict: æ¬„ä½åç¨±åˆ°æè¿°çš„æ˜ å°„\n",
    "    \"\"\"\n",
    "    descriptions = {}\n",
    "    pattern = r'''\n",
    "        ^                # Line start\n",
    "        (\\w+)            # Column name\n",
    "        :\\s+             # Colon and space\n",
    "        (.+?)            # Description text\n",
    "        (?=\\n\\w+:\\s+|\\Z) # Lookahead for next column or end of file\n",
    "    '''\n",
    "    matches = re.findall(pattern, description_text, flags=re.M | re.X)\n",
    "    for col_name, desc in matches:\n",
    "        clean_desc = ' '.join(desc.split()).strip()\n",
    "        descriptions[col_name] = clean_desc\n",
    "    return descriptions\n",
    "\n",
    "class BadmintonOperationSelector(dspy.Module):\n",
    "    def __init__(self, required_operations=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ“ä½œé¸æ“‡å™¨\n",
    "        \n",
    "        Args:\n",
    "            required_operations: å¿…é ˆåŒ…å«çš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (ä¾‹å¦‚: [1, 2, 3])\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.chain_of_thought = dspy.ChainOfThought(OperationSignature)\n",
    "        self.required_operations = required_operations or []\n",
    "\n",
    "    def forward(self, data_description, column_descriptions, rules):\n",
    "        result = self.chain_of_thought(\n",
    "            data_description=data_description,\n",
    "            column_descriptions=str(column_descriptions),\n",
    "            rules=str(rules)\n",
    "        )\n",
    "        operations = self.extract_operations_from_result(result.operations_list)\n",
    "        \n",
    "        # ç¢ºä¿å¿…éœ€çš„æ“ä½œè¢«åŒ…å«\n",
    "        operations = self.ensure_required_operations(operations)\n",
    "        \n",
    "        return operations\n",
    "\n",
    "    def extract_operations_from_result(self, operations_text):\n",
    "        \"\"\"\n",
    "        å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        æ”¯æ´å¤šç¨®æ ¼å¼:\n",
    "        - [1, 2, 3, 4]\n",
    "        - 1, 2, 3, 4\n",
    "        - 1 2 3 4\n",
    "        - Operation 1, Operation 2, etc.\n",
    "        \"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç§»é™¤ markdown ä»£ç¢¼å¡Šæ¨™è¨˜\n",
    "        operations_text = re.sub(r'```(?:python|json)?\\s*', '', operations_text)\n",
    "        operations_text = operations_text.strip('`').strip()\n",
    "        \n",
    "        # å˜—è©¦è§£æ JSON æ ¼å¼ [1, 2, 3]\n",
    "        try:\n",
    "            # å°‹æ‰¾æ–¹æ‹¬è™Ÿä¸­çš„å…§å®¹\n",
    "            list_match = re.search(r'\\[([^\\]]+)\\]', operations_text)\n",
    "            if list_match:\n",
    "                list_content = list_match.group(1)\n",
    "                # æå–æ‰€æœ‰æ•¸å­—\n",
    "                numbers = re.findall(r'\\d+', list_content)\n",
    "                operations = [int(num) for num in numbers]\n",
    "                if operations:\n",
    "                    return operations\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # å¦‚æœæ²’æœ‰æ–¹æ‹¬è™Ÿ,å˜—è©¦ç›´æ¥æå–æ‰€æœ‰æ•¸å­—\n",
    "        numbers = re.findall(r'\\d+', operations_text)\n",
    "        if numbers:\n",
    "            operations = [int(num) for num in numbers]\n",
    "            return operations\n",
    "        \n",
    "        # å¦‚æœä»¥ä¸Šéƒ½å¤±æ•—,å˜—è©¦é€è¡Œè™•ç†\n",
    "        lines = operations_text.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # æå–è©²è¡Œä¸­çš„æ‰€æœ‰æ•¸å­—\n",
    "            line_numbers = re.findall(r'\\d+', line)\n",
    "            operations.extend([int(num) for num in line_numbers])\n",
    "        \n",
    "        # å»é‡ä¸¦æ’åº\n",
    "        if operations:\n",
    "            operations = sorted(list(set(operations)))\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def ensure_required_operations(self, operations):\n",
    "        \"\"\"\n",
    "        ç¢ºä¿å¿…éœ€çš„æ“ä½œè¢«åŒ…å«åœ¨æ“ä½œåˆ—è¡¨ä¸­\n",
    "        \n",
    "        Args:\n",
    "            operations: ç•¶å‰çš„æ“ä½œåˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "            list: åŒ…å«å¿…éœ€æ“ä½œçš„å®Œæ•´åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # è½‰æ›ç‚ºé›†åˆä»¥é¿å…é‡è¤‡\n",
    "        operations_set = set(operations)\n",
    "        \n",
    "        # æ·»åŠ å¿…éœ€çš„æ“ä½œ\n",
    "        for required_op in self.required_operations:\n",
    "            operations_set.add(required_op)\n",
    "        \n",
    "        # è½‰æ›å›åˆ—è¡¨ä¸¦æ’åº\n",
    "        return sorted(list(operations_set))\n",
    "\n",
    "\n",
    "def analyze_badminton_match(data_path, column_desc_path, rules_path, \n",
    "                           model_name=\"gemini-2.0-flash-exp\", \n",
    "                           required_operations=None):\n",
    "    \"\"\"\n",
    "    åˆ†æç¾½çƒæ¯”è³½æ•¸æ“šä¸¦è­˜åˆ¥é©åˆçš„æ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        data_path: æ¯”è³½æ•¸æ“š CSV æ–‡ä»¶è·¯å¾‘\n",
    "        column_desc_path: æ¬„ä½æè¿°æ–‡ä»¶è·¯å¾‘\n",
    "        rules_path: æ“ä½œè¦å‰‡ JSON æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "        required_operations: å¿…é ˆåŒ…å«çš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (ä¾‹å¦‚: [1, 2, 3])\n",
    "    \n",
    "    Returns:\n",
    "        list: è­˜åˆ¥å‡ºçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨ (æ•´æ•¸)\n",
    "    \"\"\"\n",
    "    \n",
    "    # è¨­ç½®é»˜èªçš„å¿…éœ€æ“ä½œç‚º [1, 2, 3]\n",
    "    if required_operations is None:\n",
    "        required_operations = [1, 2, 3]\n",
    "    \n",
    "    print(\"Reading badminton match data...\")\n",
    "    try:\n",
    "        match_data = read_badminton_data(data_path)\n",
    "        columns_desc_content = read_text_file(column_desc_path)\n",
    "        rules = read_json_file(rules_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading files: {e}\")\n",
    "        return []\n",
    "\n",
    "    column_descriptions = parse_column_descriptions(columns_desc_content)\n",
    "    setup_gemini_api(api_key, model_name)\n",
    "\n",
    "    data_sample = match_data.head().to_string()\n",
    "    data_description = f\"\"\"\n",
    "    one match data:\n",
    "    {data_sample}\n",
    "\n",
    "    Data shape: {match_data.shape[0]} rows, {match_data.shape[1]} columns\n",
    "    Columns: {', '.join(match_data.columns)}\n",
    "    \"\"\"\n",
    "\n",
    "    selector = BadmintonOperationSelector(required_operations=required_operations)\n",
    "    operations = selector.forward(data_description, column_descriptions, rules)\n",
    "\n",
    "    print(f\"âœ… Identified {len(operations)} suitable operations:\")\n",
    "    print(f\"   Required operations: {required_operations}\")\n",
    "    for i, op in enumerate(operations, 1):\n",
    "        required_marker = \" (Required)\" if op in required_operations else \"\"\n",
    "        print(f\"{i}. Operation {op}{required_marker}\")\n",
    "\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e586506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading badminton match data...\n",
      "âœ… Identified 8 suitable operations:\n",
      "   Required operations: [1, 2, 3]\n",
      "1. Operation 1 (Required)\n",
      "2. Operation 2 (Required)\n",
      "3. Operation 3 (Required)\n",
      "4. Operation 4\n",
      "5. Operation 5\n",
      "6. Operation 8\n",
      "7. Operation 11\n",
      "8. Operation 12\n",
      "\n",
      "Final operations array: [1, 2, 3, 4, 5, 8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "operations = analyze_badminton_match(\n",
    "    data_path=\"filtered_set1.csv\",\n",
    "    column_desc_path=\"filtered_data_description.txt\",\n",
    "    rules_path=\"operations_info.json\"\n",
    ")\n",
    "print(\"\\nFinal operations array:\", operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d09a",
   "metadata": {},
   "source": [
    "å°‡æ‰€æŒ‘é¸å‡ºä¾†çš„æ“ä½œå¯«å…¥\"operations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "689bc29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… operations.json has been created with 8 operations.\n",
      "Selected operations: [1, 2, 3, 4, 5, 8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# å¾ JSON æª”æ¡ˆè®€å– operations\n",
    "original_operations_dict = read_json_file(\"operations_info.json\")\n",
    "\n",
    "# ä½ æƒ³è¦æŒ‘é¸çš„ operation ç·¨è™Ÿï¼ˆæ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹é€™å€‹ listï¼‰\n",
    "selected_numbers = operations\n",
    "\n",
    "def clean_operation_name(operation_text):\n",
    "    \"\"\"\n",
    "    æ¸…ç†æ“ä½œåç¨±ï¼Œåªä¿ç•™è‹±æ–‡å­—æ¯ã€æ•¸å­—å’Œåº•ç·š\n",
    "    ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å¦‚ **, -, ç­‰\n",
    "    \n",
    "    Args:\n",
    "        operation_text: åŸå§‹æ“ä½œåç¨±\n",
    "    \n",
    "    Returns:\n",
    "        str: æ¸…ç†å¾Œçš„æ“ä½œåç¨±\n",
    "    \"\"\"\n",
    "    # ç§»é™¤æ‰€æœ‰éå­—æ¯ã€æ•¸å­—ã€åº•ç·šçš„å­—ç¬¦\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '', operation_text)\n",
    "    return cleaned\n",
    "\n",
    "filtered_operations = []\n",
    "for new_number, original_number in enumerate(selected_numbers, start=1):\n",
    "    # å°‡æ•¸å­—è½‰æ›ç‚ºå­—ä¸²éµä¾†æŸ¥æ‰¾\n",
    "    key = str(original_number)\n",
    "    if key in original_operations_dict:\n",
    "        op_data = original_operations_dict[key]\n",
    "        \n",
    "        # æ¸…ç† operation åç¨±\n",
    "        cleaned_operation = clean_operation_name(op_data[\"operation\"])\n",
    "        \n",
    "        filtered_operations.append({\n",
    "            \"number\": new_number,\n",
    "            \"operation\": cleaned_operation,\n",
    "            \"description\": op_data[\"description\"]\n",
    "        })\n",
    "\n",
    "# æ–°çš„ JSON çµæ§‹\n",
    "output_json = {\n",
    "    \"description\": \"Selected operations for badminton data analysis.\",\n",
    "    \"requirements\": [\n",
    "        \"The output must be based on the input data; do not hallucinate.\",\n",
    "        \"Give me the list of numbers.\"\n",
    "    ],\n",
    "    \"operations\": filtered_operations\n",
    "}\n",
    "\n",
    "# å¯«å…¥ JSON æª”æ¡ˆ\n",
    "with open(\"operations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… operations.json has been created with {len(filtered_operations)} operations.\")\n",
    "print(f\"Selected operations: {selected_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743109f4",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "\n",
    "æ ¹æ“šçœŸå¯¦tableå°‡æ“ä½œé‡è¦æ€§æ’åºï¼Œè‹¥ç‚ºæ’åºå¾Œ30%ä¸”éä¸‰ç¨®é‡è¦æ“ä½œï¼Œå‰‡æ›¿é™¤ï¼Œä¿ç•™'write' 'select_col' 'select_row'ä¸‰å€‹é‡è¦æ“ä½œï¼Œåˆ°'selected_operations.json'\n",
    "\n",
    "æ“ä½œæå–å·²å®Œæˆ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7229122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾ operations.json æˆåŠŸè¼‰å…¥ 8 å€‹æ“ä½œ\n",
      "æ“ä½œæ•¸é‡: 8\n",
      "å˜—è©¦ API è«‹æ±‚ (ç¬¬ 1/3 æ¬¡)...\n",
      "âœ… æˆåŠŸç²å¾— API å›æ‡‰\n",
      "æå–åˆ°æ“ä½œåˆ—è¡¨: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "\n",
      "============================================================\n",
      "å®Œæ•´å›æ‡‰:\n",
      "å¥½çš„ï¼Œæˆ‘å€‘ä¾†åˆ†æé€™äº›æ“ä½œå°æ–¼ç¾½çƒæ–°èå¯«ä½œçš„é‡è¦æ€§ï¼Œä¸¦çµ¦å‡ºæ’åºã€‚\n",
      "\n",
      "**Chain-of-Thought:**\n",
      "\n",
      "ä½œç‚ºä¸€ä½è³‡æ·±çš„ç¾½çƒæ–°èè¨˜è€…ï¼Œæˆ‘çš„ç›®æ¨™æ˜¯åˆ©ç”¨æ¯”è³½æ•¸æ“šï¼Œç”¢å‡ºæœ‰æ·±åº¦ã€æœ‰åƒ¹å€¼çš„å ±å°ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦äº†è§£æ¯”è³½çš„åŸºæœ¬æƒ…æ³ï¼Œç„¶å¾Œæ·±å…¥æŒ–æ˜æ•¸æ“šä¸­çš„æ¨¡å¼å’Œæ´è¦‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘å°å„å€‹æ“ä½œçš„è©•ä¼°ï¼š\n",
      "\n",
      "*   **`value_counts` (7):**  é€™å€‹æ“ä½œå¯ä»¥å¿«é€Ÿäº†è§£å„ç¨®é¡å‹çš„çƒ (type) å‡ºç¾çš„é »ç‡ï¼Œæˆ–è€…çƒå“¡å¾—åˆ† (getpoint\\_player) çš„æ¬¡æ•¸ã€‚é€™å°æ–¼åˆ†æçƒå“¡çš„æ‰“æ³•åå¥½ã€æˆ°è¡“é¸æ“‡ï¼Œä»¥åŠæ‰¾å‡ºé—œéµå¾—åˆ†æ‰‹æ®µéå¸¸æœ‰å¹«åŠ©ã€‚èƒ½å¿«é€ŸæŒæ¡çƒå“¡æˆ–è³½äº‹çš„åˆæ­¥å°è±¡ã€‚é€™æ˜¯åŸºç¤åˆ†æçš„é—œéµã€‚\n",
      "*   **`crosstab` (8):** é€™å€‹æ“ä½œå¯ä»¥å¹«åŠ©æˆ‘å€‘å»ºç«‹å…©å€‹æˆ–å¤šå€‹å› ç´ ä¹‹é–“çš„é—œè¯æ€§ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥æ¯”è¼ƒä¸åŒçƒå“¡åœ¨ä¸åŒæƒ…æ³ä¸‹ä½¿ç”¨çš„çƒç¨®ï¼Œæˆ–è€…åˆ†æç™¼çƒå¾Œæ¥ç™¼çƒæ–¹å¾—åˆ†çš„æ©Ÿç‡ã€‚é€™ç¨®é—œè¯æ€§åˆ†æå¯ä»¥æ­ç¤ºæ›´æ·±å±¤æ¬¡çš„æˆ°è¡“ç­–ç•¥ã€‚\n",
      "*   **`group_by` (4) and `aggregate` (5):**  é€™å…©å€‹æ“ä½œé€šå¸¸ä¸€èµ·ä½¿ç”¨ï¼Œå¯ä»¥å°‡æ•¸æ“šæŒ‰ç…§ç‰¹å®šæ¢ä»¶åˆ†çµ„ï¼Œç„¶å¾Œè¨ˆç®—å„çµ„çš„çµ±è¨ˆæ•¸æ“šã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥æŒ‰ç…§çƒå“¡åˆ†çµ„ï¼Œè¨ˆç®—ä»–å€‘çš„å¹³å‡å¾—åˆ†ã€å¹³å‡å›åˆæ•¸ç­‰ã€‚æˆ–è€…æˆ‘å€‘å¯ä»¥æŒ‰ç…§ä¸åŒçš„å›åˆæ•¸åˆ†çµ„ï¼Œè¨ˆç®—ä¸åŒå›åˆçš„å¾—åˆ†ç‡ç­‰ã€‚é€™å€‹æ“ä½œå¯ä»¥å¹«åŠ©æˆ‘å€‘æ¯”è¼ƒä¸åŒçƒå“¡æˆ–ä¸åŒå›åˆä¹‹é–“çš„å·®ç•°ï¼Œæ‰¾å‡ºé—œéµå› ç´ ã€‚åœ¨è³½äº‹åˆ†æä¸­ï¼Œé€šå¸¸éœ€è¦åˆ†çµ„æ¯”è¼ƒæ•¸æ“šï¼Œä¾‹å¦‚æ¯”è¼ƒå‹è² æ–¹çš„å„é …æ•¸æ“šã€‚\n",
      "*   **`calculate` (6):** é€™å€‹æ“ä½œå¯ä»¥ç”¨ä¾†è¨ˆç®—æ–°çš„æ•¸æ“šæŒ‡æ¨™ï¼Œä¾‹å¦‚å¾—åˆ†å·®ã€å›åˆæŒçºŒæ™‚é–“ç­‰ç­‰ã€‚é€™äº›æŒ‡æ¨™å¯ä»¥å¹«åŠ©æˆ‘å€‘æ›´æ·±å…¥åœ°äº†è§£æ¯”è³½çš„é€²ç¨‹å’Œçƒå“¡çš„è¡¨ç¾ã€‚ä¾‹å¦‚ï¼Œè¨ˆç®—ã€Œä¾µç•¥æ€§æ¯”ç‡ã€ï¼ˆæ®ºçƒæ¬¡æ•¸/ç¸½æ“Šçƒæ¬¡æ•¸ï¼‰å¯ä»¥åæ˜ çƒå“¡çš„é€²æ”»é¢¨æ ¼ã€‚\n",
      "*   **`select_column` (3):** é¸æ“‡ç‰¹å®šçš„æ•¸æ“šåˆ—æ˜¯é€²è¡Œä»»ä½•åˆ†æçš„åŸºç¤ã€‚å¦‚æœæˆ‘å€‘æƒ³åˆ†æçƒå“¡çš„å¾—åˆ†æƒ…æ³ï¼Œå°±éœ€è¦é¸æ“‡`getpoint_player`é€™ä¸€åˆ—ã€‚å…¶ä»–æ“ä½œåŸºæœ¬ä¸Šéƒ½è¦å…ˆé€²è¡Œæ¬„ä½çš„é¸æ“‡ã€‚\n",
      "*   **`select_row` (2):** æ ¹æ“šæ¢ä»¶ç¯©é¸æ•¸æ“šå¯ä»¥å¹«åŠ©æˆ‘å€‘èšç„¦åˆ°ç‰¹å®šçš„æ¯”è³½éšæ®µæˆ–æƒ…å¢ƒã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥ç¯©é¸å‡ºæ¯”åˆ†æ¥è¿‘çš„éšæ®µé€²è¡Œåˆ†æï¼Œæˆ–è€…åªé—œæ³¨æŸå€‹ç‰¹å®šçƒå“¡çš„è¡¨ç¾ã€‚\n",
      "*   **`write` (1):** åœ¨åˆ†æå®Œæˆå¾Œï¼Œæ’°å¯«æ–°èå ±å°æ˜¯æœ€çµ‚ç›®çš„ã€‚æ ¹æ“šåˆ†æçµæœï¼Œå°‡æ•¸æ“šè½‰åŒ–ç‚ºæ˜“æ–¼ç†è§£çš„æ–‡å­—æè¿°ã€‚é€™å€‹æ“ä½œé›–ç„¶é‡è¦ï¼Œä½†ä¾è³´æ–¼å‰é¢æ‰€æœ‰åˆ†ææ“ä½œçš„çµæœã€‚\n",
      "\n",
      "**æ’åºçµæœ:**\n",
      "\n",
      "```\n",
      "[7, 8, 4, 5, 6, 3, 2, 1]\n",
      "```\n",
      "\n",
      "============================================================\n",
      "\n",
      "æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "é¸æ“‡äº† 8 å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: 70%)\n",
      "é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: [7, 8, 4, 5, 6, 3, 2, 1]\n",
      "âœ… selected_operations.json has been created with 8 operations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_operations_from_json(json_file_path):\n",
    "    \"\"\"\n",
    "    Load operations from JSON file\n",
    "    æ”¯æ´å…©ç¨®æ ¼å¼:\n",
    "    1. èˆŠæ ¼å¼: {\"1\": {\"operation\": \"...\", \"description\": \"...\"}, ...}\n",
    "    2. æ–°æ ¼å¼: {\"operations\": [{\"number\": 1, \"operation\": \"...\", \"description\": \"...\"}, ...]}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = read_json_file(json_file_path)\n",
    "        \n",
    "        operations_data = data['operations']\n",
    "        \n",
    "        # Create formatted operation strings for LLM processing\n",
    "        operation_strings = []\n",
    "        operation_details = []\n",
    "        \n",
    "        for op in operations_data:\n",
    "            number = op.get('number', '')\n",
    "            name = op.get('operation', '')\n",
    "            description = op.get('description', '')\n",
    "            \n",
    "            # Format as: \"number. name: description\"\n",
    "            if number and name and description:\n",
    "                formatted_op = f\"{number}. {name}: {description}\"\n",
    "                operation_strings.append(formatted_op)\n",
    "                operation_details.append({\n",
    "                    'number': number,\n",
    "                    'operation': name,  # çµ±ä¸€ä½¿ç”¨ 'operation' éµ\n",
    "                    'description': description,\n",
    "                    'formatted': formatted_op\n",
    "                })\n",
    "        \n",
    "        print(f\"å¾ {json_file_path} æˆåŠŸè¼‰å…¥ {len(operation_strings)} å€‹æ“ä½œ\")\n",
    "        return operation_details, operation_strings\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"éŒ¯èª¤: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_file_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"éŒ¯èª¤: {json_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def get_data_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the dataset\n",
    "    \"\"\"\n",
    "    summary = f\"è³‡æ–™é›†æ¦‚è¦:\\n- ç¸½è¡Œæ•¸: {dataframe.shape[0]}\\n- ç¸½åˆ—æ•¸: {dataframe.shape[1]}\\n- æ¬„ä½åç¨±: {', '.join(dataframe.columns)}\\n\\nå„æ¬„ä½è³‡è¨Š:\\n\"\n",
    "    for col in dataframe.columns:\n",
    "        summary += f\"  - {col}: \"\n",
    "        if dataframe[col].dtype in ['object', 'string']:\n",
    "            unique_vals = dataframe[col].unique()[:10]\n",
    "            summary += f\"é¡åˆ¥å‹è³‡æ–™, ç¨ç‰¹å€¼ç¯„ä¾‹: {', '.join(map(str, unique_vals))}\\n\"\n",
    "        else:\n",
    "            summary += f\"æ•¸å€¼å‹è³‡æ–™, ç¯„åœ: {dataframe[col].min()} - {dataframe[col].max()}\\n\"\n",
    "    return summary\n",
    "\n",
    "\n",
    "def extract_operation_numbers_from_response(response):\n",
    "    \"\"\"\n",
    "    å¾å›æ‡‰ä¸­æå–æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "    æ”¯æ´å¤šç¨®æ ¼å¼\n",
    "    \"\"\"\n",
    "    # æ–¹æ³•1: åŒ¹é…ä»£ç¢¼å¡Šä¸­çš„æ•¸çµ„\n",
    "    pattern1 = r'```\\s*\\[([\\d,\\s]+)\\]\\s*```'\n",
    "    match = re.search(pattern1, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',') if num]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•2: åŒ¹é…æ™®é€šæ–¹æ‹¬è™Ÿä¸­çš„æ•¸çµ„\n",
    "    pattern2 = r'\\[([\\d,\\s]+)\\]'\n",
    "    match = re.search(pattern2, response)\n",
    "    \n",
    "    if match:\n",
    "        array_str = match.group(1)\n",
    "        operation_list = [int(num) for num in array_str.replace(' ', '').split(',') if num]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    # æ–¹æ³•3: æå–æ‰€æœ‰æ•¸å­—\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        operation_list = [int(num) for num in numbers]\n",
    "        print(f\"æå–åˆ°æ“ä½œåˆ—è¡¨: {operation_list}\")\n",
    "        return operation_list\n",
    "    \n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ’åºæ•¸çµ„\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def filter_badminton_operations(operation_details, operation_strings, df, api_key, \n",
    "                                outline_path='outline.txt', model_name=\"gemini-2.0-flash\", \n",
    "                                max_retries=3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Gemini æ ¹æ“šé‡è¦æ€§æ’åºæ“ä½œ\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        operation_strings: æ“ä½œæ ¼å¼åŒ–å­—ä¸²åˆ—è¡¨\n",
    "        df: æ•¸æ“šæ¡†\n",
    "        api_key: API é‡‘é‘°\n",
    "        outline_path: å¤§ç¶±æ–‡ä»¶è·¯å¾‘\n",
    "        model_name: æ¨¡å‹åç¨±\n",
    "        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨, å®Œæ•´å›æ‡‰)\n",
    "    \"\"\"\n",
    "    gemini = GeminiOpenAI(api_key=api_key, model_name=model_name)\n",
    "    data_summary = get_data_summary(df)\n",
    "    \n",
    "    # é™åˆ¶è³‡æ–™æ¨£æœ¬å¤§å°\n",
    "    data_sample = df.head(10).to_string()\n",
    "    if len(data_sample) > 3000:\n",
    "        data_sample = data_sample[:3000] + \"...\\n[è³‡æ–™å·²æˆªæ–·]\"\n",
    "    \n",
    "    outline = read_text_file(outline_path)\n",
    "    \n",
    "    print(f\"æ“ä½œæ•¸é‡: {len(operation_strings)}\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "æˆ‘æœ‰ä¸€å€‹æ’°å¯«æ–°èçš„å¤§ç¶±èˆ‡æ¯”è³½çš„è³‡æ–™é›†å’Œ {len(operation_strings)} å€‹åˆ†ææ“ä½œï¼Œè«‹ä¾æ“šæ“ä½œé‡è¦æ€§æ’åº(ç”±é«˜åˆ°ä½)ã€‚\n",
    "\n",
    "å¤§ç¶±:\n",
    "{outline}\n",
    "\n",
    "è³‡æ–™æ¨£æœ¬:\n",
    "{data_sample}\n",
    "\n",
    "è³‡æ–™é›†è³‡è¨Š:\n",
    "{data_summary}\n",
    "\n",
    "æ“ä½œæ¸…å–®:\n",
    "{chr(10).join(operation_strings)}\n",
    "\n",
    "è«‹å…ˆæ ¹æ“š chain-of-thought åˆ†æï¼Œç„¶å¾Œå°‡æ“ä½œç·¨è™Ÿæ ¹æ“šé‡è¦æ€§æ’åºï¼Œæ¯å€‹ç·¨è™Ÿåƒ…åœ¨é™£åˆ—ä¸­å‡ºç¾ä¸€æ¬¡ï¼Œé™£åˆ—é•·åº¦æ‡‰ç‚º {len(operation_strings)}ã€‚\n",
    "\n",
    "æœ€å¾Œè«‹ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºæ’åºçµæœ:\n",
    "[1, 2, 3, ...]\"\"\"\n",
    "    \n",
    "    # ä½¿ç”¨é‡è©¦é‚è¼¯\n",
    "    response = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"å˜—è©¦ API è«‹æ±‚ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...\")\n",
    "            response = gemini.basic_request(prompt)\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤å›æ‡‰\n",
    "            if \"âš ï¸\" in response or not response:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"âš ï¸ è«‹æ±‚å¤±æ•—ï¼Œ{3}ç§’å¾Œé‡è©¦...\")\n",
    "                    import time\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"âŒ API å›æ‡‰éŒ¯èª¤ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸\")\n",
    "                    return [], response if response else \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "            \n",
    "            # æˆåŠŸç²å¾—å›æ‡‰\n",
    "            print(\"âœ… æˆåŠŸç²å¾— API å›æ‡‰\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è«‹æ±‚ç™¼ç”Ÿç•°å¸¸: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"3ç§’å¾Œé‡è©¦...\")\n",
    "                import time\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(f\"âŒ å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸\")\n",
    "                return [], f\"âš ï¸ API è«‹æ±‚å¤±æ•—: {e}\"\n",
    "    \n",
    "    if not response:\n",
    "        return [], \"âš ï¸ ç„¡æ³•å–å¾— Gemini å›æ‡‰\"\n",
    "    \n",
    "    return extract_operation_numbers_from_response(response), response\n",
    "\n",
    "\n",
    "def create_selected_operations_json(operation_details, sorted_numbers, keep_percentage=0.7, \n",
    "                                    force_include=[1, 2, 3], output_path=\"selected_operations.json\"):\n",
    "    \"\"\"\n",
    "    å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        operation_details: æ“ä½œè©³ç´°è³‡è¨Šåˆ—è¡¨\n",
    "        sorted_numbers: æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿåˆ—è¡¨\n",
    "        keep_percentage: ä¿ç•™æ¯”ä¾‹\n",
    "        force_include: å¼·åˆ¶åŒ…å«çš„æ“ä½œç·¨è™Ÿ\n",
    "        output_path: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        list: é¸æ“‡çš„æ“ä½œåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not sorted_numbers:\n",
    "        print(\"âš ï¸ è­¦å‘Š: sorted_numbers ç‚ºç©ºï¼Œç„¡æ³•å‰µå»ºæ“ä½œåˆ—è¡¨\")\n",
    "        return []\n",
    "    \n",
    "    # è¨ˆç®—è¦ä¿ç•™çš„æ“ä½œæ•¸é‡\n",
    "    keep_count = max(len(force_include), int(keep_percentage * len(sorted_numbers)))\n",
    "    \n",
    "    # é¸æ“‡å‰ N å€‹æ“ä½œ\n",
    "    selected_numbers = sorted_numbers[:keep_count]\n",
    "    \n",
    "    # ç¢ºä¿å¼·åˆ¶åŒ…å«çš„æ“ä½œåœ¨åˆ—è¡¨ä¸­\n",
    "    selected_numbers = list(set(selected_numbers) | set(force_include))\n",
    "    \n",
    "    # é‡æ–°æ’åº: å…ˆæŒ‰ç…§ sorted_numbers çš„é †åºï¼Œç„¶å¾ŒåŠ ä¸Š force_include ä¸­æœªå‡ºç¾çš„\n",
    "    final_selected = []\n",
    "    for num in sorted_numbers:\n",
    "        if num in selected_numbers and num not in final_selected:\n",
    "            final_selected.append(num)\n",
    "    \n",
    "    for num in force_include:\n",
    "        if num not in final_selected:\n",
    "            final_selected.append(num)\n",
    "    \n",
    "    print(f\"é¸æ“‡äº† {len(final_selected)} å€‹æ“ä½œ (ä¿ç•™æ¯”ä¾‹: {keep_percentage*100:.0f}%)\")\n",
    "    print(f\"é¸æ“‡çš„æ“ä½œç·¨è™Ÿ: {final_selected}\")\n",
    "    \n",
    "    # å‰µå»ºæ“ä½œç·¨è™Ÿåˆ°è©³ç´°è³‡è¨Šçš„æ˜ å°„\n",
    "    operation_map = {int(detail['number']): detail for detail in operation_details}\n",
    "    \n",
    "    # å‰µå»ºæ–°çš„æ“ä½œåˆ—è¡¨\n",
    "    new_operations = []\n",
    "    missing_operations = []\n",
    "    \n",
    "    for new_id, num in enumerate(final_selected, 1):\n",
    "        if num in operation_map:\n",
    "            detail = operation_map[num]\n",
    "            new_operations.append({\n",
    "                'number': new_id,\n",
    "                'operation': detail['operation'],\n",
    "                'description': detail['description']\n",
    "            })\n",
    "        else:\n",
    "            missing_operations.append(num)\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ“ä½œç·¨è™Ÿ {num}\")\n",
    "    \n",
    "    if missing_operations:\n",
    "        print(f\"âš ï¸ ç¼ºå¤±çš„æ“ä½œç·¨è™Ÿ: {missing_operations}\")\n",
    "    \n",
    "    output_json = {\n",
    "        \"description\": \"Selected operations for badminton data analysis.\",\n",
    "        \"requirements\": [\n",
    "            \"The output must be based on the input data; do not hallucinate.\",\n",
    "            \"Give me the list of numbers.\"\n",
    "        ],\n",
    "        \"operations\": new_operations\n",
    "    }\n",
    "\n",
    "    # å¯«å…¥ JSON æ–‡ä»¶\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… {output_path} has been created with {len(new_operations)} operations.\")\n",
    "    return new_operations\n",
    "\n",
    "\n",
    "def read_badminton_data(file_path):\n",
    "    \"\"\"\n",
    "    è®€å–ç¾½çƒæ¯”è³½æ•¸æ“š CSV æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "\n",
    "# ==================== ä¸»ç¨‹å¼ ====================\n",
    "\n",
    "# è¼‰å…¥æ“ä½œ\n",
    "json_file_path = \"operations.json\"\n",
    "operation_details, operation_strings = load_operations_from_json(json_file_path)\n",
    "\n",
    "if not operation_details:\n",
    "    print(\"âŒ ç„¡æ³•è¼‰å…¥æ“ä½œï¼Œç¨‹å¼çµ‚æ­¢\")\n",
    "else:\n",
    "    # è¼‰å…¥æ•¸æ“š\n",
    "    df = read_badminton_data(\"filtered_set1.csv\")\n",
    "    \n",
    "    # ç²å– API é‡‘é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\") \n",
    "    \n",
    "    # æ’åºæ“ä½œ\n",
    "    sorted_numbers, response = filter_badminton_operations(\n",
    "        operation_details, \n",
    "        operation_strings, \n",
    "        df, \n",
    "        api_key, \n",
    "        outline_path='main.txt'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\nå®Œæ•´å›æ‡‰:\\n{response}\\n{'='*60}\\n\")\n",
    "    print(f\"æ’åºå¾Œçš„æ“ä½œç·¨è™Ÿ: {sorted_numbers}\")\n",
    "    \n",
    "    # å‰µå»ºé¸æ“‡çš„æ“ä½œ JSON\n",
    "    if sorted_numbers:\n",
    "        selected_ops = create_selected_operations_json(\n",
    "            operation_details,\n",
    "            sorted_numbers,\n",
    "            keep_percentage=0.7,\n",
    "            force_include=[1, 2, 3],\n",
    "            output_path=\"selected_operations.json\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"âŒ ç„¡æ³•æå–æ’åºçµæœï¼Œè·³éå‰µå»º selected_operations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a7919",
   "metadata": {},
   "source": [
    "# STEP final\n",
    "\n",
    "æ“ä½œç”Ÿæˆ (ContentPlanner)ã€å®‰å…¨åŸ·è¡Œ DataFrame æ“ä½œ (SafeDataFrameOperator)ã€æ¨¹çµæ§‹è¿½è¹¤ (TreeNode / TreeOfReport)ã€ä»¥åŠ æ–‡æœ¬ç”Ÿæˆ (TextGenerator)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:10,171 - INFO - Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\n",
      "2025-09-29 14:52:10,172 - INFO - ==================================================\n",
      "2025-09-29 14:52:10,173 - INFO - æ­£åœ¨è¼‰å…¥æ•¸æ“š...\n",
      "2025-09-29 14:52:10,177 - INFO - æˆåŠŸè¼‰å…¥CSV: 315 è¡Œ, 8 åˆ—\n",
      "2025-09-29 14:52:10,177 - INFO - æœ€å¤§æ·±åº¦: 3\n",
      "2025-09-29 14:52:10,178 - INFO - æœ€å¤§åˆ†æ”¯åº¦: 4\n",
      "2025-09-29 14:52:10,179 - INFO - è¼‰å…¥æ“ä½œæ± : ['description', 'requirements', 'operations']\n",
      "2025-09-29 14:52:10,180 - INFO - é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\n",
      "2025-09-29 14:52:10,181 - INFO - è™•ç†ç¯€é» - Level: 0, Operation: root(None)\n",
      "2025-09-29 14:52:10,196 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:11,829 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:11,830 - INFO - ç”Ÿæˆæ“ä½œ: ['select_column(type)', 'value_counts(type)', 'write()']\n",
      "2025-09-29 14:52:15,261 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (315, 11)\n",
      "2025-09-29 14:52:15,263 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(type), çµæœå½¢ç‹€: (315, 11)\n",
      "2025-09-29 14:52:15,263 - INFO - æ·»åŠ å­ç¯€é»: b6f7d70e to 02cf7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: name 'isinstance' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:17,169 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:17,170 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:17,170 - INFO - æ·»åŠ å­ç¯€é»: 92d2bd5f to 02cf7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts('type')æ“ä½œå·²å®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-29 14:52:19,526 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:19,527 - INFO - æ·»åŠ å­ç¯€é»: e393a128 to 02cf7060\n",
      "2025-09-29 14:52:19,527 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: select_column(type)\n",
      "2025-09-29 14:52:19,548 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:21,475 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:21,476 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'write()']\n",
      "2025-09-29 14:52:24,403 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:24,404 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:24,405 - INFO - æ·»åŠ å­ç¯€é»: f20f110e to b6f7d70e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed input_tmp.csv and saved to tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-29 14:52:25,958 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:25,959 - INFO - æ·»åŠ å­ç¯€é»: dae8701a to b6f7d70e\n",
      "2025-09-29 14:52:25,961 - INFO - è™•ç†ç¯€é» - Level: 1, Operation: value_counts(type)\n",
      "2025-09-29 14:52:25,967 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:26,840 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:26,841 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'select_column(type, count)']\n",
      "2025-09-29 14:52:28,543 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:28,543 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:28,544 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: value_counts(type)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_counts() æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:32,014 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (5, 1)\n",
      "2025-09-29 14:52:32,015 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: select_column(type, count), çµæœå½¢ç‹€: (5, 1)\n",
      "2025-09-29 14:52:32,017 - INFO - æ·»åŠ å­ç¯€é»: 9e598416 to 92d2bd5f\n",
      "2025-09-29 14:52:32,018 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-09-29 14:52:32,023 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:32,822 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:32,823 - INFO - ç”Ÿæˆæ“ä½œ: ['aggregate(count,type)', 'calculate(count/sum(count)', 'write()']\n",
      "2025-09-29 14:52:34,418 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:34,419 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: aggregate(count,type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:34,420 - INFO - æ·»åŠ å­ç¯€é»: 1f2746ef to f20f110e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èšåˆæ“ä½œå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:36,472 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:36,472 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: calculate(count/sum(count), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:36,473 - INFO - æ·»åŠ å­ç¯€é»: 6edcaae7 to f20f110e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame å·²æˆåŠŸè™•ç†ä¸¦å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-29 14:52:38,006 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:38,007 - INFO - æ·»åŠ å­ç¯€é»: 2521a37d to f20f110e\n",
      "2025-09-29 14:52:38,008 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: value_counts(type)\n",
      "2025-09-29 14:52:38,010 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:39,063 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:39,064 - INFO - ç”Ÿæˆæ“ä½œ: ['value_counts(type)', 'crosstab(type, count)', 'write()']\n",
      "2025-09-29 14:52:41,261 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:41,263 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: value_counts(type), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:41,263 - WARNING - å­ç¯€é»é©—è­‰å¤±æ•—: ['æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: value_counts(type)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜è‡³ tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:43,239 - INFO - æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:43,240 - INFO - å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: crosstab(type, count), çµæœå½¢ç‹€: (18, 2)\n",
      "2025-09-29 14:52:43,241 - INFO - æ·»åŠ å­ç¯€é»: d843f889 to a7ded6b3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crosstabæ“ä½œå®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° tmp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:44,766 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:44,767 - INFO - æ·»åŠ å­ç¯€é»: 29044b95 to a7ded6b3\n",
      "2025-09-29 14:52:44,768 - INFO - è™•ç†ç¯€é» - Level: 2, Operation: select_column(type, count)\n",
      "2025-09-29 14:52:44,770 - INFO - æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\n",
      "2025-09-29 14:52:46,141 - INFO - æˆåŠŸç²å¾—Geminiå›æ‡‰\n",
      "2025-09-29 14:52:46,142 - INFO - ç”Ÿæˆæ“ä½œ: ['write()']\n",
      "2025-09-29 14:52:47,262 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:47,263 - INFO - æ·»åŠ å­ç¯€é»: c0e7cd14 to 9e598416\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-29 14:52:48,589 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:48,590 - INFO - æ·»åŠ å­ç¯€é»: e4f86007 to 1f2746ef\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_32588\\350718677.py:516: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
      "2025-09-29 14:52:50,738 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:50,739 - INFO - æ·»åŠ å­ç¯€é»: 409eae89 to 6edcaae7\n",
      "2025-09-29 14:52:53,387 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:53,388 - INFO - æ·»åŠ å­ç¯€é»: c15d6c3d to fb2dae1c\n",
      "2025-09-29 14:52:54,691 - INFO - å‰µå»º write ç¯€é»: write()\n",
      "2025-09-29 14:52:54,692 - INFO - æ·»åŠ å­ç¯€é»: 081175fa to d843f889\n",
      "2025-09-29 14:52:54,692 - INFO - ç¯€é» e4f86007 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      åˆ‡çƒ     31\n",
      "1      å‹¾çƒ     12\n",
      "2      å¹³çƒ      2\n",
      "3   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "4      æŒ‘çƒ     35\n",
      "5      æ¨çƒ     31\n",
      "6      æ’²çƒ      5\n",
      "7     æ“‹å°çƒ     20\n",
      "8     æ”¾å°çƒ     28\n",
      "9    æœªçŸ¥çƒç¨®     16\n",
      "10     æ®ºçƒ     36\n",
      "11    ç™¼çŸ­çƒ     10\n",
      "12    ç™¼é•·çƒ     10\n",
      "13   éåº¦åˆ‡çƒ      6\n",
      "14     é•·çƒ     55\n",
      "15   é˜²å®ˆå›æŠ½      5\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     é»æ‰£      4\n",
      "ç¯€é»æ–‡æœ¬: æ³¨æ„åˆ°äº†å—ï¼Ÿæ®ºçƒå¾—åˆ†36æ¬¡ï¼Œé•·çƒæ›´æ˜¯é«˜é”55æ¬¡ï¼Œä½†ä¹Ÿä¸å¯å¿½è¦–ï¼ŒæŒ‘çƒå¤±èª¤ä¹Ÿæœ‰35æ¬¡ï¼Œåˆ‡çƒå¤±èª¤31æ¬¡ï¼Œçœ‹ä¾†ï¼Œæ§åˆ¶å¾Œå ´æ˜¯é—œéµã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:55,989 - INFO - ç¯€é» 1f2746ef æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:52:55,992 - INFO - ç¯€é» 409eae89 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      åˆ‡çƒ     31\n",
      "1      å‹¾çƒ     12\n",
      "2      å¹³çƒ      2\n",
      "3   å¾Œå ´æŠ½å¹³çƒ      7\n",
      "4      æŒ‘çƒ     35\n",
      "5      æ¨çƒ     31\n",
      "6      æ’²çƒ      5\n",
      "7     æ“‹å°çƒ     20\n",
      "8     æ”¾å°çƒ     28\n",
      "9    æœªçŸ¥çƒç¨®     16\n",
      "10     æ®ºçƒ     36\n",
      "11    ç™¼çŸ­çƒ     10\n",
      "12    ç™¼é•·çƒ     10\n",
      "13   éåº¦åˆ‡çƒ      6\n",
      "14     é•·çƒ     55\n",
      "15   é˜²å®ˆå›æŠ½      5\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     é»æ‰£      4\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½äº®é»åœ¨æ–¼æ®ºçƒå¾—åˆ†36æ¬¡å’Œé•·çƒ55æ¬¡ï¼Œé¡¯ç¤ºå‡ºå¼·å¤§çš„é€²æ”»èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒæŒ‘çƒå¤±èª¤35æ¬¡å’Œåˆ‡çƒå¤±èª¤31æ¬¡ä¹Ÿä¸å®¹å¿½è¦–ã€‚æ§åˆ¶å¾Œå ´å°‡æ˜¯æå‡è¡¨ç¾çš„é—œéµã€‚\n",
      "node.table:      type     count\n",
      "0      é•·çƒ  0.174603\n",
      "1      æ®ºçƒ  0.114286\n",
      "2      æŒ‘çƒ  0.111111\n",
      "3      åˆ‡çƒ  0.098413\n",
      "4      æ¨çƒ  0.098413\n",
      "5     æ”¾å°çƒ  0.088889\n",
      "6     æ“‹å°çƒ  0.063492\n",
      "7    æœªçŸ¥çƒç¨®  0.050794\n",
      "8      å‹¾çƒ  0.038095\n",
      "9     ç™¼é•·çƒ  0.031746\n",
      "10    ç™¼çŸ­çƒ  0.031746\n",
      "11  å¾Œå ´æŠ½å¹³çƒ  0.022222\n",
      "12   éåº¦åˆ‡çƒ  0.019048\n",
      "13   é˜²å®ˆå›æŠ½  0.015873\n",
      "14     æ’²çƒ  0.015873\n",
      "15     é»æ‰£  0.012698\n",
      "16   é˜²å®ˆå›æŒ‘  0.006349\n",
      "17     å¹³çƒ  0.006349\n",
      "ç¯€é»æ–‡æœ¬: å¾æ•¸æ“šä¾†çœ‹ï¼Œé•·çƒçš„ä½¿ç”¨é »ç‡æœ€é«˜ï¼Œä½”æ¯”17.46%ï¼Œè€Œæ®ºçƒä¹Ÿä½”æ“šäº†11.43%ã€‚çœ‹ä¾†ï¼Œå ´ä¸Šé¸æ‰‹å¤šä»¥é•·çƒèª¿å‹•ï¼Œä¼ºæ©Ÿä»¥æ®ºçƒå¾—åˆ†ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:52:58,130 - ERROR - Gemini å›æ‡‰å¤±æ•—: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15\n",
      "Please retry in 2.347348832s. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "2025-09-29 14:52:58,131 - INFO - å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… 30 ç§’å¾Œé‡è©¦ (1/3)...\n",
      "2025-09-29 14:53:47,004 - INFO - ç¯€é» 6edcaae7 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:53:47,008 - INFO - ç¯€é» 2521a37d æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type     count\n",
      "0      é•·çƒ  0.174603\n",
      "1      æ®ºçƒ  0.114286\n",
      "2      æŒ‘çƒ  0.111111\n",
      "3      åˆ‡çƒ  0.098413\n",
      "4      æ¨çƒ  0.098413\n",
      "5     æ”¾å°çƒ  0.088889\n",
      "6     æ“‹å°çƒ  0.063492\n",
      "7    æœªçŸ¥çƒç¨®  0.050794\n",
      "8      å‹¾çƒ  0.038095\n",
      "9     ç™¼é•·çƒ  0.031746\n",
      "10    ç™¼çŸ­çƒ  0.031746\n",
      "11  å¾Œå ´æŠ½å¹³çƒ  0.022222\n",
      "12   éåº¦åˆ‡çƒ  0.019048\n",
      "13   é˜²å®ˆå›æŠ½  0.015873\n",
      "14     æ’²çƒ  0.015873\n",
      "15     é»æ‰£  0.012698\n",
      "16   é˜²å®ˆå›æŒ‘  0.006349\n",
      "17     å¹³çƒ  0.006349\n",
      "ç¯€é»æ–‡æœ¬: å¾æ•¸æ“šåˆ†æï¼Œé•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œä½”æ¯”é”17.46%ï¼Œæ®ºçƒä½”æ¯”11.43%ã€‚é¡¯ç¤ºå ´ä¸Šé¸æ‰‹å‚¾å‘ä»¥é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¸¦ä¼ºæ©Ÿä»¥æ®ºçƒå¾—åˆ†ã€‚\n",
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æ¯”è³½ä¸­é•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œé”åˆ°55æ¬¡ï¼Œæ®ºçƒä¹Ÿå…·å‚™ä¸€å®šå¨è„…ï¼Œå…±36æ¬¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæœªçŸ¥çƒå“¡å¾—åˆ†æ¬¡æ•¸æœ€å¤šï¼Œéœ€è¦é€²ä¸€æ­¥åˆ†æå…¶å¾—åˆ†æ‰‹æ®µã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:48,213 - INFO - ç¯€é» f20f110e æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:53:48,216 - INFO - ç¯€é» dae8701a æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½äº®é»ç‚ºæ®ºçƒ36æ¬¡å’Œé•·çƒ55æ¬¡ï¼Œå±•ç¾å¼·å¤§é€²æ”»èƒ½åŠ›ã€‚æ•¸æ“šé¡¯ç¤ºï¼Œé•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œä½”æ¯”17.46%ï¼Œæ®ºçƒä½”æ¯”11.43%ï¼Œé¸æ‰‹å‚¾å‘ä»¥é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¸¦ä¼ºæ©Ÿæ®ºçƒå¾—åˆ†ã€‚ç„¶è€Œï¼ŒæŒ‘çƒå¤±èª¤35æ¬¡å’Œåˆ‡çƒå¤±èª¤31æ¬¡ä¸å®¹å¿½è¦–ï¼Œæ§åˆ¶å¾Œå ´æ˜¯é—œéµã€‚æœªçŸ¥çƒå“¡å¾—åˆ†æœ€å¤šï¼Œéœ€é€²ä¸€æ­¥åˆ†æå…¶å¾—åˆ†æ‰‹æ®µã€‚\n",
      "node.table:      Unnamed: 0  rally                 time  roundscore_A  roundscore_B  \\\n",
      "0             0      1  2025-09-28 00:05:47             1             0   \n",
      "1             1      1  2025-09-28 00:05:49             1             0   \n",
      "2             2      1  2025-09-28 00:05:50             1             0   \n",
      "3             3      1  2025-09-28 00:05:51             1             0   \n",
      "4             4      1  2025-09-28 00:05:52             1             0   \n",
      "..          ...    ...                  ...           ...           ...   \n",
      "310         314     36  2025-09-28 00:25:02            21            15   \n",
      "311         311     36  2025-09-28 00:24:58            21            15   \n",
      "312         310     36  2025-09-28 00:24:44            21            15   \n",
      "313         312     36  2025-09-28 00:25:00            21            15   \n",
      "314         313     36  2025-09-28 00:25:01            21            15   \n",
      "\n",
      "    player  type lose_reason getpoint_player  score_difference best_type  \n",
      "0        B   ç™¼é•·çƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "1        A    åˆ‡çƒ         NaN             NaN                 1      æœªçŸ¥çƒç¨®  \n",
      "2        B    æŒ‘çƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "3        A    é•·çƒ         NaN             NaN                 1      æœªçŸ¥çƒç¨®  \n",
      "4        B    æ®ºçƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "..     ...   ...         ...             ...               ...       ...  \n",
      "310      B    é•·çƒ          å‡ºç•Œ               A                 6      éåº¦åˆ‡çƒ  \n",
      "311      A    åˆ‡çƒ         NaN             NaN                 6      æœªçŸ¥çƒç¨®  \n",
      "312      B  æœªçŸ¥çƒç¨®         NaN             NaN                 6      éåº¦åˆ‡çƒ  \n",
      "313      B    æŒ‘çƒ         NaN             NaN                 6      éåº¦åˆ‡çƒ  \n",
      "314      A    é•·çƒ         NaN             NaN                 6      æœªçŸ¥çƒç¨®  \n",
      "\n",
      "[315 rows x 11 columns]\n",
      "ç¯€é»æ–‡æœ¬: æ³¨æ„åˆ°äº†å—ï¼ŸAé¸æ‰‹å–„æ–¼åˆ©ç”¨æ®ºçƒç›´æ¥å¾—åˆ†ï¼Œè€ŒBé¸æ‰‹å‰‡æ›´å¤šå› ç‚ºã€Œå°æ‰‹è½åœ°è‡´å‹ã€è€Œå¤±åˆ†ï¼Œé€™æˆ–è¨±æ˜¯æˆ°è¡“èª¿æ•´çš„é—œéµé»ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:51,521 - INFO - ç¯€é» b6f7d70e æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:53:51,524 - INFO - ç¯€é» c0e7cd14 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      Unnamed: 0  rally                 time  roundscore_A  roundscore_B  \\\n",
      "0             0      1  2025-09-28 00:05:47             1             0   \n",
      "1             1      1  2025-09-28 00:05:49             1             0   \n",
      "2             2      1  2025-09-28 00:05:50             1             0   \n",
      "3             3      1  2025-09-28 00:05:51             1             0   \n",
      "4             4      1  2025-09-28 00:05:52             1             0   \n",
      "..          ...    ...                  ...           ...           ...   \n",
      "310         314     36  2025-09-28 00:25:02            21            15   \n",
      "311         311     36  2025-09-28 00:24:58            21            15   \n",
      "312         310     36  2025-09-28 00:24:44            21            15   \n",
      "313         312     36  2025-09-28 00:25:00            21            15   \n",
      "314         313     36  2025-09-28 00:25:01            21            15   \n",
      "\n",
      "    player  type lose_reason getpoint_player  score_difference best_type  \n",
      "0        B   ç™¼é•·çƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "1        A    åˆ‡çƒ         NaN             NaN                 1      æœªçŸ¥çƒç¨®  \n",
      "2        B    æŒ‘çƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "3        A    é•·çƒ         NaN             NaN                 1      æœªçŸ¥çƒç¨®  \n",
      "4        B    æ®ºçƒ         NaN             NaN                 1      éåº¦åˆ‡çƒ  \n",
      "..     ...   ...         ...             ...               ...       ...  \n",
      "310      B    é•·çƒ          å‡ºç•Œ               A                 6      éåº¦åˆ‡çƒ  \n",
      "311      A    åˆ‡çƒ         NaN             NaN                 6      æœªçŸ¥çƒç¨®  \n",
      "312      B  æœªçŸ¥çƒç¨®         NaN             NaN                 6      éåº¦åˆ‡çƒ  \n",
      "313      B    æŒ‘çƒ         NaN             NaN                 6      éåº¦åˆ‡çƒ  \n",
      "314      A    é•·çƒ         NaN             NaN                 6      æœªçŸ¥çƒç¨®  \n",
      "\n",
      "[315 rows x 11 columns]\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½äº®é»åœ¨æ–¼æ®ºçƒ36æ¬¡å’Œé•·çƒ55æ¬¡ï¼Œé€²æ”»èƒ½åŠ›å¼·å¤§ã€‚é•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œä½”æ¯”17.46%ï¼Œæ®ºçƒä½”æ¯”11.43%ï¼Œé¸æ‰‹å‚¾å‘æ–¼ä»¥é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¼ºæ©Ÿæ®ºçƒå¾—åˆ†ã€‚ç„¶è€Œï¼ŒæŒ‘çƒå¤±èª¤35æ¬¡å’Œåˆ‡çƒå¤±èª¤31æ¬¡ä¸å®¹å¿½è¦–ï¼Œæ§åˆ¶å¾Œå ´æ˜¯é—œéµã€‚Aé¸æ‰‹å–„æ–¼åˆ©ç”¨æ®ºçƒç›´æ¥å¾—åˆ†ï¼ŒBé¸æ‰‹å‰‡æ›´å¤šå› ç‚ºã€Œå°æ‰‹è½åœ°è‡´å‹ã€è€Œå¤±åˆ†ï¼Œæˆ°è¡“èª¿æ•´æ˜¯é—œéµé»ã€‚æœªçŸ¥çƒå“¡å¾—åˆ†æœ€å¤šï¼Œéœ€é€²ä¸€æ­¥åˆ†æå…¶å¾—åˆ†æ‰‹æ®µã€‚\n",
      "node.table:   type\n",
      "0   é•·çƒ\n",
      "1   æ®ºçƒ\n",
      "2   æŒ‘çƒ\n",
      "3   åˆ‡çƒ\n",
      "4   æ¨çƒ\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½ä¾†çœ‹ï¼Œå„é¡çƒç¨®éƒ½æœ‰å‡ºç¾ï¼Œé•·çƒã€æ®ºçƒã€æŒ‘çƒé€™äº›åŸºæœ¬çƒè·¯æ˜¯å…µå®¶å¿…çˆ­ï¼Œå·§å¦™çš„åˆ‡çƒå’Œåˆé‘½çš„æ¨çƒä¹Ÿèƒ½å‰µé€ å¾—åˆ†æ©Ÿæœƒã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:52,423 - INFO - ç¯€é» 9e598416 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:   type\n",
      "0   é•·çƒ\n",
      "1   æ®ºçƒ\n",
      "2   æŒ‘çƒ\n",
      "3   åˆ‡çƒ\n",
      "4   æ¨çƒ\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½ä¸­ï¼Œå„é¡çƒç¨®å‡æœ‰å±•ç¾ã€‚é•·çƒã€æ®ºçƒã€æŒ‘çƒç­‰åŸºç¤çƒè·¯æ˜¯çˆ­å¥ªé‡é»ï¼Œè€Œå·§å¦™çš„åˆ‡çƒå’Œåˆé‘½çš„æ¨çƒäº¦èƒ½å‰µé€ å¾—åˆ†è‰¯æ©Ÿã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:53,376 - INFO - ç¯€é» 92d2bd5f æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:53:53,378 - INFO - ç¯€é» e393a128 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      type  count\n",
      "0      é•·çƒ     55\n",
      "1      æ®ºçƒ     36\n",
      "2      æŒ‘çƒ     35\n",
      "3      åˆ‡çƒ     31\n",
      "4      æ¨çƒ     31\n",
      "5     æ”¾å°çƒ     28\n",
      "6     æ“‹å°çƒ     20\n",
      "7    æœªçŸ¥çƒç¨®     16\n",
      "8      å‹¾çƒ     12\n",
      "9     ç™¼é•·çƒ     10\n",
      "10    ç™¼çŸ­çƒ     10\n",
      "11  å¾Œå ´æŠ½å¹³çƒ      7\n",
      "12   éåº¦åˆ‡çƒ      6\n",
      "13   é˜²å®ˆå›æŠ½      5\n",
      "14     æ’²çƒ      5\n",
      "15     é»æ‰£      4\n",
      "16   é˜²å®ˆå›æŒ‘      2\n",
      "17     å¹³çƒ      2\n",
      "ç¯€é»æ–‡æœ¬: æœ¬å ´æ¯”è³½å„é¡çƒç¨®å‡æœ‰å±•ç¾ã€‚é•·çƒã€æ®ºçƒã€æŒ‘çƒç­‰åŸºç¤çƒè·¯æ˜¯çˆ­å¥ªé‡é»ï¼Œå·§å¦™çš„åˆ‡çƒå’Œåˆé‘½çš„æ¨çƒäº¦èƒ½å‰µé€ å¾—åˆ†è‰¯æ©Ÿã€‚\n",
      "node.table:      Unnamed: 0  roundscore_A  roundscore_B player getpoint_player  type  \\\n",
      "0             0             1             0      B             NaN   ç™¼é•·çƒ   \n",
      "1             1             1             0      A             NaN    åˆ‡çƒ   \n",
      "2             2             1             0      B             NaN    æŒ‘çƒ   \n",
      "3             3             1             0      A             NaN    é•·çƒ   \n",
      "4             4             1             0      B             NaN    æ®ºçƒ   \n",
      "..          ...           ...           ...    ...             ...   ...   \n",
      "310         310            21            15      B             NaN  æœªçŸ¥çƒç¨®   \n",
      "311         311            21            15      A             NaN    åˆ‡çƒ   \n",
      "312         312            21            15      B             NaN    æŒ‘çƒ   \n",
      "313         313            21            15      A             NaN    é•·çƒ   \n",
      "314         314            21            15      B               A    é•·çƒ   \n",
      "\n",
      "     rally      time  \n",
      "0        1  00:05:47  \n",
      "1        1  00:05:49  \n",
      "2        1  00:05:50  \n",
      "3        1  00:05:51  \n",
      "4        1  00:05:52  \n",
      "..     ...       ...  \n",
      "310     36  00:24:44  \n",
      "311     36  00:24:58  \n",
      "312     36  00:25:00  \n",
      "313     36  00:25:01  \n",
      "314     36  00:25:02  \n",
      "\n",
      "[315 rows x 8 columns]\n",
      "ç¯€é»æ–‡æœ¬: å„ä½è§€çœ¾ï¼ŒAé¸æ‰‹ä»Šå¤©æ‰‹æ„Ÿç«ç†±ï¼Œå¤šæ¬¡åˆ©ç”¨ã€Œç™¼çŸ­çƒã€å¾Œçš„ã€Œæ¨çƒã€å’Œã€Œæ®ºçƒã€é€£è²«é€²æ”»ï¼Œé »é »å¾—åˆ†ã€‚Bé¸æ‰‹ä¹Ÿä¸ç”˜ç¤ºå¼±ï¼Œåœ¨å¤šæ‹ã€Œé•·çƒã€çš„ç›¸æŒå¾Œï¼Œä¼ºæ©Ÿã€Œé»æ‰£ã€å¾—åˆ†ï¼Œæ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œæˆ°æ³æ¿€çƒˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:55,037 - INFO - ç¯€é» 02cf7060 æ–‡æœ¬ç”Ÿæˆå®Œæˆ\n",
      "2025-09-29 14:53:55,048 - INFO - ç”Ÿæˆæœ€çµ‚å ±å‘Š...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.table:      Unnamed: 0  roundscore_A  roundscore_B player getpoint_player  type  \\\n",
      "0             0             1             0      B             NaN   ç™¼é•·çƒ   \n",
      "1             1             1             0      A             NaN    åˆ‡çƒ   \n",
      "2             2             1             0      B             NaN    æŒ‘çƒ   \n",
      "3             3             1             0      A             NaN    é•·çƒ   \n",
      "4             4             1             0      B             NaN    æ®ºçƒ   \n",
      "..          ...           ...           ...    ...             ...   ...   \n",
      "310         310            21            15      B             NaN  æœªçŸ¥çƒç¨®   \n",
      "311         311            21            15      A             NaN    åˆ‡çƒ   \n",
      "312         312            21            15      B             NaN    æŒ‘çƒ   \n",
      "313         313            21            15      A             NaN    é•·çƒ   \n",
      "314         314            21            15      B               A    é•·çƒ   \n",
      "\n",
      "     rally      time  \n",
      "0        1  00:05:47  \n",
      "1        1  00:05:49  \n",
      "2        1  00:05:50  \n",
      "3        1  00:05:51  \n",
      "4        1  00:05:52  \n",
      "..     ...       ...  \n",
      "310     36  00:24:44  \n",
      "311     36  00:24:58  \n",
      "312     36  00:25:00  \n",
      "313     36  00:25:01  \n",
      "314     36  00:25:02  \n",
      "\n",
      "[315 rows x 8 columns]\n",
      "ç¯€é»æ–‡æœ¬: è³‡æ–™åˆ†æå ±å‘Š\n",
      "\n",
      "æœ¬å ´æ¯”è³½äº®é»åœ¨æ–¼æ®ºçƒ36æ¬¡å’Œé•·çƒ55æ¬¡ï¼Œé€²æ”»èƒ½åŠ›å¼·å¤§ã€‚é•·çƒä½¿ç”¨é »ç‡æœ€é«˜ï¼Œä½”æ¯”17.46%ï¼Œæ®ºçƒä½”æ¯”11.43%ï¼Œé¸æ‰‹å‚¾å‘æ–¼ä»¥é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¼ºæ©Ÿæ®ºçƒå¾—åˆ†ã€‚æŒ‘çƒå¤±èª¤35æ¬¡å’Œåˆ‡çƒå¤±èª¤31æ¬¡ä¸å®¹å¿½è¦–ï¼Œæ§åˆ¶å¾Œå ´æ˜¯é—œéµã€‚Aé¸æ‰‹å–„æ–¼åˆ©ç”¨æ®ºçƒç›´æ¥å¾—åˆ†ï¼ŒBé¸æ‰‹å‰‡æ›´å¤šå› ç‚ºã€Œå°æ‰‹è½åœ°è‡´å‹ã€è€Œå¤±åˆ†ï¼Œæˆ°è¡“èª¿æ•´æ˜¯é—œéµé»ã€‚æœªçŸ¥çƒå“¡å¾—åˆ†æœ€å¤šï¼Œéœ€é€²ä¸€æ­¥åˆ†æå…¶å¾—åˆ†æ‰‹æ®µã€‚å„é¡çƒç¨®å‡æœ‰å±•ç¾ï¼Œé•·çƒã€æ®ºçƒã€æŒ‘çƒç­‰åŸºç¤çƒè·¯æ˜¯çˆ­å¥ªé‡é»ï¼Œåˆ‡çƒå’Œæ¨çƒäº¦èƒ½å‰µé€ å¾—åˆ†è‰¯æ©Ÿã€‚Aé¸æ‰‹æ‰‹æ„Ÿç«ç†±ï¼Œåˆ©ç”¨ç™¼çŸ­çƒå¾Œçš„æ¨çƒå’Œæ®ºçƒé€£è²«é€²æ”»ï¼ŒBé¸æ‰‹åœ¨å¤šæ‹é•·çƒç›¸æŒå¾Œé»æ‰£å¾—åˆ†ï¼Œæˆ°æ³æ¿€çƒˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 14:53:59,498 - INFO - æ¨¹çµæ§‹å·²å°å‡ºè‡³: tree_structure.json\n",
      "2025-09-29 14:53:59,501 - INFO - \n",
      "==================================================\n",
      "2025-09-29 14:53:59,501 - INFO - TREE-OF-REPORT æœ€çµ‚å ±å‘Š\n",
      "2025-09-29 14:53:59,502 - INFO - ==================================================\n",
      "2025-09-29 14:53:59,506 - INFO - å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: 109.33 ç§’\n",
      "2025-09-29 14:53:59,507 - INFO - ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "2025-09-29 14:53:59,508 - INFO - - tree_of_report.md: æœ€çµ‚å ±å‘Š\n",
      "2025-09-29 14:53:59,509 - INFO - - tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\n",
      "2025-09-29 14:53:59,509 - INFO - - tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\n",
      "2025-09-29 14:53:59,509 - INFO - - execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\n",
      "2025-09-29 14:53:59,510 - INFO - - tree_visualization.html: å¯è¦–åŒ–é é¢\n",
      "2025-09-29 14:53:59,512 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: input_tmp.csv\n",
      "2025-09-29 14:53:59,513 - INFO - æ¸…ç†æš«å­˜æª”æ¡ˆ: tmp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish generate report\n",
      "## ç¾½çƒæ¿€æˆ°ï¼šé•·çƒæ®ºçƒäº¤ç¹”ï¼Œå¾Œå ´æ§åˆ¶æˆå‹è² é—œéµ\n",
      "\n",
      "**ï¼ˆè¨˜è€…[ä½ çš„åå­—]å ±å°ï¼‰** ä¸€å ´é«˜å¼·åº¦çš„ç¾½çƒå°æ±ºæ˜¨æ—¥è½å¹•ï¼Œæ¯”è³½å……æ»¿é€Ÿåº¦èˆ‡åŠ›é‡çš„å±•ç¾ï¼Œé•·çƒèˆ‡æ®ºçƒæˆç‚ºä¸»æ—‹å¾‹ï¼Œä½†ç´°ç¯€ä¸Šçš„æ§åˆ¶ä¹Ÿæ±ºå®šäº†æœ€çµ‚çš„å‹è² ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½çŒ¶å¦‚å…©ä½é¸æ‰‹åœ¨ç¾½çƒå ´ä¸Šé€²è¡Œçš„ä¸€å ´ç²¾å¯†æ£‹å±€ï¼Œé•·çƒå¦‚åŒè©¦æ¢çš„æ£‹å­ï¼Œé »ç¹å‡ºç¾ï¼Œä½”æ“šäº†æ‰€æœ‰çƒè·¯ä½¿ç”¨çš„æœ€é«˜æ¯”ä¾‹ï¼Œå…©ä½é¸æ‰‹æ˜é¡¯å‚¾å‘æ–¼åˆ©ç”¨é•·çƒèª¿å‹•å°æ‰‹ï¼Œä¼ºæ©Ÿå°‹æ‰¾æ®ºçƒçš„æ©Ÿæœƒï¼Œå…¨å ´å…±å‡ºç¾äº†36æ¬¡ç²¾é‡‡çš„æ®ºçƒï¼Œå±•ç¾äº†é¸æ‰‹å€‘å¼·å¤§çš„é€²æ”»ç«åŠ›ã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œåœ¨çŒ›çƒˆé€²æ”»çš„èƒŒå¾Œï¼Œå¤±èª¤ä¹Ÿå¦‚å½±éš¨å½¢ã€‚æŒ‘çƒå¤±èª¤èˆ‡åˆ‡çƒå¤±èª¤çš„æ¬¡æ•¸æé†’è‘—é¸æ‰‹å€‘ï¼Œå°å¾Œå ´çš„æ§åˆ¶è‡³é—œé‡è¦ã€‚å¾Œå ´ä¸€æ—¦å¤±å®ˆï¼Œå³ä½¿æ“æœ‰å†å¼·çš„é€²æ”»èƒ½åŠ›ï¼Œä¹Ÿé›£ä»¥è½‰åŒ–ç‚ºå‹å‹¢ã€‚\n",
      "\n",
      "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPlayer A æ“…é•·ä»¥é›·éœ†è¬éˆä¹‹å‹¢çš„æ®ºçƒç›´æ¥å¾—åˆ†ï¼Œè€Œ Player B å‰‡åœ¨é˜²å®ˆåæ“Šä¸­ç•¥é¡¯è¢«å‹•ï¼Œä¸å°‘å¤±åˆ†æºæ–¼å°æ‰‹æ®ºçƒè½åœ°è‡´å‹ï¼Œå¦‚ä½•æœ‰æ•ˆæ‡‰å°å°æ‰‹çš„é€²æ”»ï¼Œæˆç‚ºPlayer Bæœªä¾†æˆ°è¡“èª¿æ•´çš„é—œéµã€‚\n",
      "\n",
      "å„˜ç®¡æ¯”è³½ä¸­å„é¡çƒç¨®å‡æœ‰å±•ç¾ï¼Œä½†é•·çƒã€æ®ºçƒã€æŒ‘çƒç­‰åŸºç¤çƒè·¯çš„æŒæ¡ç¨‹åº¦ï¼Œä»ç„¶æ˜¯é›™æ–¹çˆ­å¥ªçš„ç„¦é»ã€‚åˆ‡çƒå’Œæ¨çƒçš„å·§å¦™é‹ç”¨ï¼Œå‰‡èƒ½ç‚ºé¸æ‰‹å‰µé€ æ›´å¤šå¾—åˆ†çš„è‰¯æ©Ÿã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼ŒPlayer A æ‰‹æ„Ÿç«ç†±ï¼Œé »é »åˆ©ç”¨ç™¼çŸ­çƒå¾Œçš„æ¨çƒå’Œæ®ºçƒé€£è²«é€²æ”»ï¼Œæ‰“å‡ºä¸€æ³¢æ³¢é«˜æ½®ã€‚Player B å‰‡åœ¨å¤šæ‹é•·çƒç›¸æŒå¾Œï¼Œä»¥ä¸€è¨˜å‹¢å¤§åŠ›æ²‰çš„é»æ‰£å¾—åˆ†ï¼Œå±•ç¾äº†å¼·å‹çš„å¯¦åŠ›ã€‚\n",
      "\n",
      "ç¸½è€Œè¨€ä¹‹ï¼Œé€™å ´æ¯”è³½ä¸åƒ…æ˜¯ä¸€å ´åŠ›é‡èˆ‡æŠ€å·§çš„è¼ƒé‡ï¼Œæ›´æ˜¯ä¸€å ´æ™ºæ…§èˆ‡æˆ°è¡“çš„åšå¼ˆã€‚èª°èƒ½æ›´å¥½åœ°æ§åˆ¶å¾Œå ´ï¼Œæ¸›å°‘å¤±èª¤ï¼Œä¸¦åœ¨é€²æ”»ç«¯æ›´å…·å¨è„…ï¼Œå°±èƒ½åœ¨æœªä¾†çš„æ¯”è³½ä¸­ä½”æ“šå„ªå‹¢ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import dspy\n",
    "import ast\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "import copy\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import builtins\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ===== åŸºæ–¼åƒè€ƒç¨‹å¼ç¢¼çš„å‡½æ•¸ =====\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"è®€å–æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"No file available\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®€å–æ–‡ä»¶éŒ¯èª¤: {e}\")\n",
    "        return \"Error reading file\"\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"è®€å–JSONæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin1') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # è¿”å›é»˜èªæ“ä½œé›†åˆ\n",
    "        return [\n",
    "            {\"name\": \"select_column\", \"description\": \"é¸æ“‡ç‰¹å®šæ¬„ä½\"},\n",
    "            {\"name\": \"value_counts\", \"description\": \"è¨ˆç®—å€¼çš„é »æ¬¡\"},\n",
    "            {\"name\": \"groupby\", \"description\": \"æŒ‰æ¬„ä½åˆ†çµ„\"},\n",
    "            {\"name\": \"sort_values\", \"description\": \"æ’åºæ•¸æ“š\"},\n",
    "            {\"name\": \"filter_rows\", \"description\": \"éæ¿¾è¡Œæ•¸æ“š\"},\n",
    "            {\"name\": \"write\", \"description\": \"æ’°å¯«åˆ†ææ–‡æœ¬\"}\n",
    "        ]\n",
    "\n",
    "# ===== æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ =====\n",
    "class TreeNode:\n",
    "    \"\"\"æ”¹é€²çš„æ¨¹ç¯€é»é¡åˆ¥ï¼Œå¢åŠ èªæ„é©—è­‰å’Œè¿½è¹¤åŠŸèƒ½\"\"\"\n",
    "    def __init__(self, level: int = 0, text: str = \"\", table: pd.DataFrame = None, operation: str = None):\n",
    "        self.children: List['TreeNode'] = []\n",
    "        self.level: int = level\n",
    "        self.text: str = text\n",
    "        self.table: pd.DataFrame = table if table is not None else pd.DataFrame()\n",
    "        self.operation: str = operation\n",
    "        self.parent: Optional['TreeNode'] = None\n",
    "        self.operation_history: List[str] = []\n",
    "        \n",
    "        # æ–°å¢å±¬æ€§ç”¨æ–¼æ”¹é€²åŠŸèƒ½\n",
    "        self.node_id: str = self._generate_node_id()\n",
    "        self.created_at: datetime = datetime.now()\n",
    "        self.validation_errors: List[str] = []\n",
    "        self.table_hash: str = self._calculate_table_hash()\n",
    "        self.semantic_score: float = 0.0\n",
    "        \n",
    "    def _generate_node_id(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆå”¯ä¸€ç¯€é»ID\"\"\"\n",
    "        content = f\"{self.level}_{self.operation}_{datetime.now().isoformat()}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        \n",
    "    def _calculate_table_hash(self) -> str:\n",
    "        \"\"\"è¨ˆç®—è¡¨æ ¼å…§å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨æ–¼æª¢æ¸¬é‡è¤‡\"\"\"\n",
    "        if self.table.empty:\n",
    "            return \"\"\n",
    "        try:\n",
    "            return hashlib.md5(str(self.table.values.tobytes()).encode()).hexdigest()[:8]\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def add_child(self, child: 'TreeNode'):\n",
    "        \"\"\"æ·»åŠ å­ç¯€é»ä¸¦é€²è¡Œé©—è­‰\"\"\"\n",
    "        if self._validate_child(child):\n",
    "            child.parent = self\n",
    "            self.children.append(child)\n",
    "            logger.info(f\"æ·»åŠ å­ç¯€é»: {child.node_id} to {self.node_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"å­ç¯€é»é©—è­‰å¤±æ•—: {child.validation_errors}\")\n",
    "    \n",
    "    def _validate_child(self, child: 'TreeNode') -> bool:\n",
    "        \"\"\"é©—è­‰å­ç¯€é»çš„åˆç†æ€§\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„è¡¨æ ¼ç‹€æ…‹\n",
    "        if child.table_hash and child.table_hash == self.table_hash:\n",
    "            if not child.operation.lower().startswith('write'):\n",
    "                errors.append(\"è¡¨æ ¼å…§å®¹æœªç™¼ç”Ÿè®ŠåŒ–ä½†éå¯«ä½œæ“ä½œ\")\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œæ˜¯å¦é‚è¼¯åˆç†\n",
    "        if self._is_redundant_operation(child.operation):\n",
    "            errors.append(f\"æª¢æ¸¬åˆ°å†—é¤˜æ“ä½œ: {child.operation}\")\n",
    "        \n",
    "        child.validation_errors = errors\n",
    "        return len(errors) == 0\n",
    "    \n",
    "    def _is_redundant_operation(self, operation: str) -> bool:\n",
    "        \"\"\"æª¢æŸ¥æ“ä½œæ˜¯å¦å†—é¤˜\"\"\"\n",
    "        if len(self.operation_history) < 2:\n",
    "            return False\n",
    "            \n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ“ä½œåœ¨è¿‘æœŸæ­·å²ä¸­\n",
    "        recent_ops = self.operation_history[-3:]  # æª¢æŸ¥æœ€è¿‘3å€‹æ“ä½œ\n",
    "        op_name = operation.split('(')[0].lower()\n",
    "        \n",
    "        for hist_op in recent_ops:\n",
    "            if hist_op.split('(')[0].lower() == op_name:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºè‘‰ç¯€é»\"\"\"\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"å°‡ç¯€é»è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–\"\"\"\n",
    "        return {\n",
    "            \"node_id\": self.node_id,\n",
    "            \"level\": self.level,\n",
    "            \"operation\": self.operation,\n",
    "            \"text_preview\": self.text[:100] + \"...\" if len(self.text) > 100 else self.text,\n",
    "            \"table_shape\": list(self.table.shape) if not self.table.empty else [0, 0],\n",
    "            \"table_columns\": list(self.table.columns) if not self.table.empty else [],\n",
    "            \"children_count\": len(self.children),\n",
    "            \"validation_errors\": self.validation_errors,\n",
    "            \"semantic_score\": self.semantic_score,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"table_hash\": self.table_hash\n",
    "        }\n",
    "\n",
    "# ===== æ”¹é€²çš„æ“ä½œè§£æå™¨ =====\n",
    "class OperationParser:\n",
    "    \"\"\"å°ˆé–€è² è²¬è§£æå’Œé©—è­‰æ“ä½œçš„é¡åˆ¥\"\"\"\n",
    "    \n",
    "    def __init__(self):    #æ”¹\n",
    "        self.valid_operations = {\n",
    "            'select_column', 'select_row',  'sort', 'calculate',\n",
    "            'group_by', 'value_counts', 'aggregate', 'crosstab','pivot_table', 'write'\n",
    "        }\n",
    "        \n",
    "    def parse_operations(self, response_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"æ”¹é€²çš„æ“ä½œè§£æï¼Œè¿”å›çµæ§‹åŒ–çµæœ\"\"\"\n",
    "        try:\n",
    "            parsed_operations = []\n",
    "            \n",
    "            # å¤šç¨®è§£æç­–ç•¥\n",
    "            operations = self._extract_operations_multiple_strategies(response_text)\n",
    "            \n",
    "            for op_str in operations:\n",
    "                parsed_op = self._parse_single_operation(op_str)\n",
    "                if parsed_op and self._validate_operation(parsed_op):\n",
    "                    parsed_operations.append(parsed_op)\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ•ˆæ“ä½œè¢«å¿½ç•¥: {op_str}\")\n",
    "            \n",
    "            return parsed_operations[:5]  # é™åˆ¶æœ€å¤š5å€‹æ“ä½œ\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œå¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_operations_multiple_strategies(self, text: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨å¤šç¨®ç­–ç•¥æå–æ“ä½œ\"\"\"\n",
    "        operations = []\n",
    "        \n",
    "        # ç­–ç•¥1: å°‹æ‰¾æ–¹æ‹¬è™Ÿå…§å®¹\n",
    "        bracket_match = re.search(r'\\[(.*?)\\]', text, re.DOTALL)\n",
    "        if bracket_match:\n",
    "            content = bracket_match.group(1)\n",
    "            # ä½¿ç”¨æ­£å‰‡æå–å‡½æ•¸èª¿ç”¨æ ¼å¼\n",
    "            pattern = r'([a-zA-Z_]+\\([^)]*\\))'\n",
    "            ops = re.findall(pattern, content)\n",
    "            operations.extend(ops)\n",
    "        \n",
    "        # ç­–ç•¥2: é€è¡Œè§£æ\n",
    "        if not operations:\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '(' in line and ')' in line:\n",
    "                    operations.append(line)\n",
    "        \n",
    "        # ç­–ç•¥3: é€—è™Ÿåˆ†å‰²\n",
    "        if not operations:\n",
    "            parts = text.replace('[', '').replace(']', '').split(',')\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part and '(' in part:\n",
    "                    operations.append(part)\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    def _parse_single_operation(self, op_str: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"è§£æå–®å€‹æ“ä½œå­—ç¬¦ä¸²\"\"\"\n",
    "        try:\n",
    "            # ç§»é™¤å¤šé¤˜çš„å­—ç¬¦\n",
    "            op_str = op_str.strip().rstrip(',').strip()\n",
    "            \n",
    "            # æå–æ“ä½œåç¨±å’Œåƒæ•¸\n",
    "            if '(' not in op_str:\n",
    "                return {\"name\": op_str, \"args\": [], \"raw\": op_str}\n",
    "            \n",
    "            name_part = op_str.split('(')[0].strip()\n",
    "            args_part = op_str[op_str.find('(')+1:op_str.rfind(')')].strip()\n",
    "            \n",
    "            # è§£æåƒæ•¸\n",
    "            args = []\n",
    "            if args_part:\n",
    "                # ç°¡å–®çš„åƒæ•¸åˆ†å‰²ï¼ˆå¯ä»¥é€²ä¸€æ­¥æ”¹é€²ï¼‰\n",
    "                for arg in args_part.split(','):\n",
    "                    arg = arg.strip().strip('\\'\"')\n",
    "                    if arg:\n",
    "                        args.append(arg)\n",
    "            \n",
    "            return {\n",
    "                \"name\": name_part.lower(),\n",
    "                \"args\": args,\n",
    "                \"raw\": op_str\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ææ“ä½œ '{op_str}' å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _validate_operation(self, operation: Dict[str, Any]) -> bool:\n",
    "        \"\"\"é©—è­‰æ“ä½œçš„æœ‰æ•ˆæ€§\"\"\"\n",
    "        name = operation.get(\"name\", \"\").lower()\n",
    "        \n",
    "        # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "        if name not in self.valid_operations:\n",
    "            logger.warning(f\"æœªçŸ¥æ“ä½œ: {name}\")\n",
    "            return False\n",
    "        \n",
    "        # æª¢æŸ¥ç‰¹å®šæ“ä½œçš„åƒæ•¸\n",
    "        args = operation.get(\"args\", [])\n",
    "        \n",
    "        if name in ['select_column', 'sort_values'] and not args:\n",
    "            logger.warning(f\"{name} æ“ä½œéœ€è¦åƒæ•¸\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ===== æ”¹é€²çš„å…§å®¹è¦åŠƒå™¨ =====\n",
    "class ContentPlanner:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.parser = OperationParser()\n",
    "        \n",
    "    def generate_operations(self, tables, table_description, operation_description, \n",
    "                          operation_history, operation_pool, max_depth=5, max_degree=3, outline_path='main.txt'):\n",
    "        \"\"\"\n",
    "        æ”¹é€²çš„æ“ä½œç”Ÿæˆï¼ŒåŠ å…¥é‡è¤‡æª¢æ¸¬å’Œèªæ„é©—è­‰\n",
    "        \"\"\"\n",
    "        \n",
    "        # æª¢æ¸¬è¿‘æœŸæ“ä½œï¼Œé¿å…é‡è¤‡\n",
    "        recent_operations = self._extract_recent_operations(operation_history)\n",
    "        \n",
    "        # æ§‹å»ºæ”¹é€²çš„æç¤ºè©\n",
    "        prompt = f\"\"\"System : You are a content planner for the report. Please follow the outline. Please select candidate Operations and corresponding Arguments from the Operation Pool based on the input Tables and Operation History. These candidate Operations will be the next Operation in the Operation History .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in English .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The length of Operation History must be less than or equal to {max_depth}.\n",
    "5. The number of Operations must be less than or equal to {max_degree}  and more than zero.\n",
    "6. Only select Opertions from the Operation Pool .\n",
    "7. Arguments must match the format required by the corresponding Operations .\n",
    "8. Operations & Arguments must follow this format : [ operation_1 ( argument_1 , ...) , operation_2 ( argument_2 , ...) , operation_3 ( argument_3 , ...) , ...]\n",
    "9. Only output Operations & Arguments !\n",
    "10. If Table is big or Level is low, it should be more Operations include select_col or select_row not write.\n",
    "11. If the length of Operation History is short, then more operations or more arguments.\n",
    "12. Write operations do not need argument.\n",
    "13. AVOID repeating recent operations: {recent_operations}\n",
    "14. Prioritize operations that will meaningfully transform the data.\n",
    "15. Avoid give the arguments that not match by the operation.\n",
    "\n",
    "#outline\n",
    "{read_text_file(outline_path) if os.path.exists(outline_path) else \"Generate comprehensive data analysis\"}\n",
    "\n",
    "# Table Description\n",
    "{table_description}\n",
    "\n",
    "# Operation Description\n",
    "{json.dumps(operation_description, indent=2, ensure_ascii=False)}\n",
    "\n",
    "User : # Test\n",
    "## Tables\n",
    "{tables}\n",
    "\n",
    "## Operation History\n",
    "{operation_history}\n",
    "\n",
    "## Operation Pool\n",
    "{operation_pool}\n",
    "\n",
    "## Operations & Arguments\"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"æ­£åœ¨å‘Geminiç™¼é€è«‹æ±‚...\")\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            if response.text:\n",
    "                logger.info(\"æˆåŠŸç²å¾—Geminiå›æ‡‰\")\n",
    "                parsed_ops = self.parser.parse_operations(response.text.strip())\n",
    "                return [op[\"raw\"] for op in parsed_ops]  # è¿”å›åŸå§‹å­—ç¬¦ä¸²æ ¼å¼\n",
    "            else:\n",
    "                logger.warning(\"Geminiå›æ‡‰ç‚ºç©º\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini APIè«‹æ±‚å¤±æ•—: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_recent_operations(self, operation_history: List[str]) -> List[str]:\n",
    "        \"\"\"æå–æœ€è¿‘çš„æ“ä½œåç¨±\"\"\"\n",
    "        recent = []\n",
    "        for op in operation_history[-3:]:  # æœ€è¿‘3å€‹æ“ä½œ\n",
    "            if '(' in op:\n",
    "                name = op.split('(')[0].strip()\n",
    "                recent.append(name)\n",
    "        return recent\n",
    "\n",
    "# ===== å®‰å…¨çš„DataFrameæ“ä½œå™¨ =====\n",
    "class SafeDataFrameOperator:\n",
    "    \"\"\"å®‰å…¨çš„DataFrameæ“ä½œå™¨ï¼Œä½¿ç”¨ASTé©—è­‰è€Œéç›´æ¥exec\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.allowed_modules = {'pandas', 'numpy', 're'}\n",
    "        self.allowed_functions = {\n",
    "            'pd.read_csv', 'pd.DataFrame', 'df.head', 'df.tail', 'df.sort_values',\n",
    "            'df.groupby', 'df.filter', 'df.select', 'df.drop', 'df.fillna',\n",
    "            'df.to_csv', 'df.value_counts', 'df.describe', 'df.info'\n",
    "        }\n",
    "\n",
    "    def generate_code(self, operation, df_info, df_path=\"input_tmp.csv\"):\n",
    "        prompt = f\"\"\"\n",
    "        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„Pythonè³‡æ–™åˆ†æåŠ©æ‰‹ã€‚æ¬„ä½åç¨±ä»¥è³‡æ–™æ¬„ä½é¡å‹æä¾›ç‚ºä¸»ï¼Œæ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆæ“ä½œDataFrameçš„ç¨‹å¼ç¢¼ï¼š\n",
    "\n",
    "        è¦åŸ·è¡Œçš„æ“ä½œ: {operation}\n",
    "\n",
    "        CSVæ•¸æ“šé›†: {df_path}\n",
    "\n",
    "        è³‡æ–™æ¬„ä½é¡å‹:\n",
    "        {df_info}\n",
    "\n",
    "        ç”Ÿæˆè¦æ±‚ï¼š\n",
    "        1. è®€å–CSVæ•¸æ“šé›†ï¼Œä¸¦å­˜å…¥DataFrameå¾Œï¼Œä½¿ç”¨è¦åŸ·è¡Œçš„æ“ä½œå¾Œï¼Œå°‡ä¿®æ”¹å¾Œçš„DataFrameå­˜å…¥'tmp.csv'\n",
    "        2. åªä½¿ç”¨pandasåŸºæœ¬æ“ä½œï¼Œé¿å…è¤‡é›œçš„è‡ªå®šç¾©å‡½æ•¸\n",
    "        3. ç¢ºä¿ä»£ç¢¼å®‰å…¨ï¼Œä¸åŒ…å«æ–‡ä»¶ç³»çµ±æ“ä½œï¼ˆé™¤äº†æŒ‡å®šçš„CSVè®€å¯«ï¼‰\n",
    "        4. æ’°å¯«å®Œæ•´python codeï¼ŒåŒ…å«éŒ¯èª¤è™•ç†\n",
    "\n",
    "        è¼¸å‡ºæ ¼å¼ï¼š\n",
    "        ```python\n",
    "        # ä½ çš„ç¨‹å¼ç¢¼\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=2):\n",
    "        \"\"\"å¸¶é‡è©¦çš„ç”Ÿæˆè«‹æ±‚\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ç”Ÿæˆä»£ç¢¼å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    import time\n",
    "                    time.sleep(1)\n",
    "        return \"\"\n",
    "\n",
    "    def safe_execute(self, code: str, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"å®‰å…¨åŸ·è¡Œç”Ÿæˆçš„ä»£ç¢¼\"\"\"\n",
    "        try:\n",
    "            # æå–ä»£ç¢¼å¡Š\n",
    "            code_block = re.search(r'```python\\n(.*?)\\n```', code, re.DOTALL)\n",
    "            #print(f'python code: {code_block}')\n",
    "            if code_block:\n",
    "                code = code_block.group(1)\n",
    "\n",
    "            # ASTå®‰å…¨é©—è­‰\n",
    "            if not self._validate_code_safety(code):\n",
    "                logger.error(\"ä»£ç¢¼å®‰å…¨é©—è­‰å¤±æ•—\")\n",
    "                return df\n",
    "\n",
    "            # å¯«å…¥æš«å­˜ CSV æª”æ¡ˆ\n",
    "            df.to_csv(\"input_tmp.csv\", index=False)\n",
    "\n",
    "            allowed_builtin_names = [\n",
    "                'int', 'float', 'str', 'bool', 'list', 'dict', 'set', 'tuple',\n",
    "                'len', 'range', 'enumerate', 'zip', 'min', 'max', 'sum', 'abs',\n",
    "                'print',\n",
    "                'Exception', 'TypeError', 'ValueError', 'KeyError', 'IndexError',\n",
    "                'FileNotFoundError', 'ZeroDivisionError', 'AttributeError', 'ImportError'\n",
    "            ]       \n",
    "\n",
    "            safe_globals = {\n",
    "                'pd': pd,\n",
    "                '__name__': '__main__',\n",
    "                '__builtins__': {name: getattr(builtins, name) for name in allowed_builtin_names}\n",
    "            }\n",
    "\n",
    "            safe_locals = {}\n",
    "\n",
    "            # åŸ·è¡Œä»£ç¢¼\n",
    "            exec(code, safe_globals, safe_locals)\n",
    "\n",
    "            # è®€å–çµæœ\n",
    "            if os.path.exists(\"tmp.csv\"):\n",
    "                result_df = pd.read_csv(\"tmp.csv\")\n",
    "                logger.info(f\"æ“ä½œæˆåŠŸï¼Œçµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                return result_df\n",
    "            else:\n",
    "                logger.warning(\"æœªç”Ÿæˆçµæœæ–‡ä»¶ï¼Œè¿”å›åŸå§‹DataFrame\")\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            print(\"éŒ¯èª¤ä»£ç¢¼å¦‚ä¸‹ï¼š\\n\" + \"-\" * 30)\n",
    "            print(code)  # âœ… è¼¸å‡ºé€ æˆéŒ¯èª¤çš„ç¨‹å¼ç¢¼\n",
    "            print(\"-\" * 30)\n",
    "            logger.error(error_msg)\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    def _validate_code_safety(self, code: str) -> bool:\n",
    "        \"\"\"ä½¿ç”¨ASTé©—è­‰ä»£ç¢¼å®‰å…¨æ€§\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                # æª¢æŸ¥å±éšªçš„å‡½æ•¸èª¿ç”¨\n",
    "                if isinstance(node, ast.Call):\n",
    "                    if isinstance(node.func, ast.Name):\n",
    "                        func_name = node.func.id\n",
    "                        if func_name in ['exec', 'eval', 'compile', '__import__', 'open']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°å±éšªå‡½æ•¸: {func_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥æ–‡ä»¶æ“ä½œï¼ˆé™¤äº†å…è¨±çš„CSVæ“ä½œï¼‰\n",
    "                if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):\n",
    "                    if hasattr(node.func, 'attr'):\n",
    "                        attr_name = node.func.attr\n",
    "                        if attr_name in ['system', 'popen', 'subprocess']:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ç³»çµ±èª¿ç”¨: {attr_name}\")\n",
    "                            return False\n",
    "                \n",
    "                # æª¢æŸ¥å°å…¥èªå¥\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for alias in node.names:\n",
    "                        if alias.name not in self.allowed_modules:\n",
    "                            logger.error(f\"æª¢æ¸¬åˆ°ä¸å…è¨±çš„æ¨¡çµ„å°å…¥: {alias.name}\")\n",
    "                            return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ASTé©—è­‰å¤±æ•—: {e}\")\n",
    "            return False\n",
    "\n",
    "# ===== æ–‡æœ¬ç”Ÿæˆå™¨ =====\n",
    "import time\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, api_key, table_description=\"\"):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "        self.table_description = table_description\n",
    "\n",
    "    def extract_highlights_from_table(self, table: pd.DataFrame) -> str:\n",
    "        try:\n",
    "            if 'lose_reason' in table.columns:\n",
    "                top_reason = table['lose_reason'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_reason = \"ç„¡è³‡æ–™\"\n",
    "            if 'getpoint_player' in table.columns:\n",
    "                top_player = table['getpoint_player'].value_counts().idxmax()\n",
    "            else:\n",
    "                top_player = \"æœªçŸ¥çƒå“¡\"\n",
    "            return f\"æœ€å¤šå¤±åˆ†åŸå› ç‚ºã€Œ{top_reason}ã€ï¼Œå¾—åˆ†æœ€å¤šçš„æ˜¯ {top_player}ã€‚\"\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_table_features(self, table: pd.DataFrame) -> str:\n",
    "        summary = []\n",
    "        for col in table.columns:\n",
    "            dtype = str(table[col].dtype)\n",
    "            line = f\"æ¬„ä½ã€Œ{col}ã€é¡å‹ï¼š{dtype}\"\n",
    "\n",
    "            # é¡¯ç¤ºå¸¸è¦‹å€¼åƒ…é™é¡åˆ¥å‹æ¬„ä½\n",
    "            if table[col].nunique() <= 10 or dtype == 'object' or pd.api.types.is_categorical_dtype(table[col]):\n",
    "                top_values = table[col].value_counts().head(3).to_dict()\n",
    "                line += f\"ï¼Œå¸¸è¦‹å€¼ï¼š{list(top_values.keys())}\"\n",
    "            summary.append(line)\n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def _retry_generate(self, prompt, max_retries=3, delay_seconds=30):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                if response.text:\n",
    "                    return response.text.strip()\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                logger.error(f\"Gemini å›æ‡‰å¤±æ•—: {err}\")\n",
    "                if \"429\" in err:\n",
    "                    logger.info(f\"å·²é”é…é¡é™åˆ¶ï¼Œç­‰å¾… {delay_seconds} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(delay_seconds)\n",
    "                else:\n",
    "                    break\n",
    "        return \"âš ï¸ å¯«ä½œè«‹æ±‚å¤±æ•—ï¼šAPI é™åˆ¶æˆ–å…¶ä»–éŒ¯èª¤\"\n",
    "\n",
    "    def generate_text_for_write_operation(self, table: pd.DataFrame, operation_history: List[str]) -> str:\n",
    "        table_str = table.to_string()\n",
    "        WRITE_TOKENS = 50\n",
    "        TABLE_FORMAT = \"Pandas DataFrame as plain text\"\n",
    "        highlight_summary = self.extract_highlights_from_table(table)\n",
    "        table_feature_summary = self.extract_table_features(table)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a professional content writer for the badminton game report .\n",
    "Please write the Report based on the input Table, just pick one or two lightspots.\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The Table format is {TABLE_FORMAT}.\n",
    "5. The Report can only describe the content included in the Tables and cannot describe anything not included in the Tables .\n",
    "6. The Report must consist of only one paragraph .\n",
    "7. The number of tokens in the Report must be within {WRITE_TOKENS}.\n",
    "8. è«‹å°ˆæ³¨æè¿°å¾—åˆ†èˆ‡å¤±åˆ†æ¨¡å¼ã€é—œéµæ¬„ä½è¶¨å‹¢æˆ–çƒå“¡äº®é»ã€‚\n",
    "9. è«‹æ¨¡ä»¿æ¯”è³½è½‰æ’­å“¡æˆ–æ•™ç·´çš„èªæ°£æè¿°ï¼Œå¥å¼è‡ªç„¶ã€æœ‰ç¯€å¥æ„Ÿã€‚\n",
    "10. è«‹ç‰¹åˆ¥è§€å¯Ÿçƒç¨®ä¹‹é–“çš„é€£çºŒè½‰æ›ï¼Œä¾‹å¦‚ æ”¾å°çƒ æ¥ æ®ºçƒ ç­‰ï¼Œæ‰¾å‡ºå…¶ä¸­æœ‰æ•ˆå¾—åˆ†æˆ–ä¸å°‹å¸¸çš„çµ„åˆä¸¦æè¿°ã€‚\n",
    "\n",
    "# Highlights Summary\n",
    "{highlight_summary}\n",
    "\n",
    "# Table Features\n",
    "{table_feature_summary}\n",
    "\n",
    "# Table Description\n",
    "{self.table_description}\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Tables\n",
    "{table_str}\n",
    "## Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "    def merge_child_texts(self, child_texts: List[str], parent_operation: str) -> str:\n",
    "        if not child_texts:\n",
    "            return \"\"\n",
    "\n",
    "        GENERATING_TOKENS = 100\n",
    "        reports_str = \"\\n\".join([f\"- {txt}\" for txt in child_texts])\n",
    "        prompt = f\"\"\"\n",
    "System :\n",
    "You are a content generator for the badminton game report .\n",
    "Please merge and rewrite a New Report based on the input Reports .\n",
    "\n",
    "# Requirements\n",
    "1. Strictly adhere to the requirements .\n",
    "2. The output must be in ä¸­æ–‡ .\n",
    "3. The output must be based on the input data ; do not hallucinate .\n",
    "4. The New Report must include all the content from the input Reports ; do not omit any information .\n",
    "5. The New Report must follow the order of the input Reports .\n",
    "6. The number of tokens in the New Report must be within {GENERATING_TOKENS}.\n",
    "7. è«‹ä¾åºæ•´åˆæ¯æ®µå…§å®¹ï¼Œå½¢æˆçµæ§‹æ¸…æ™°çš„æ®µè½ï¼ŒåŒ…æ‹¬äº®é»ã€å¤±èª¤æ¨¡å¼èˆ‡çƒå“¡è²¢ç»ã€‚\n",
    "\n",
    "User :\n",
    "# Test\n",
    "## Reports\n",
    "{reports_str}\n",
    "## New Report\n",
    "\"\"\"\n",
    "        return self._retry_generate(prompt)\n",
    "\n",
    "# ===== OperationParser._validate_operation å¼·åŒ–åƒæ•¸é©—è­‰ï¼ˆè£œå…¥ df æ¬„ä½æ¯”å°ï¼‰ =====\n",
    "def validate_operation_with_columns(operation: Dict[str, Any], df_columns: List[str]) -> bool:\n",
    "    name = operation.get(\"name\", \"\").lower()\n",
    "    args = operation.get(\"args\", [])\n",
    "\n",
    "    # æª¢æŸ¥æ“ä½œåç¨±æ˜¯å¦æœ‰æ•ˆ\n",
    "    if name not in {\n",
    "        'select_column', 'select_row', 'sort', 'calculate',\n",
    "        'group_by', 'value_counts', 'aggregate', 'crosstab', 'pivot_table', 'write'\n",
    "    }:\n",
    "        return False\n",
    "\n",
    "    # åƒ…é‡å°éœ€åƒæ•¸æ“ä½œæª¢æŸ¥æ¬„ä½\n",
    "    if name in ['select_column', 'sort', 'group_by']:\n",
    "        for arg in args:\n",
    "            if arg not in df_columns:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# ===== æ”¹é€²çš„TreeOfReporté¡åˆ¥ =====\n",
    "class TreeOfReport:\n",
    "    def __init__(self, api_key: str, max_depth: int = 5, max_degree: int = 5):\n",
    "        self.api_key = api_key\n",
    "        self.max_depth = max_depth\n",
    "        self.max_degree = max_degree\n",
    "\n",
    "        # è¼‰å…¥é…ç½®æª”æ¡ˆ\n",
    "        self.load_configurations()\n",
    "\n",
    "        # åˆå§‹åŒ–æ”¹é€²çš„çµ„ä»¶\n",
    "        self.content_planner = ContentPlanner(api_key)\n",
    "        self.df_operator = SafeDataFrameOperator(api_key)  # ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬\n",
    "        self.text_generator = TextGenerator(api_key, table_description=self.table_description)\n",
    "        \n",
    "        # æ–°å¢è¿½è¹¤åŠŸèƒ½\n",
    "        self.execution_log: List[Dict[str, Any]] = []\n",
    "        self.node_registry: Dict[str, TreeNode] = {}\n",
    "\n",
    "    def load_configurations(self):\n",
    "        self.table_description = read_text_file(\"filtered_data _description.txt\")\n",
    "        if not self.table_description or self.table_description == \"No file available\":\n",
    "            self.table_description = \"æ•¸æ“šåˆ†æè¡¨æ ¼ï¼ŒåŒ…å«å„ç¨®æ¬„ä½ç”¨æ–¼åˆ†æ\"\n",
    "\n",
    "        self.operation_description = read_json_file(\"selected_operations.json\")\n",
    "        if isinstance(self.operation_description, list):\n",
    "            self.operation_pool = [op['name'] for op in self.operation_description]\n",
    "        else:\n",
    "            self.operation_pool = list(self.operation_description.keys())\n",
    "\n",
    "        logger.info(f\"è¼‰å…¥æ“ä½œæ± : {self.operation_pool}\")\n",
    "\n",
    "    def build_tree(self, root_table: pd.DataFrame) -> TreeNode:\n",
    "        \"\"\"æ”¹é€²çš„æ¨¹æ§‹å»ºï¼ŒåŠ å…¥å®Œæ•´çš„è¿½è¹¤å’Œé©—è­‰\"\"\"\n",
    "        root = TreeNode(level=0, text=\"è³‡æ–™åˆ†æå ±å‘Š\", table=root_table, operation=\"root(None)\")\n",
    "        root.operation_history = ['root(None)']\n",
    "        self.node_registry[root.node_id] = root\n",
    "        \n",
    "        queue = [root]\n",
    "        \n",
    "        while queue:\n",
    "            current_node = queue.pop(0)\n",
    "            \n",
    "            # è¨˜éŒ„è™•ç†æ—¥èªŒ\n",
    "            self._log_node_processing(current_node)\n",
    "\n",
    "            if current_node.operation.lower().startswith('write'):\n",
    "                continue\n",
    "\n",
    "            if current_node.level >= self.max_depth:\n",
    "                write_node = self.create_child_node(current_node, 'write()')\n",
    "                if write_node:\n",
    "                    current_node.add_child(write_node)\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"è™•ç†ç¯€é» - Level: {current_node.level}, Operation: {current_node.operation}\")\n",
    "\n",
    "            tables_str = current_node.table.to_string()\n",
    "            operations = self.content_planner.generate_operations(\n",
    "                tables=tables_str,\n",
    "                table_description=self.table_description,\n",
    "                operation_description=self.operation_description,\n",
    "                operation_history=current_node.operation_history,\n",
    "                operation_pool=self.operation_pool,\n",
    "                max_depth=self.max_depth,\n",
    "                max_degree=self.max_degree\n",
    "            )\n",
    "\n",
    "            logger.info(f\"ç”Ÿæˆæ“ä½œ: {operations}\")\n",
    "\n",
    "            for operation in operations[:self.max_degree]:\n",
    "                if operation.strip():\n",
    "                    child_node = self.create_child_node(current_node, operation)\n",
    "                    if child_node:\n",
    "                        current_node.add_child(child_node)\n",
    "                        queue.append(child_node)\n",
    "\n",
    "        self.generate_all_texts(root)\n",
    "        return root\n",
    "    \n",
    "    def _log_node_processing(self, node: TreeNode):\n",
    "        \"\"\"è¨˜éŒ„ç¯€é»è™•ç†æ—¥èªŒ\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"node_id\": node.node_id,\n",
    "            \"level\": node.level,\n",
    "            \"operation\": node.operation,\n",
    "            \"table_shape\": list(node.table.shape) if not node.table.empty else [0, 0],\n",
    "            \"validation_errors\": node.validation_errors\n",
    "        }\n",
    "        self.execution_log.append(log_entry)\n",
    "    \n",
    "    def create_child_node(self, parent: TreeNode, operation: str) -> Optional[TreeNode]:\n",
    "        \"\"\"æ”¹é€²çš„å­ç¯€é»å‰µå»ºï¼ŒåŠ å…¥å®Œæ•´é©—è­‰\"\"\"\n",
    "        try:\n",
    "            # å»ºç«‹æ–°çš„æ“ä½œæ­·å²\n",
    "            new_operation_history = parent.operation_history + [operation]\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦ç‚º write æ“ä½œ\n",
    "            if operation.lower().startswith('write'):\n",
    "                text = self.text_generator.generate_text_for_write_operation(\n",
    "                    parent.table,\n",
    "                    new_operation_history\n",
    "                )\n",
    "                child = TreeNode(\n",
    "                    level=parent.level + 1,\n",
    "                    text=text,\n",
    "                    table=parent.table.copy(),\n",
    "                    operation=operation\n",
    "                )\n",
    "                child.operation_history = new_operation_history\n",
    "                self.node_registry[child.node_id] = child\n",
    "                logger.info(f\"å‰µå»º write ç¯€é»: {operation}\")\n",
    "                return child\n",
    "            else:\n",
    "                # å…¶ä»–æ“ä½œï¼šåŸ·è¡Œæ•¸æ“šæ“ä½œ\n",
    "                df_info = f\"Shape: {parent.table.shape}\\nColumns: {list(parent.table.columns)}\\nData types:\\n{parent.table.dtypes.to_string()}\"\n",
    "                code = self.df_operator.generate_code(operation, df_info)\n",
    "                \n",
    "                if code:\n",
    "                    result_df = self.df_operator.safe_execute(code, parent.table)\n",
    "                    child = TreeNode(\n",
    "                        level=parent.level + 1,\n",
    "                        text=\"\",\n",
    "                        table=result_df,\n",
    "                        operation=operation\n",
    "                    )\n",
    "                    child.operation_history = new_operation_history\n",
    "                    self.node_registry[child.node_id] = child\n",
    "                    logger.info(f\"å‰µå»ºæ•¸æ“šæ“ä½œç¯€é»: {operation}, çµæœå½¢ç‹€: {result_df.shape}\")\n",
    "                    return child\n",
    "                else:\n",
    "                    logger.warning(f\"ç„¡æ³•ç”Ÿæˆæ“ä½œä»£ç¢¼: {operation}\")\n",
    "                    return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‰µå»ºå­ç¯€é»å¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_texts(self, node: TreeNode):\n",
    "        \"\"\"éæ­¸ç”Ÿæˆæ‰€æœ‰ç¯€é»çš„æ–‡æœ¬\"\"\"\n",
    "        for child in node.children:\n",
    "            self.generate_all_texts(child)\n",
    "        \n",
    "        if node.is_leaf() and not node.text and node.operation and not node.operation.lower().startswith('write'):\n",
    "            node.text = self.text_generator.generate_text_for_write_operation(\n",
    "                node.table, \n",
    "                node.operation_history\n",
    "            )\n",
    "            print(f'node table: {node.table}')\n",
    "        elif node.children:\n",
    "            child_texts = [child.text for child in node.children if child.text.strip()]\n",
    "            if child_texts:\n",
    "                merged_text = self.text_generator.merge_child_texts(\n",
    "                    child_texts, \n",
    "                    node.operation or \"root\"\n",
    "                )\n",
    "                if node.text:\n",
    "                    node.text = node.text + \"\\n\\n\" + merged_text\n",
    "                else:\n",
    "                    node.text = merged_text\n",
    "        logger.info(f'ç¯€é» {node.node_id} æ–‡æœ¬ç”Ÿæˆå®Œæˆ')\n",
    "        print(f'node.table: {node.table}')\n",
    "        print(f'ç¯€é»æ–‡æœ¬: {node.text}')\n",
    "        \n",
    "    def export_tree_structure(self, root: TreeNode, output_path: str = \"tree_structure.json\"):\n",
    "        \"\"\"å°å‡ºæ¨¹çµæ§‹ç‚ºJSONæ ¼å¼ï¼Œç”¨æ–¼å¯è¦–åŒ–å’Œåˆ†æ\"\"\"\n",
    "        def node_to_dict(node: TreeNode) -> Dict[str, Any]:\n",
    "            result = node.to_dict()\n",
    "            result[\"children\"] = [node_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_time\": datetime.now().isoformat(),\n",
    "                \"total_nodes\": len(self.node_registry),\n",
    "                \"max_depth\": self.max_depth,\n",
    "                \"max_degree\": self.max_degree\n",
    "            },\n",
    "            \"execution_log\": self.execution_log,\n",
    "            \"tree\": node_to_dict(root)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(tree_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"æ¨¹çµæ§‹å·²å°å‡ºè‡³: {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å°å‡ºæ¨¹çµæ§‹å¤±æ•—: {e}\")\n",
    "    \n",
    "    def generate_execution_report(self) -> str:\n",
    "        \"\"\"ç”ŸæˆåŸ·è¡Œéç¨‹å ±å‘Š\"\"\"\n",
    "        total_nodes = len(self.node_registry)\n",
    "        error_nodes = sum(1 for node in self.node_registry.values() if node.validation_errors)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Tree-of-Report åŸ·è¡Œå ±å‘Š\n",
    "\n",
    "## çµ±è¨ˆä¿¡æ¯\n",
    "- ç¸½ç¯€é»æ•¸: {total_nodes}\n",
    "- éŒ¯èª¤ç¯€é»æ•¸: {error_nodes}\n",
    "- æ¨¹æœ€å¤§æ·±åº¦: {self.max_depth}\n",
    "- æœ€å¤§åˆ†æ”¯åº¦: {self.max_degree}\n",
    "\n",
    "## ç¯€é»åˆ†å¸ƒ\n",
    "\"\"\"\n",
    "        \n",
    "        # æŒ‰å±¤ç´šçµ±è¨ˆç¯€é»\n",
    "        level_counts = {}\n",
    "        for node in self.node_registry.values():\n",
    "            level = node.level\n",
    "            level_counts[level] = level_counts.get(level, 0) + 1\n",
    "        \n",
    "        for level, count in sorted(level_counts.items()):\n",
    "            report += f\"- Level {level}: {count} å€‹ç¯€é»\\n\"\n",
    "        \n",
    "        # éŒ¯èª¤æ‘˜è¦\n",
    "        if error_nodes > 0:\n",
    "            report += \"\\n## é©—è­‰éŒ¯èª¤æ‘˜è¦\\n\"\n",
    "            for node in self.node_registry.values():\n",
    "                if node.validation_errors:\n",
    "                    report += f\"- ç¯€é» {node.node_id} ({node.operation}): {'; '.join(node.validation_errors)}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def generate_report(self, node: TreeNode, level: int = 0) -> str:\n",
    "        \"\"\"æ”¹é€²çš„å ±å‘Šç”Ÿæˆ\"\"\"\n",
    "        if node.level == 0:\n",
    "            prompt = f\"\"\"\n",
    "            ä½ æ˜¯ä¸€ä½æ–°èè¨˜è€…ï¼Œæ ¹æ“šä»¥ä¸‹åˆ†æç¸½çµï¼Œè«‹æ’°å¯«ä¸€ç¯‡è³½äº‹æ–°èå ±å°ï¼Œæä¾›å…¨é¢æ·±å…¥çš„åˆ†æï¼Œçµ±æ•´æˆæ–°èå ±å°ï¼Œæ–‡è¾­ä¸­éå¤šç›´æ¥ä½¿ç”¨æ¬„ä½åç¨±èˆ‡ç›´æ¥æ¬¡æ•¸çµ±è¨ˆï¼Œç”¨player_Aèˆ‡player_Bè¡¨ç¤ºå…©çƒå“¡ï¼Œç”¨ç”Ÿå‹•çš„æ–‡å¥æè¿°ï¼Œå‹¿å‡ºç¾ç´¯è´…çš„å¥å­ï¼Œè«‹å¾åˆ†æç¸½çµä¸­æå–è½‰æ›ï¼Œç¦æ­¢å‡ºç¾å¹»è¦ºã€‚\n",
    "            è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ï¼Œè³‡è¨Šæº–ç¢ºã€‚\n",
    "\n",
    "            åˆ†æç¸½çµ:\n",
    "            {node.text}\n",
    "            \"\"\"\n",
    "            final_text = self.text_generator._retry_generate(prompt)\n",
    "            \n",
    "            # ä¿å­˜å¤šç¨®æ ¼å¼çš„å ±å‘Š\n",
    "            with open(\"tree_of_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(final_text)\n",
    "            \n",
    "            # å°å‡ºæ¨¹çµæ§‹\n",
    "            self.export_tree_structure(node)\n",
    "            \n",
    "            # ç”ŸæˆåŸ·è¡Œå ±å‘Š\n",
    "            exec_report = self.generate_execution_report()\n",
    "            with open(\"execution_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(exec_report)\n",
    "                print(\"finish generate report\")\n",
    "            \n",
    "            return final_text\n",
    "        else:\n",
    "            logger.info(f'generate report from not root')\n",
    "            indent = \"  \" * level\n",
    "            report = f\"{indent}{'#' * (level + 1)} {node.operation or 'Root'}\\n\\n\"\n",
    "\n",
    "            if node.text:\n",
    "                report += f\"{indent}{node.text}\\n\\n\"\n",
    "\n",
    "            if node.table is not None and not node.table.empty and level < 2:\n",
    "                report += f\"{indent}**è³‡æ–™æ‘˜è¦:** Shape {node.table.shape}\\n\"\n",
    "                if len(node.table) <= 10:\n",
    "                    report += f\"{indent}```\\n{node.table.to_string()}\\n{indent}```\\n\\n\"\n",
    "                else:\n",
    "                    report += f\"{indent}```\\n{node.table.head().to_string()}\\n{indent}```\\n\\n\"\n",
    "\n",
    "            for child in node.children:\n",
    "                report += self.generate_report(child, level + 1)\n",
    "\n",
    "            return report\n",
    "\n",
    "\n",
    "# ===== ä¸»ç¨‹åº =====\n",
    "def main():\n",
    "    \"\"\"æ”¹é€²çš„ä¸»å‡½æ•¸\"\"\"\n",
    "    \n",
    "    # è¨­ç½®APIå¯†é‘°\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "\n",
    "    logger.info(\"Tree-of-Report for Data Analysis (æ”¹é€²ç‰ˆ)\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    logger.info(\"æ­£åœ¨è¼‰å…¥æ•¸æ“š...\")\n",
    "    \n",
    "    # è®€å–CSVæª”æ¡ˆ\n",
    "\n",
    "    TABLES = pd.read_csv('filtered_set1.csv')\n",
    "    logger.info(f\"æˆåŠŸè¼‰å…¥CSV: {TABLES.shape[0]} è¡Œ, {TABLES.shape[1]} åˆ—\")\n",
    "\n",
    "    \n",
    "    # è¨­ç½®åƒæ•¸\n",
    "    MAX_DEPTH = 3\n",
    "    MAX_DEGREE = 4\n",
    "    \n",
    "    logger.info(f\"æœ€å¤§æ·±åº¦: {MAX_DEPTH}\")\n",
    "    logger.info(f\"æœ€å¤§åˆ†æ”¯åº¦: {MAX_DEGREE}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ”¹é€²çš„ Tree-of-Report\n",
    "    tree_report = TreeOfReport(api_key, max_depth=MAX_DEPTH, max_degree=MAX_DEGREE)\n",
    "    \n",
    "    # å»ºæ§‹å ±å‘Šæ¨¹\n",
    "    logger.info(\"é–‹å§‹å»ºæ§‹å ±å‘Šæ¨¹...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        root = tree_report.build_tree(TABLES)\n",
    "        \n",
    "        # ç”Ÿæˆæœ€çµ‚å ±å‘Š\n",
    "        logger.info(\"ç”Ÿæˆæœ€çµ‚å ±å‘Š...\")\n",
    "        final_report = tree_report.generate_report(root)\n",
    "        \n",
    "        # è¼¸å‡ºå ±å‘Š\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(\"TREE-OF-REPORT æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"=\"*50)\n",
    "        print(final_report)\n",
    "        \n",
    "        # å„²å­˜å ±å‘Š\n",
    "        with open('tree_of_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Tree-of-Report æ•¸æ“šåˆ†æå ±å‘Š (æ”¹é€²ç‰ˆ)\\n\\n\")\n",
    "            f.write(final_report)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        logger.info(f\"å ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’\")\n",
    "        logger.info(\"ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        logger.info(\"- tree_of_report.md: æœ€çµ‚å ±å‘Š\")\n",
    "        logger.info(\"- tree_of_report.txt: ç´”æ–‡æœ¬å ±å‘Š\")\n",
    "        logger.info(\"- tree_structure.json: æ¨¹çµæ§‹æ•¸æ“š\")\n",
    "        logger.info(\"- execution_report.md: åŸ·è¡Œéç¨‹å ±å‘Š\")\n",
    "        logger.info(\"- tree_visualization.html: å¯è¦–åŒ–é é¢\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # æ¸…ç†æš«å­˜æª”æ¡ˆ\n",
    "        for temp_file in ['input_tmp.csv', 'tmp.csv']:\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                    logger.info(f\"æ¸…ç†æš«å­˜æª”æ¡ˆ: {temp_file}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1db8ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ç¬¬ 1/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 2/3 æ¬¡ç”Ÿæˆ...\n",
      "â³ ç¬¬ 3/3 æ¬¡ç”Ÿæˆ...\n",
      "\n",
      "âœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\n",
      "[1] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "[2] åˆ†æ•¸: 0.6 â†’ é€™å ´ç¾½çƒè³½äº‹æˆ°æ³è† è‘—ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚æ¯”è³½åˆæ®µï¼Œé›™æ–¹éƒ½ä»¥è©¦æ¢æ€§çš„ç™¼çƒé–‹å±€ï¼Œéš¨å¾Œçƒè·¯è®ŠåŒ–å¤šç«¯ï¼Œæœ‰æ™‚æ˜¯è¼•å·§çš„ç¶²å‰å°çƒï¼Œæœ‰æ™‚æ˜¯åŠ›é“åè¶³çš„å¾Œå ´é‡æ“Šï¼Œçœ‹å¾—å‡ºé›™æ–¹é¸æ‰‹éƒ½åœ¨ç©æ¥µå°‹æ‰¾å°æ–¹çš„ç ´ç¶»ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé¸æ‰‹Aä¸€åº¦å–å¾—é ˜å…ˆï¼Œä½†é¸æ‰‹BéŸŒæ€§åè¶³ï¼Œç·Šå’¬æ¯”åˆ†ã€‚åœ¨å¤šæ‹ä¾†å›ä¸­ï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µä½³çš„é˜²å®ˆèƒ½åŠ›ï¼Œå¤šæ¬¡å°‡çœ‹ä¼¼å¿…æ®ºçš„çƒè·¯åŒ–è§£ã€‚ç¶²å‰çš„ç´°è†©æ‰‹æ³•å’Œå¾Œå ´çš„å¼·åŠ›é€²æ”»äº¤ç¹”ï¼Œè®“è§€çœ¾çœ‹å¾—ç›®ä¸æš‡çµ¦ã€‚\n",
      "\n",
      "åœ¨é—œéµæ™‚åˆ»ï¼Œé¸æ‰‹Aåˆ©ç”¨ä¸€æ¬¡ç²¾æº–çš„åˆ¤æ–·ï¼Œè®“å°æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸå¾—åˆ†ã€‚ç„¶è€Œï¼Œé¸æ‰‹Bä¹Ÿæ¯«ä¸ç¤ºå¼±ï¼Œéš¨å³ä»¥ä¸€è¨˜æ¼‚äº®çš„è½åœ°å¾—åˆ†é‚„ä»¥é¡è‰²ã€‚æ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œæ¯”è³½æ°£æ°›ä¹Ÿè¶Šç™¼ç·Šå¼µã€‚\n",
      "\n",
      "æœ€çµ‚ï¼Œé¸æ‰‹Aç©©ä½é™£è…³ï¼Œæ†‘è—‰è‘—ç©©å®šçš„ç™¼æ®å’Œé—œéµæ™‚åˆ»çš„æœæ–·é€²æ”»ï¼ŒæˆåŠŸæ‹¿ä¸‹åˆ†æ•¸ã€‚ä½†é¸æ‰‹Bçš„è¡¨ç¾ä¹ŸåŒæ¨£ç²¾å½©ï¼Œé›–æ•—çŒ¶æ¦®ã€‚æ•´å ´æ¯”è³½é«˜æ½®è¿­èµ·ï¼Œå……åˆ†å±•ç¾äº†ç¾½çƒé‹å‹•çš„é­…åŠ›ã€‚è§€çœ¾å€‘ä¹Ÿç‚ºé€™å ´ç²¾å½©çš„å°æ±ºç»ä¸Šäº†ç†±çƒˆçš„æŒè²ã€‚\n",
      "[3] åˆ†æ•¸: 0.75 â†’ é€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹é¸æ‰‹ä½ ä¾†æˆ‘å¾€ï¼Œæ”»é˜²è½‰æ›ç¯€å¥å¿«é€Ÿã€‚é–‹å±€é›™æ–¹äº’æœ‰é ˜å…ˆï¼Œæ¯”åˆ†äº¤æ›¿ä¸Šå‡ï¼Œé¦–å±€å‰åŠæ®µAé¸æ‰‹ç¨ä½”å„ªå‹¢ï¼Œä¸€åº¦å°‡æ¯”åˆ†æ‹‰é–‹è‡³2:1ï¼Œä½†Bé¸æ‰‹éš¨å³å±•é–‹åæ“Šï¼Œåˆ©ç”¨ç²¾æº–çš„è½é»æ§åˆ¶å’Œå¼·å‹¢çš„é€²æ”»ï¼Œå°‡æ¯”åˆ†è¿½å¹³ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å¤šå›åˆçš„ç²¾é‡‡å°æ±ºã€‚ä¾‹å¦‚ç¬¬ä¸‰åˆ†ï¼Œé›™æ–¹é¸æ‰‹ç¶“éå¤šæ¬¡çš„çŸ­çƒã€æŒ‘çƒã€é•·çƒã€æŠ½çƒã€åˆ‡çƒç­‰æˆ°è¡“é‹ç”¨ï¼Œè¶³è¶³ä¾†å›äº†17æ‹æ‰ç”±Aé¸æ‰‹æŠ“ä½æ©Ÿæœƒï¼Œä¸€è¨˜å°æ‰‹ç„¡æ³•æ¥åˆ°çš„çƒæ‹¿ä¸‹åˆ†æ•¸ã€‚å„˜ç®¡å¦‚æ­¤ï¼ŒBé¸æ‰‹ä¹Ÿæ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œéš¨å¾Œä¹Ÿä»¥é€£çºŒçš„ç©æ¥µé€²æ”»ï¼ŒåŒ…æ‹¬å¤šæ¬¡çš„æ®ºçƒï¼Œçµ¦Aé¸æ‰‹å¸¶ä¾†äº†æ¥µå¤§çš„å£“åŠ›ã€‚\n",
      "\n",
      "æ¯”è³½é€²å…¥ä¸­æ®µå¾Œï¼ŒAé¸æ‰‹åœ¨ç™¼çƒç’°ç¯€ä¸Šæ›´æ³¨æ„ç­–ç•¥ï¼Œå¶çˆ¾æ¡ç”¨çŸ­ç™¼ï¼Œå¸Œæœ›æ“¾äº‚Bé¸æ‰‹çš„ç¯€å¥ã€‚ä½†Bé¸æ‰‹ä¹Ÿç©æ¥µèª¿æ•´ï¼Œä¸¦åˆ©ç”¨Aé¸æ‰‹å¹¾æ¬¡åˆ¤æ–·å¤±èª¤åŠå›çƒæ›ç¶²çš„æ©Ÿæœƒï¼ŒæˆåŠŸå°‡æ¯”åˆ†åè¶…ã€‚\n",
      "\n",
      "æ¯”è³½æœ«æ®µï¼Œé›™æ–¹éƒ½å±•ç¾äº†æ¥µå¼·çš„éŸŒæ€§ã€‚å„˜ç®¡é«”åŠ›æ¶ˆè€—å·¨å¤§ï¼Œä½†ä¾èˆŠåŠªåŠ›åœ¨æ¯ä¸€æ¬¡æ“Šçƒä¸­å°‹æ‰¾æ©Ÿæœƒã€‚Aé¸æ‰‹æ›¾ä¾é ç²¾æº–çš„è½é»å’Œå¹¾æ¬¡æ¼‚äº®çš„é˜²å®ˆåæ“Šï¼Œå°‡æ¯”åˆ†è¿½è¿‘ï¼Œä½†Bé¸æ‰‹ç¸½èƒ½åœ¨é—œéµæ™‚åˆ»æŒºèº«è€Œå‡ºï¼Œå¤šæ¬¡åˆ©ç”¨å¼·åŠ›çš„æ‰£æ®ºä»¥åŠç²¾æº–çš„ç¶²å‰å°çƒï¼Œç©©ä½é™£è…³ã€‚æœ€çµ‚ï¼ŒBé¸æ‰‹ä»¥ä¸€è¨˜è§’åº¦åˆé‘½çš„æ’²çƒï¼Œè®“Aé¸æ‰‹æªæ‰‹ä¸åŠï¼ŒæˆåŠŸæ‹¿ä¸‹é€™å±€çš„ç¬¬15åˆ†ã€‚éš¨å¾ŒAé¸æ‰‹é›–å¥®åŠ›è¿½è¶•ï¼Œä½†æœ€çµ‚Bé¸æ‰‹æ†‘è—‰ä¸€è¨˜å¹¸é‹çš„é•·çƒå‡ºç•Œï¼Œä»¥21:15æ‹¿ä¸‹æ­¤å±€å‹åˆ©ã€‚\n",
      "\n",
      "æ•´å ´æ¯”è³½å……æ»¿äº†å„å¼å„æ¨£çš„æˆ°è¡“é‹ç”¨ï¼ŒåŒ…å«é•·çƒã€çŸ­çƒã€åˆ‡çƒã€æŒ‘çƒã€æ®ºçƒã€æ¨çƒã€å‹¾çƒï¼Œä»¥åŠç¶²å‰å°çƒçš„çˆ­å¥ªï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„çƒæŠ€å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œç‚ºè§€çœ¾å¸¶ä¾†äº†ä¸€å ´ç²¾é‡‡çµ•å€«çš„ç¾½çƒé¥—å®´ã€‚ æ¯”è³½çµæœé›–æœ‰å‹è² ï¼Œä½†é›™æ–¹é‹å‹•å“¡çš„é‹å‹•å®¶ç²¾ç¥ï¼Œéƒ½ä»¤äººå°è±¡æ·±åˆ»ã€‚\n",
      "\n",
      "ğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ 1 æ¬¡ï¼šé€™å ´ç¾½çƒè³½äº‹å¯è¬‚é«˜æ½®è¿­èµ·ï¼Œé›™æ–¹ä½ ä¾†æˆ‘å¾€ï¼Œäº’ä¸ç›¸è®“ã€‚å¾æ¯”è³½ä¼Šå§‹ï¼Œé›™æ–¹ä¾¿å±•é–‹äº†æ¿€çƒˆçš„æ”»é˜²è½‰æ›ï¼Œç™¼çƒã€éæ¸¡çƒã€åˆ°é€²æ”»ï¼Œæ¯å€‹å›åˆéƒ½å……æ»¿äº†è®Šæ•¸ã€‚å¯ä»¥çœ‹åˆ°çƒå“¡Aç‡å…ˆå–å¾—é ˜å…ˆï¼Œä¸€è·¯å°‡æ¯”åˆ†æ‹‰é–‹ï¼Œä¸€åº¦å–å¾—11:6çš„å„ªå‹¢ã€‚ç„¶è€Œï¼Œçƒå“¡Bä¸¦æ²’æœ‰è¼•æ˜“æ”¾æ£„ï¼Œå±•ç¾äº†é ‘å¼·çš„éŸŒæ€§ï¼Œé€æ¼¸å°‡æ¯”åˆ†è¿½è¶•ä¸Šä¾†ã€‚\n",
      "\n",
      "æ¯”è³½ä¸­ï¼Œé›™æ–¹é¸æ‰‹éƒ½åŠ›åœ–åœ¨å‰å ´å°‹æ‰¾æ©Ÿæœƒï¼ŒçŸ­çƒçš„é‹ç”¨é »ç¹ï¼Œå°çƒèˆ‡æŒ‘çƒçš„æ­é…ä¹Ÿè€ƒé©—è‘—é›™æ–¹çš„æŠ€è¡“ã€‚ä¸€äº›å›åˆçš„æ‹‰é‹¸éå¸¸é•·ï¼Œçƒå“¡å€‘ä¸æ–·åœ°é€²è¡Œæ”»é˜²è½‰æ›ï¼Œå¾Œå ´çš„å¼·åŠ›æ“Šçƒèˆ‡å‰å ´çš„ç²¾å·§æ§åˆ¶ç›¸äº’äº¤ç¹”ï¼Œå‘ˆç¾å‡ºç²¾å½©çš„å°æŠ—å ´é¢ã€‚å¤±èª¤ä¹Ÿå¶çˆ¾å‡ºç¾ï¼Œæ›ç¶²ã€å‡ºç•Œç­‰æƒ…æ³è®“æ¯”è³½æ›´å…·æ‡¸å¿µã€‚\n",
      "\n",
      "æ¯”è³½å¾ŒåŠæ®µï¼Œçƒå“¡Bé€æ¼¸æ‰¾åˆ°ç‹€æ…‹ï¼Œæ†‘è—‰ç©æ¥µçš„è·‘å‹•å’ŒæŠ“ä½æ©Ÿæœƒçš„èƒ½åŠ›ï¼Œå°‡æ¯”åˆ†åè¶…ï¼Œæœ€çµ‚ä»¥21:15çš„æ¯”åˆ†è´å¾—äº†å‹åˆ©ã€‚æ•´å ´æ¯”è³½ç¯€å¥ç·Šæ¹Šï¼Œé›™æ–¹éƒ½å±•ç¾äº†é«˜è¶…çš„ç¾½çƒæŠ€è—å’Œé ‘å¼·çš„é¬¥å¿—ï¼Œæ˜¯ä¸€å ´å€¼å¾—å›å‘³çš„ç²¾å½©å°æ±ºã€‚\n",
      "âœ”ï¸ å·²å„²å­˜è‡³ï¼šbest_of_three_report_20250928_195942.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === å¯«ä½œé¢¨æ ¼è©å½™ ===\n",
    "BADMINTON_TERMS = {\n",
    "    'net': 'ç¶²å‰å¤±èª¤', 'out': 'å‡ºç•Œ', 'long': 'éåº•ç·š', 'smash': 'æ®ºçƒ',\n",
    "    'clear': 'é«˜é çƒ', 'drop': 'åˆ‡çƒ', 'drive': 'å¹³æŠ½çƒ', 'serve': 'ç™¼çƒ', 'return': 'å›çƒ'\n",
    "}\n",
    "ACTION_VERBS = ['å±•ç¾', 'ç™¼æ®', 'æŒæ¡', 'é‹ç”¨', 'æ–½å±•', 'æ§åˆ¶', 'ä¸»å°', 'å£“åˆ¶', 'çªç ´', 'å‰µé€ ', 'ç· é€ ', 'å¥ å®š', 'ç¢ºç«‹', 'éå›º', 'æ‰­è½‰', 'é€†è½‰']\n",
    "TECHNICAL_TERMS = ['lose_reason', 'getpoint_player', 'type', 'column', 'row']\n",
    "\n",
    "# === Gemini æ¨¡å‹åˆå§‹åŒ– ===\n",
    "def init_model(api_key: str):\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# === å“è³ªè©•ä¼° ===\n",
    "def assess_text_quality(text: str) -> float:\n",
    "    score = 0.0\n",
    "    if 30 <= len(text) <= 120:\n",
    "        score += 0.2\n",
    "    score += min(0.2, sum(1 for t in BADMINTON_TERMS.values() if t in text) * 0.1)\n",
    "    score += min(0.2, sum(1 for v in ACTION_VERBS if v in text) * 0.05)\n",
    "    if not any(t in text for t in TECHNICAL_TERMS):\n",
    "        score += 0.2\n",
    "    if 'ï¼Œ' in text or 'ã€‚' in text:\n",
    "        score += 0.2\n",
    "    return round(min(score, 1.0), 2)\n",
    "\n",
    "# === ä¸»æµç¨‹ï¼šé‡è¤‡3æ¬¡ç”Ÿæˆä¸¦è©•ä¼° ===\n",
    "def generate_best_of_three(df: pd.DataFrame, api_key: str):\n",
    "    model = init_model(api_key)\n",
    "    table_str = df.to_string(index=False)\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­é«”è‚²æ–°èè¨˜è€…ï¼Œæ“…é•·æ’°å¯«ç¾½çƒæ¯”è³½å ±å°ã€‚\n",
    "è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šè¡¨æ ¼æ’°å¯«è³½äº‹æè¿°ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…å‡ºç¾æŠ€è¡“æ¬„ä½åç¨±ã€‚\n",
    "\n",
    "# è³½äº‹æ•¸æ“šè¡¨æ ¼ï¼š\n",
    "{table_str}\n",
    "\n",
    "è«‹æ’°å¯«æè¿°ï¼š\n",
    "\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            print(f\"â³ ç¬¬ {i+1}/3 æ¬¡ç”Ÿæˆ...\")\n",
    "            response = model.generate_content(prompt_template)\n",
    "            time.sleep(1)\n",
    "            text = response.text.strip() if response.text else \"âš ï¸ ç„¡å…§å®¹\"\n",
    "        except Exception as e:\n",
    "            text = f\"âš ï¸ ç”ŸæˆéŒ¯èª¤: {e}\"\n",
    "        score = assess_text_quality(text)\n",
    "        results.append({'index': i+1, 'text': text, 'score': score})\n",
    "\n",
    "    # é¸å‡ºæœ€ä½³çµæœ\n",
    "    best = max(results, key=lambda x: x['score'])\n",
    "\n",
    "    # è¼¸å‡ºåˆ°æª”æ¡ˆ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"best_of_three_report_{timestamp}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(f\"[ç‰ˆæœ¬ {r['index']}] å“è³ªåˆ†æ•¸: {r['score']}\\n{r['text']}\\n\\n\")\n",
    "        f.write(f\"ğŸ† æœ€ä½³ç‰ˆæœ¬ç‚ºç¬¬ {best['index']} æ¬¡ï¼Œåˆ†æ•¸: {best['score']}\\n\")\n",
    "        f.write(best['text'])\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ç‰ˆæœ¬å·²ç”Ÿæˆ\")\n",
    "    for r in results:\n",
    "        print(f\"[{r['index']}] åˆ†æ•¸: {r['score']} â†’ {r['text']}\")\n",
    "    print(f\"\\nğŸ† æœ€ä½³ç‰ˆæœ¬æ˜¯ç¬¬ {best['index']} æ¬¡ï¼š{best['text']}\")\n",
    "    print(f\"âœ”ï¸ å·²å„²å­˜è‡³ï¼š{file_name}\")\n",
    "    return best\n",
    "\n",
    "# === æ¸¬è©¦å…¥å£ ===\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"Gemini_API\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"è«‹è¨­ç½® Gemini_API ç’°å¢ƒè®Šæ•¸\")\n",
    "\n",
    "    df = pd.read_csv(\"filtered_set1.csv\")\n",
    "    \n",
    "\n",
    "    generate_best_of_three(df, api_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
